{"pages":[],"posts":[{"title":"복습day0617","text":"R MarkdownThis is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see http://rmarkdown.rstudio.com. When you click the Knit button a document will be generated that includes both content as well as the output of any embedded R code chunks within the document. You can embed an R code chunk like this: 1summary(cars) 1234567## speed dist ## Min. : 4.0 Min. : 2.00 ## 1st Qu.:12.0 1st Qu.: 26.00 ## Median :15.0 Median : 36.00 ## Mean :15.4 Mean : 42.98 ## 3rd Qu.:19.0 3rd Qu.: 56.00 ## Max. :25.0 Max. :120.00 Including PlotsYou can also embed plots, for example: Note that the echo = FALSE parameter was added to the code chunk to prevent printing of the R code that generated the plot. ggplot2 시각화 다음과 같이 시각화를 작성한다. 1234library(ggplot2)ggplot(data = iris, aes(x = Sepal.Length, y = Sepal.Width)) + geom_point()","link":"/2022/06/20/day0617_1/"},{"title":"Hello World","text":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new &quot;My New Post&quot; More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment","link":"/2022/06/20/hello-world/"},{"title":"test","text":"##csv파일 불러오기 csv파일을 불러옵니다 12mpg1 &lt;- read.csv(&quot;mpg1.csv&quot;)str(mpg1) 123456## 'data.frame': 234 obs. of 5 variables:## $ manufacturer: chr &quot;audi&quot; &quot;audi&quot; &quot;audi&quot; &quot;audi&quot; ...## $ trans : chr &quot;auto&quot; &quot;manual&quot; &quot;manual&quot; &quot;auto&quot; ...## $ drv : chr &quot;f&quot; &quot;f&quot; &quot;f&quot; &quot;f&quot; ...## $ cty : int 18 21 20 21 16 18 18 18 16 20 ...## $ hwy : int 29 29 31 30 26 26 27 26 25 28 ... ##데이터 시각화 하기-cty, hwy산점도를 그려본다 123library(ggplot2)ggplot(mpg1,aes(x =cty, y = hwy))+ geom_point()","link":"/2022/06/20/test/"},{"title":"220621","text":"#220621에 한 일들.데이터 불러오기#경로 설정이 매우 중요#getwd()#현재 경로를 확인하는 함수#setwd(“C:/Users/human/Desktop/R_lecure/source/data”)#csv파일/엑셀파일 불어오기#오늘 할 일 P91 12mpg1&lt;-read.csv(&quot;mpg1.csv&quot;)mpg1 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235## manufacturer trans drv cty hwy## 1 audi auto f 18 29## 2 audi manual f 21 29## 3 audi manual f 20 31## 4 audi auto f 21 30## 5 audi auto f 16 26## 6 audi manual f 18 26## 7 audi auto f 18 27## 8 audi manual 4 18 26## 9 audi auto 4 16 25## 10 audi manual 4 20 28## 11 audi auto 4 19 27## 12 audi auto 4 15 25## 13 audi manual 4 17 25## 14 audi auto 4 17 25## 15 audi manual 4 15 25## 16 audi auto 4 15 24## 17 audi auto 4 17 25## 18 audi auto 4 16 23## 19 chevrolet auto r 14 20## 20 chevrolet auto r 11 15## 21 chevrolet auto r 14 20## 22 chevrolet auto r 13 17## 23 chevrolet auto r 12 17## 24 chevrolet manual r 16 26## 25 chevrolet auto r 15 23## 26 chevrolet manual r 16 26## 27 chevrolet auto r 15 25## 28 chevrolet manual r 15 24## 29 chevrolet auto 4 14 19## 30 chevrolet auto 4 11 14## 31 chevrolet auto 4 11 15## 32 chevrolet auto 4 14 17## 33 chevrolet auto f 19 27## 34 chevrolet auto f 22 30## 35 chevrolet auto f 18 26## 36 chevrolet auto f 18 29## 37 chevrolet auto f 17 26## 38 dodge auto f 18 24## 39 dodge auto f 17 24## 40 dodge auto f 16 22## 41 dodge auto f 16 22## 42 dodge auto f 17 24## 43 dodge auto f 17 24## 44 dodge auto f 11 17## 45 dodge auto f 15 22## 46 dodge auto f 15 21## 47 dodge auto f 16 23## 48 dodge auto f 16 23## 49 dodge manual 4 15 19## 50 dodge auto 4 14 18## 51 dodge auto 4 13 17## 52 dodge manual 4 14 17## 53 dodge auto 4 14 19## 54 dodge auto 4 14 19## 55 dodge auto 4 9 12## 56 dodge manual 4 11 17## 57 dodge auto 4 11 15## 58 dodge auto 4 13 17## 59 dodge auto 4 13 17## 60 dodge auto 4 9 12## 61 dodge auto 4 13 17## 62 dodge auto 4 11 16## 63 dodge auto 4 13 18## 64 dodge auto 4 11 15## 65 dodge manual 4 12 16## 66 dodge auto 4 9 12## 67 dodge auto 4 13 17## 68 dodge auto 4 13 17## 69 dodge manual 4 12 16## 70 dodge manual 4 9 12## 71 dodge auto 4 11 15## 72 dodge manual 4 11 16## 73 dodge auto 4 13 17## 74 dodge auto 4 11 15## 75 ford auto r 11 17## 76 ford auto r 11 17## 77 ford auto r 12 18## 78 ford auto 4 14 17## 79 ford manual 4 15 19## 80 ford auto 4 14 17## 81 ford auto 4 13 19## 82 ford auto 4 13 19## 83 ford auto 4 13 17## 84 ford auto 4 14 17## 85 ford manual 4 14 17## 86 ford manual 4 13 16## 87 ford auto 4 13 16## 88 ford auto 4 13 17## 89 ford auto 4 11 15## 90 ford auto 4 13 17## 91 ford manual r 18 26## 92 ford auto r 18 25## 93 ford manual r 17 26## 94 ford auto r 16 24## 95 ford auto r 15 21## 96 ford manual r 15 22## 97 ford manual r 15 23## 98 ford auto r 15 22## 99 ford manual r 14 20## 100 honda manual f 28 33## 101 honda auto f 24 32## 102 honda manual f 25 32## 103 honda manual f 23 29## 104 honda auto f 24 32## 105 honda manual f 26 34## 106 honda auto f 25 36## 107 honda auto f 24 36## 108 honda manual f 21 29## 109 hyundai auto f 18 26## 110 hyundai manual f 18 27## 111 hyundai auto f 21 30## 112 hyundai manual f 21 31## 113 hyundai auto f 18 26## 114 hyundai manual f 18 26## 115 hyundai auto f 19 28## 116 hyundai auto f 19 26## 117 hyundai manual f 19 29## 118 hyundai manual f 20 28## 119 hyundai auto f 20 27## 120 hyundai auto f 17 24## 121 hyundai manual f 16 24## 122 hyundai manual f 17 24## 123 jeep auto 4 17 22## 124 jeep auto 4 15 19## 125 jeep auto 4 15 20## 126 jeep auto 4 14 17## 127 jeep auto 4 9 12## 128 jeep auto 4 14 19## 129 jeep auto 4 13 18## 130 jeep auto 4 11 14## 131 land rover auto 4 11 15## 132 land rover auto 4 12 18## 133 land rover auto 4 12 18## 134 land rover auto 4 11 15## 135 lincoln auto r 11 17## 136 lincoln auto r 11 16## 137 lincoln auto r 12 18## 138 mercury auto 4 14 17## 139 mercury auto 4 13 19## 140 mercury auto 4 13 19## 141 mercury auto 4 13 17## 142 nissan manual f 21 29## 143 nissan auto f 19 27## 144 nissan auto f 23 31## 145 nissan manual f 23 32## 146 nissan manual f 19 27## 147 nissan auto f 19 26## 148 nissan auto f 18 26## 149 nissan manual f 19 25## 150 nissan auto f 19 25## 151 nissan auto 4 14 17## 152 nissan manual 4 15 17## 153 nissan auto 4 14 20## 154 nissan auto 4 12 18## 155 pontiac auto f 18 26## 156 pontiac auto f 16 26## 157 pontiac auto f 17 27## 158 pontiac auto f 18 28## 159 pontiac auto f 16 25## 160 subaru manual 4 18 25## 161 subaru auto 4 18 24## 162 subaru manual 4 20 27## 163 subaru manual 4 19 25## 164 subaru auto 4 20 26## 165 subaru auto 4 18 23## 166 subaru auto 4 21 26## 167 subaru manual 4 19 26## 168 subaru manual 4 19 26## 169 subaru auto 4 19 26## 170 subaru auto 4 20 25## 171 subaru auto 4 20 27## 172 subaru manual 4 19 25## 173 subaru manual 4 20 27## 174 toyota manual 4 15 20## 175 toyota auto 4 16 20## 176 toyota auto 4 15 19## 177 toyota manual 4 15 17## 178 toyota auto 4 16 20## 179 toyota auto 4 14 17## 180 toyota manual f 21 29## 181 toyota auto f 21 27## 182 toyota manual f 21 31## 183 toyota auto f 21 31## 184 toyota auto f 18 26## 185 toyota manual f 18 26## 186 toyota auto f 19 28## 187 toyota auto f 21 27## 188 toyota manual f 21 29## 189 toyota manual f 21 31## 190 toyota auto f 22 31## 191 toyota auto f 18 26## 192 toyota manual f 18 26## 193 toyota auto f 18 27## 194 toyota auto f 24 30## 195 toyota auto f 24 33## 196 toyota manual f 26 35## 197 toyota manual f 28 37## 198 toyota auto f 26 35## 199 toyota auto 4 11 15## 200 toyota auto 4 13 18## 201 toyota manual 4 15 20## 202 toyota auto 4 16 20## 203 toyota manual 4 17 22## 204 toyota manual 4 15 17## 205 toyota auto 4 15 19## 206 toyota manual 4 15 18## 207 toyota auto 4 16 20## 208 volkswagen manual f 21 29## 209 volkswagen auto f 19 26## 210 volkswagen manual f 21 29## 211 volkswagen auto f 22 29## 212 volkswagen manual f 17 24## 213 volkswagen manual f 33 44## 214 volkswagen manual f 21 29## 215 volkswagen auto f 19 26## 216 volkswagen auto f 22 29## 217 volkswagen manual f 21 29## 218 volkswagen auto f 21 29## 219 volkswagen manual f 21 29## 220 volkswagen auto f 16 23## 221 volkswagen manual f 17 24## 222 volkswagen manual f 35 44## 223 volkswagen auto f 29 41## 224 volkswagen manual f 21 29## 225 volkswagen auto f 19 26## 226 volkswagen manual f 20 28## 227 volkswagen auto f 20 29## 228 volkswagen manual f 21 29## 229 volkswagen auto f 18 29## 230 volkswagen auto f 19 28## 231 volkswagen manual f 21 29## 232 volkswagen auto f 16 26## 233 volkswagen manual f 18 26## 234 volkswagen auto f 17 26 1str(mpg1) 123456## 'data.frame': 234 obs. of 5 variables:## $ manufacturer: chr &quot;audi&quot; &quot;audi&quot; &quot;audi&quot; &quot;audi&quot; ...## $ trans : chr &quot;auto&quot; &quot;manual&quot; &quot;manual&quot; &quot;auto&quot; ...## $ drv : chr &quot;f&quot; &quot;f&quot; &quot;f&quot; &quot;f&quot; ...## $ cty : int 18 21 20 21 16 18 18 18 16 20 ...## $ hwy : int 29 29 31 30 26 26 27 26 25 28 ... 1mean(mtcars$mpg)#평균 1## [1] 20.09062 1var(mtcars$mpg)#분산 1## [1] 36.3241 1sd(mtcars$mpg)#표준편차 1## [1] 6.026948 #기술통계-표준편차를 이용하여 그 데이터의 생김새를 상상할 수 있는 통계법#사분위수#IQR:1사분위수와 3사분위수#통계분석/머신러닝-&gt;왜곡된 이상치를 제거할 때 사용 1quantile(mtcars$mpg) 12## 0% 25% 50% 75% 100% ## 10.400 15.425 19.200 22.800 33.900 #통계 요약 구하기(p90) 1summary(iris) 1234567891011121314## Sepal.Length Sepal.Width Petal.Length Petal.Width ## Min. :4.300 Min. :2.000 Min. :1.000 Min. :0.100 ## 1st Qu.:5.100 1st Qu.:2.800 1st Qu.:1.600 1st Qu.:0.300 ## Median :5.800 Median :3.000 Median :4.350 Median :1.300 ## Mean :5.843 Mean :3.057 Mean :3.758 Mean :1.199 ## 3rd Qu.:6.400 3rd Qu.:3.300 3rd Qu.:5.100 3rd Qu.:1.800 ## Max. :7.900 Max. :4.400 Max. :6.900 Max. :2.500 ## Species ## setosa :50 ## versicolor:50 ## virginica :50 ## ## ## 1summary(iris$Sepal.Length) 12## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 4.300 5.100 5.800 5.843 6.400 7.900 1table(mpg1$trans) 123## ## auto manual ## 157 77 1table(mpg1$manufacturer) 1234567## ## audi chevrolet dodge ford honda hyundai jeep ## 18 19 37 25 9 14 8 ## land rover lincoln mercury nissan pontiac subaru toyota ## 4 3 4 13 5 14 34 ## volkswagen ## 27 1table(mpg1$trans,mpg1$manufacturer) 12345678## ## audi chevrolet dodge ford honda hyundai jeep land rover lincoln## auto 11 16 30 17 4 7 8 4 3## manual 7 3 7 8 5 7 0 0 0## ## mercury nissan pontiac subaru toyota volkswagen## auto 4 8 5 7 20 13## manual 0 5 0 7 14 14 #빈도의 비율 구하기 12a&lt;-table(mpg1$trans)prop.table(a) 123## ## auto manual ## 0.6709402 0.3290598 12b&lt;-table(mpg1$trans,mpg1$drv)prop.table(b) 1234## ## 4 f r## auto 0.32051282 0.27777778 0.07264957## manual 0.11965812 0.17521368 0.03418803 1prop.table(table(mpg1$manufacturer)) 1234567## ## audi chevrolet dodge ford honda hyundai jeep ## 0.07692308 0.08119658 0.15811966 0.10683761 0.03846154 0.05982906 0.03418803 ## land rover lincoln mercury nissan pontiac subaru toyota ## 0.01709402 0.01282051 0.01709402 0.05555556 0.02136752 0.05982906 0.14529915 ## volkswagen ## 0.11538462 #행과 열의 비율 형식 맞추기(각각의 행열을 1이되게) 1?prop.table 1## httpd 도움말 서버를 시작합니다 ... 완료 1prop.table(b,margin = 1)#행의 합이 1 1234## ## 4 f r## auto 0.4777070 0.4140127 0.1082803## manual 0.3636364 0.5324675 0.1038961 1prop.table(b,margin = 2)#열의 합이 1 1234## ## 4 f r## auto 0.7281553 0.6132075 0.6800000## manual 0.2718447 0.3867925 0.3200000 #소수점 아래 자리 지정 1round(0.322323,2)#round(정보값,자릿수) 1## [1] 0.32 1round(prop.table(table(mpg1$manufacturer)),2) 1234567## ## audi chevrolet dodge ford honda hyundai jeep ## 0.08 0.08 0.16 0.11 0.04 0.06 0.03 ## land rover lincoln mercury nissan pontiac subaru toyota ## 0.02 0.01 0.02 0.06 0.02 0.06 0.15 ## volkswagen ## 0.12 12a= table(mpg1$trans)a 123## ## auto manual ## 157 77 12b= prop.table(a)b 123## ## auto manual ## 0.6709402 0.3290598 1round(b,2) 123## ## auto manual ## 0.67 0.33 #시각화 이미지##보통은 ggplot2 패키지를 사용하지만 테이블 이미지를 이쁘게 사용하고 싶으면 gt table 패키지도 있다.고급","link":"/2022/06/21/220621/"},{"title":"220622","text":"라이브러리 불러오기1library(dplyr) 12## ## 다음의 패키지를 부착합니다: 'dplyr' 123## The following objects are masked from 'package:stats':## ## filter, lag 123## The following objects are masked from 'package:base':## ## intersect, setdiff, setequal, union 1library(ggplot2) 데이터 불러오기 데이터를 불러오세요 12exam_na &lt;- read.csv(&quot;data/exam_na.csv&quot;)str(exam_na) 123456## 'data.frame': 5 obs. of 5 variables:## $ id : int 1 2 3 4 5## $ sex : chr &quot;M&quot; &quot;F&quot; &quot;F&quot; &quot;M&quot; ...## $ korean : int 87 92 95 NA 87## $ english: int NA 95 92 84 NA## $ math : int 82 93 90 80 88 결측치 확인 후 빈도 구하기(1) 결측치 확인1is.na(exam_na) 123456## id sex korean english math## [1,] FALSE FALSE FALSE TRUE FALSE## [2,] FALSE FALSE FALSE FALSE FALSE## [3,] FALSE FALSE FALSE FALSE FALSE## [4,] FALSE FALSE TRUE FALSE FALSE## [5,] FALSE FALSE FALSE TRUE FALSE (2)결측치 빈도 구하기-table():빈도구하기 1table(is.na(exam_na)) 123## ## FALSE TRUE ## 22 3 1table(is.na(exam_na$korean)) 123## ## FALSE TRUE ## 4 1 1summary(is.na(exam_na)) 12345678## id sex korean english ## Mode :logical Mode :logical Mode :logical Mode :logical ## FALSE:5 FALSE:5 FALSE:4 FALSE:3 ## TRUE :1 TRUE :2 ## math ## Mode :logical ## FALSE:5 ## 1summary(exam_na) 12345678## id sex korean english math ## Min. :1 Length:5 Min. :87.00 Min. :84.00 Min. :80.0 ## 1st Qu.:2 Class :character 1st Qu.:87.00 1st Qu.:88.00 1st Qu.:82.0 ## Median :3 Mode :character Median :89.50 Median :92.00 Median :88.0 ## Mean :3 Mean :90.25 Mean :90.33 Mean :86.6 ## 3rd Qu.:4 3rd Qu.:92.75 3rd Qu.:93.50 3rd Qu.:90.0 ## Max. :5 Max. :95.00 Max. :95.00 Max. :93.0 ## NA's :1 NA's :2 결측치 처리 방법-제거하고 처리하기 -다른 값으로 대체하기 +평균입력 (1)결측치를 제외하고 분석하기 -p160 na.rm=T1mean(exam_na$korean,na.rm = T) 1## [1] 90.25 -na.omit() -결측치가 있는 행을 모두 제거 +가급적 쓰지 말것 -filter()활용 +is.na(korea) 1exam_na %&gt;% filter(is.na(korean)) 12## id sex korean english math## 1 4 M NA 84 80 -이번에는 !is.na(korean)을 적용한다 1exam_na %&gt;% filter(!is.na(korean)) 12345## id sex korean english math## 1 1 M 87 NA 82## 2 2 F 92 95 93## 3 3 F 95 92 90## 4 5 F 87 NA 88 (2) 결측치를 다른 값으로 대체하기 imputation 참고자료 A Solution to Missing Data: Imputation Using R R 결측값(NA) 제거, 대체 방법 이상치-데이터의 특정 값이 뭔가 “이상”이 있다. -case 1: 정해진 범주에서 벗어난 데이터 ex)2월 31일 -case 2: 숫자 /아웃라이어(outlier)/극단값 ex) 평균임금에서 삼성 이재용이 들어가면 안됨 12mpg1_out&lt;-read.csv(&quot;data/mpg1_out.csv&quot;)glimpse(mpg1_out) 12345## Rows: 234## Columns: 3## $ trans &lt;int&gt; 1, 2, 2, 1, 1, 2, 1, 2, 1, 2, 1, 1, 2, 1, 2, 1, 1, 1, 1, 3, 1, 1…## $ drv &lt;chr&gt; &quot;f&quot;, &quot;f&quot;, &quot;f&quot;, &quot;f&quot;, &quot;f&quot;, &quot;f&quot;, &quot;f&quot;, &quot;4&quot;, &quot;4&quot;, &quot;4&quot;, &quot;4&quot;, &quot;4&quot;, &quot;5&quot;,…## $ cty &lt;int&gt; 18, 21, 20, 21, 16, 18, 18, 18, 16, 20, 19, 15, 17, 17, 15, 15, … -trans의 빈도의갯수를 구하면 1이 몇개 2가 몇개 3이 몇개가 나온다 1table(mpg1_out$trans) 123## ## 1 2 3 ## 154 76 4 -만약 3을 그냥 제거하고 싶으면 다음과 같이 해도 된다. -mpg1_out %&gt;% filter(trans !=3) -ifelse란? -만약~라면 내가 무엇을 할 것이다.(가정법) -만약 trans의 값이 3이라면 결측치로 바꿔주세요.나머지는 그대로 유지하세요. 123mpg1_out$trans&lt;-ifelse(mpg1_out$trans == 3 ,NA ,mpg1_out$trans)table(is.na(mpg1_out$trans)) 123## ## FALSE TRUE ## 230 4 -결측치 제거 12result &lt;-mpg1_out %&gt;% filter(!is.na(trans))table(is.na(result$trans)) 123## ## FALSE ## 230 극단치 처리-숫자 데이터 boxplot() -boxlot() 함수를 통해서 극단치가 있는지 없는지 확인 가능 -IQR:3사분위-1사분위 -경계값:IQR+IQR1.5 상한/IQR-IQR1.5 123mpg1&lt;- read.csv(&quot;data/mpg1.csv&quot;)boxplot(mpg1$cty)boxplot(mpg1$cty)$stats 123456## [,1]## [1,] 9## [2,] 14## [3,] 17## [4,] 19## [5,] 26","link":"/2022/06/22/day0622/"},{"title":"0623","text":"복습-iris 데이터, sepal.length, sepal. width활용해서 종별로 산점도를 그리세요-제목과 x축 y축 제목을 변경하세요. +x축 길이 , y축 너비 12library(ggplot2)str(iris) 123456## 'data.frame': 150 obs. of 5 variables:## $ Sepal.Length: num 5.1 4.9 4.7 4.6 5 5.4 4.6 5 4.4 4.9 ...## $ Sepal.Width : num 3.5 3 3.2 3.1 3.6 3.9 3.4 3.4 2.9 3.1 ...## $ Petal.Length: num 1.4 1.4 1.3 1.5 1.4 1.7 1.4 1.5 1.4 1.5 ...## $ Petal.Width : num 0.2 0.2 0.2 0.2 0.2 0.4 0.3 0.2 0.2 0.1 ...## $ Species : Factor w/ 3 levels &quot;setosa&quot;,&quot;versicolor&quot;,..: 1 1 1 1 1 1 1 1 1 1 ... 1234567891011121314ggplot(iris, aes( x = Sepal.Length, y = Sepal.Width, colour =Species))+ geom_point()+ labs( title =&quot; 제목&quot;, x = &quot;길이&quot;, y = &quot;너비&quot;, )+ scale_color_manual( labels = c(&quot;setosa&quot;,&quot;versicolor&quot;,&quot;virginica&quot;), values = c(&quot;pink&quot;,&quot;orange&quot;,&quot;blue&quot;) )+ theme_classic() – 색깔을 부꿀 수 있는 명령어 scale_color_manual– 테마를 바꾸는 옵션 theme_classic()-&gt; 농도가 각각 다르게 함.– ggstatsplot라는 것도 통계그림 그리기에 좋음 통계 기술통계 :평균, 최촛값, 최댓값, 중간값 추론통계 : 변수간의 관계를 파악/새로운 사실을 발견(추정)+평균차이 검정 -수치데이터 -가설검정 : 평균의 차이를 검정 -남자의 평균 키와 여자의 평균 키는 차이가 있을 것이다. (남자 여자라는 그룹을 평균을 기준으로 비교 ) +교차분석(빈도분석) -범주 데이터 -가설검정 : 각 범주별 빈도를 활용하여 관계성을 검정 +상관관계 분석 -수치 데이터 -변수사이의 연관성을 수치로 표현 +단순 회귀분석 -y(종속변수)= a(기울기=회기계수)x(독립변수)+b(절편=상수항:의미없음) -가설검정-&gt;기온x(독립변수)이 판매량y(종속변수)에 긍정적(+)a영향을 준다.+다중 회귀분석 -y=a1x1+a2x2+a3x3….+b 통계 검정 P176 가설 -hypothesis –&gt;공부방법: 선행연구, 해당분석방법의 가설 설정 예시를 보고 연습 -연구:내가 궁금한 것을 찾는 것 ex)남자와 여자의 평균키가 차이가 있다. -귀무가설:궁금한 것의 반대내용(ex.남여 평균키 차이가 없다.) -대립가설:궁금한 사항이 여기로 옴(ex.남여 평균키 차이가 있다) -가설 검정에서 인정하는 유의 수준:5%,1%,0.1% 또는 10% -오차 범위 내에 있으면 차이가 크지 않음. 오차 범위 밖에 있으면 결과가 결정됨.(신뢰수준&lt;-&gt;유의수준) -유의수준&gt;0.05가 넘으면 데이터의 의미가 없어진다.귀무가설 실패 t.test-어떻게 데이터를 입력하는지 확인-p-value, 유의수준 0.05이상: 귀무가설(p&gt;0.05) 0.05이내: 대립가설(p&lt;0.05) 12mpg1&lt;-read.csv(&quot;data/mpg1.csv&quot;,stringsAsFactors = F)str(mpg1) 123456## 'data.frame': 234 obs. of 5 variables:## $ manufacturer: chr &quot;audi&quot; &quot;audi&quot; &quot;audi&quot; &quot;audi&quot; ...## $ trans : chr &quot;auto&quot; &quot;manual&quot; &quot;manual&quot; &quot;auto&quot; ...## $ drv : chr &quot;f&quot; &quot;f&quot; &quot;f&quot; &quot;f&quot; ...## $ cty : int 18 21 20 21 16 18 18 18 16 20 ...## $ hwy : int 29 29 31 30 26 26 27 26 25 28 ... -시각화 +여기서는 큰 차이를 알 수가 없다. 123library(ggplot2)ggplot(mpg1, aes(x = trans, y = cty)) + geom_boxplot() -t.test 검정 +귀무가설 :auto와 manual의 cty평균은 차이가 없다. +대립가설 :auto와 manual의 cty평균은 차이가 있다. 1t.test(data = mpg1, cty ~ trans) 1234567891011## ## Welch Two Sample t-test## ## data: cty by trans## t = -4.5375, df = 132.32, p-value = 1.263e-05## alternative hypothesis: true difference in means between group auto and group manual is not equal to 0## 95 percent confidence interval:## -3.887311 -1.527033## sample estimates:## mean in group auto mean in group manual ## 15.96815 18.67532 1#종속변수 ~ 독립변수 Y ~ X 반응변수 ~ 설명변수 –시각화한 것은 차이를 모르겠지만 통계를 통해 차이가 있음을 알 수 있다.(대립가설 성립)P-value&lt;0.5 -cf..두 그룹의 평균 차이 검정하기 전에 +사전 필수 검증요소가 있다.바로 등분산 검정 +등분산 검정 -&gt;두 그룹간의 분산이 비슷하면 –&gt;t.test(모수검정) -&gt;두 그룹간의 분산이 다르면–&gt;(비모수검정) -&gt;귀무가설: 두 그룹간의 분산이 비슷하다 p.value&gt;0.05 -&gt;대립가설: 두 그룹간의 분산이 다르다. 1var.test(data =mpg1, cty~trans) 1234567891011## ## F test to compare two variances## ## data: cty by trans## F = 0.73539, num df = 156, denom df = 76, p-value = 0.1101## alternative hypothesis: true ratio of variances is not equal to 1## 95 percent confidence interval:## 0.4912917 1.0719468## sample estimates:## ratio of variances ## 0.7353887 p.value&gt;0.05이므로 귀무가설 성립. 등분산으로 본다. -시각화 123ggplot(mpg1, aes(x = cty, fill = trans)) + # geom_histogram() + geom_density(alpha = 0.1) 교차 분석 -범주형 변수들이 관계가 있다는 것을 검정 -비율에 차이가 있는 지 검정 -교차분석 검정은 R의 chisq.test()함수로 진행 -귀무가설: tras에 따라 drv(4,f,r)의 (비율)차이가 없다. -대립가설: tras에 따라 drv의 차이가 있다. -빈도표/비율 -1빈도표 1table(mpg1$trans,mpg1$drv)#교차분석석 1234## ## 4 f r## auto 75 65 17## manual 28 41 8 -2비율 1prop.table(table(mpg1$trans,mpg1$drv),1) 1234## ## 4 f r## auto 0.4777070 0.4140127 0.1082803## manual 0.3636364 0.5324675 0.1038961 -auto 4륜구동이 47% -manual 전륜구동이 53% 가장 많음 -실제로 통계적으로 봤을 때 차이가 있는지 검정 -귀무가설: tras에 따라 drv(4,f,r)의 (비율)차이가 없다. -대립가설: tras에 따라 drv의 차이가 있다. 1chisq.test(mpg1$trans,mpg1$drv) 12345## ## Pearson's Chi-squared test## ## data: mpg1$trans and mpg1$drv## X-squared = 3.1368, df = 2, p-value = 0.2084 p.value&gt;0.05이므로 귀무가설 성립. 차이가 없다.","link":"/2022/06/23/day0623/"},{"title":"0624","text":"복습 -통계검정 평균차이검정 :(두그룹간의)수치데이터&amp;범주데이터-&gt;세그룹이상 평균 차이 검정(중급이상이라면) 비율차이검정(교차분석) :범주 데이터 상관관계 :수치 데이터 회귀 -통계검정을 할 때는 분석을 위한 데이터가 적절한지 검정 등분산 검정, 수치 데이터가 정규분포를 이루는가?(정규성 검정) -귀무가설 , 대립가설을 적절하게 설정 서울이 부산보다 잘산다(X)-&gt;서울의 평균임금과 부산의 평균임금이 차이가 있다. 선행연구(논문등)을 찾아 응용 -테스트 실시 +함수 t.test, chisq.test, cor.test를 통해 P.value를 찾음 +P.value -&gt;P.value&gt;0.5–&gt;귀무가설 지지 -&gt;P.value&lt;0.5–&gt;대립가설 지지 ##회귀분석(p.184) 회귀가 재일 중요하다~ 기초통계 : 특정한 결과에 영향을 주는 주 요인이 무엇인가? 이걸 찾는 것이 회귀 회귀분석과 종류 1세대 회귀 방법론: 다항회귀분석, 다중회귀분석, 포아송 회귀분석 등 2세대 회귀 방법론: 구조방정식 귀무가설 &amp; 대립가설 존재 귀무가설 : x(=독립변수)가 y(=종속변수)에 영향을 주지 않는다. 대립가설 : x가 y에 영향을 준다 -lm(종속변수~독립변수, data)-&gt;p.185 p.value로 대립가설을 확인 anova(분산분석)y=ax+b+a1x1+a2x2+a3x3….. 실무에서는 독립변수x1를 계속 변경해봐서 대립가설이 되는지 여부를 찾아본다.-&gt; 독립변수가 너무 많으면 전진소거법,후진소거법을 사용한다. R_Squared(결정계수)=설명력 = 0~1 -&gt;1로 수렴할 수록 설명력이 좋다. 123RA &lt;- lm(data=mtcars,mpg ~ disp)summary(RA) 123456789101112131415161718## ## Call:## lm(formula = mpg ~ disp, data = mtcars)## ## Residuals:## Min 1Q Median 3Q Max ## -4.8922 -2.2022 -0.9631 1.6272 7.2305 ## ## Coefficients:## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 29.599855 1.229720 24.070 &lt; 2e-16 ***## disp -0.041215 0.004712 -8.747 9.38e-10 ***## ---## Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1## ## Residual standard error: 3.251 on 30 degrees of freedom## Multiple R-squared: 0.7183, Adjusted R-squared: 0.709 ## F-statistic: 76.51 on 1 and 30 DF, p-value: 9.38e-10 -머신러닝, 인공지능 주 목적은 예측 Y= aX+b","link":"/2022/06/24/day0624/"},{"title":"파이선 기초문법:::","text":"Hello World 1print (&quot;Hello World&quot;) Hello World 주석처리 1줄 주석, 여러줄 주석처리: 여러줄 주석 처리 함수 또는 클래스를 문서화 할때 주로 사용 프로잭트 할 때 전체 공정100 코드 / 코드 문서화 / 한글작업 문서화 12345678910# print()함수 사용print(&quot;1줄주석&quot;)&quot;&quot;&quot;&quot;여러 줄의 주석에는 쌍따옴표 3개를 입력해주세요앞과 뒤로&quot;&quot;&quot;print(&quot;여러줄 주석&quot;) 1줄주석 여러줄 주석 변수 (Scalar) 자료형 Scalar형 Non-Scalar형 수치형 자료형 int, float 1234num_int =1print(num_int)print(type(num_int)) 1 &lt;class 'int'&gt; 1234num_float =0.1print(num_float)print(type(num_float)) 0.1 &lt;class 'float'&gt; Bool형 True,False cf)R: TRUE, FALSE 123bool_true =Trueprint(bool_true)print(type(bool_true)) True &lt;class 'bool'&gt; None 자료형 Null값- 값이 정해지지 않은 자료형 123none_x= Noneprint(none_x)print(type(none_x)) None &lt;class 'NoneType'&gt; 사칙연산 정수형 사칙연산, 실수형 사칙연산 결괏값의 자료형 정수형 사칙연산 +.-,*,/ 단 정수형 나누기는 실수형으로 바뀐다 123456a = 3b = 2print('a+b=',a+b)print('a-b=',a-b)print('a*b=',a*b)print('a/b=',a/b) a+b= 5 a-b= 1 a*b= 6 a/b= 1.5 실수형 사칙연산123456a = 1.5b = 2.5print('a+b=', a+b)print('a-b=',a-b)print('a*b=',a*b)print('a/b=',a/b) a+b= 4.0 a-b= -1.0 a*b= 3.75 a/b= 0.6 논리형 연산자 bool형 True와 False 값으로 정의 조건식 교집합(=and), 합집합(=or) 1234print(True and True)print(True and False)print(False and True)print(False and False) True False False False 1234print(True or True)print(True or False)print(False or True)print(False or False) True True True False 비교 연산자 비교연산자는 부등호를 의미한다. 1234print(4&gt;3) #참 = Trueprint(4&lt;3) #거짓 = Falseprint(4&gt;3 or 4&lt;3) # False True False True 논리형 &amp; 비교 연산자 응용 input() 형변환 데이터 타입을 바꾸는 것 123var = input(&quot;숫자를 입력하세요!&quot;)print(var)print(type(var)) 숫자를 입력하세요!3 3 &lt;class 'str'&gt; 123var = int(input(&quot;숫자를 입력하세요!&quot;))print(var)print(type(var)) 숫자를 입력하세요!1 1 &lt;class 'int'&gt; –계산이 안될 때 타입을 확인해보고 숫자로 형변환 해주세요 1234num1 = int(input(&quot;첫번째 숫자를 입력하세요.&quot;))num2 = int(input(&quot;두번째 숫자를 입력하세요.&quot;))num3 = int(input(&quot;세번째 숫자를 입력하세요.&quot;))num4 = int(input(&quot;네번째 숫자를 입력하세요.&quot;)) 첫번째 숫자를 입력하세요.40 두번째 숫자를 입력하세요.50 세번째 숫자를 입력하세요.60 네번째 숫자를 입력하세요.70 12var1 = num1 &lt;= num2 var2 = num3 &gt; num4 12print( var1 and var2) False String Non Scalar 12345print('hello World')print(&quot;hello World&quot;)print(&quot;'hello World'&quot;)print('&quot;hello World&quot;') hello World hello World 'hello World' &quot;hello World&quot; String Operators 문자열 연산자 +, * 가능 123str1 = &quot;Hello &quot;str2 = &quot;World &quot;print(str1 + str2) Hello World 12greet = str1 + str2print(greet * 2) Hello World Hello World 문자열 인덱싱 인덱싱은 0번째부터 시작 123greeting='hello kaggle'print(greeting)print(greeting[0]) hello kaggle h 123greeting= &quot;hello kabble&quot;i=int(input(&quot;숫자를 입력하세요&quot;))print(greeting[i]) 숫자를 입력하세요3 l 슬라이싱123456789greeting = &quot;hello kaggle&quot;#print(greeting[시작인덱스:끝인덱스-1])print(greeting[0:2])print(greeting[0:8])print(greeting[:8])print(greeting[6:8])print(greeting[0:10:2])print(greeting[0:10:3])print(greeting[0:10:4]) he hello ka hello ka ka hlokg hlkg hog 123alphabet_letter = &quot;1234567890qbcde&quot;print(alphabet_letter[0:14:2])print(alphabet_letter[0:14:3]) 13579qc 1470c 인덱스가 넘어갈때 뜨는 에러 메세지 12greeting = &quot;hello Kabble&quot;print(greeting[100]) --------------------------------------------------------------------------- IndexError Traceback (most recent call last) &lt;ipython-input-84-40fb216685e4&gt; in &lt;module&gt;() 1 greeting = &quot;hello Kabble&quot; ----&gt; 2 print(greeting[100]) IndexError: string index out of range 문자열 관련 메서드 split() sort() etc 리스트 [] (대괄호)로 표시 [item1, item2, item3..] 1234567a = []#빈 리스트a_func = list() # 빈 리스트 생성b= [1] #숫자요소c=['apple'] #문자요소d = [1,2,['apple'],'apple']#리스트 자체를 리스트 요소로 만들 수도 있다print(a,b,c)print(d) [] [1] ['apple'] [1, 2, ['apple'], 'apple'] 리스트 값 수정하기 리스트 값 수정 123a =[ 0,1,2]a[0] = &quot;아무값&quot;print (a) ['아무값', 1, 2] 메서드 사용 123456789a = [100, 200, 300]a.append(400)print(a)a.append([500,600])print(a)a.extend([700,800])print(a) [100, 200, 300, 400] [100, 200, 300, 400, [500, 600]] [100, 200, 300, 400, [500, 600], 700, 800] insert(인덱스 위치, 값) 123a = [100,200,300 ]a.insert(1,1000)print(a) [100, 1000, 200, 300] remover() 리스트에서 첫 번째로 나오는 x를 삭제하는 함수이다 123a = [1,2,1,2]a.remove(1)print(a) [2, 1, 2] del 123456a = [0,1,2,3,4,5,6,7,8,9]del a[1]print(a)del a[0:2]print(a) [0, 2, 3, 4, 5, 6, 7, 8, 9] [3, 4, 5, 6, 7, 8, 9] pop() : pop()은 리스트의 맨 마지막 요소를 돌려주고 그 요소는 삭제한다 1234567a = [1,2,3,4,5]rem = a.pop(1)print(a)print(rem)x = a.pop()print(a)print(x) [1, 3, 4, 5] 2 [1, 3, 4] 5 clear() : 리스트 내 모든 값 삭제 index(“값”) : 값의 위치를 불러옴 1234a= [1,4,5,2,3]b= [&quot;철수&quot;,&quot;영희&quot;,&quot;길동&quot;]print(a.index(3))print(b.index(&quot;영희&quot;)) 4 1 sort : 리스트의 정렬 123456a= [1,4,5,2,3]a.sort()print(a)help(list.sort)#help(list.index) [1, 2, 3, 4, 5] Help on method_descriptor: sort(self, /, *, key=None, reverse=False) Stable sort *IN PLACE*. 튜플 면접질문 :리스트와 튜플의 차이가 뭔가요? 형태로 리스트는 [ ]으로 둘러싸지만 튜플은 ( )으로 둘러싼다. 리스트는 그 값의 생성, 삭제, 수정이 가능하지만 튜플은 그 값을 바꿀 수 없다 1234567891011tuple1 = (0)tuple2 = (0,)tuple3 = 0, 1, 2print(type(tuple1))print(type(tuple2))print(type(tuple2))print(tuple1)print(tuple2)print(tuple3) &lt;class 'int'&gt; &lt;class 'tuple'&gt; &lt;class 'tuple'&gt; 0 (0,) (0, 1, 2) 튜플은 소가로()가 있고 ,(쉼표)가 있다. 정수는 숫자만 있다. 123a = (0,1,2,3,'a')#del a[4]#print(a)-&gt;에러가 뜨며 tuple' object doesn't support item deletion 튜플(=리스트)연산자 문자열 연산자 +, * 1234t1 = (0,1,2)t2 = (3,4,5)print(t1 +t2)print(t1 * 2) (0, 1, 2, 3, 4, 5) (0, 1, 2, 0, 1, 2) 딕셔너리 Dictionary 키(Key)와 값(value)으로 구성됨 슬라이싱!= (값의 순서가 존재해야 됨) 그러나 딕셔너리는 순서라는 개념자체가 존재하지 않기에 슬라이싱이 안됨(cf.0부터 시작된다는 개념 자체도 없으니 조심) 1234567891011temp_dict = { #'키' : '값' 'teacher' : 'evan', 'class' : 15, 'students' : ['s1', 's2','s3']}print(temp_dict[&quot;teacher&quot;])print(temp_dict['class'])print(temp_dict['students']) evan 15 ['s1', 's2', 's3'] key()값만 출 1list(temp_dict.keys()) ['teacher', 'class', 'students'] values()값만 출 1temp_dict.values() dict_values(['evan', 15, ['s1', 's2', 's3']]) items() key-value 쌍으로, list와 tuple 형태로 반환 1temp_dict.items() dict_items([('teacher', 'evan'), ('class', 15), ('students', ['s1', 's2', 's3'])]) 조건문 잊지말고 :를 찍으세요 123456789a = int(input(&quot;숫자를 입력하세요&quot;))if a&gt;5: print(&quot;a는 5보다 크다&quot;)elif a&gt;0: print(&quot;a는 0보다 크다&quot;)elif a&gt;-5: print(&quot;a는 -5보다 크다&quot;)else: print(&quot;a는 매우 작다&quot;) 숫자를 입력하세요2 a는 0보다 크다 반복문1234#Hello World 3번 출력하세요for idx in range(3): print(idx+1) print(&quot;Hello World, &quot;) 1 Hello World, 2 Hello World, 3 Hello World, 123#가독성이 좋게 하려고 인덱스의 숫자를 앞에 넣을 수도 있다.for idx in range(3): print(idx+1,&quot;Hello World, &quot;) 1 Hello World, 2 Hello World, 3 Hello World, for loop if 조건문 사용 문자열 ,리스트 등–&gt;시퀀스 데이터(반복문 사용가능) 123456a= &quot;kaggle&quot;# g가 시작하면 반복문을 멈추세요&quot;for x in a: print(x) if x== 'g': break k a g 123456a= &quot;kaggle&quot;# print(x)의 위치에 따라 출력이 달라진다&quot;for x in a: if x== 'g': breakprint(x) g enumerate() 123alphabets = ['A','B','c']for index, value in enumerate(alphabets): print(index,value) 0 A 1 B 2 c 리스트 컴프리헨션(list comprehension) 1&quot;a&quot; in &quot;kiwi&quot; False 123456789fruits = ['apple','kiwi','mango']newlists= []#알파벳 a가 있는 과일만 추출 후 새로운 리스트에 담기for fruit in fruits: #print(fruit) if &quot;a&quot; in fruit : newlists.append(fruit)print(newlists) ['apple', 'mango'] 123#리스트 컴프리헨션으로 표현하면newlist = [fruit for fruit in fruits if 'a' in fruit]print(newlist) ['apple', 'mango']","link":"/2022/06/27/day0627/"},{"title":"파이선 기초문법2+Numpy","text":"##복습-반복문 연습 for loop and while loop 123for i in range(3): print(&quot;Hellow World&quot;) print(&quot;안녕하세요&quot;) Hellow World 안녕하세요 Hellow World 안녕하세요 Hellow World 안녕하세요 123456for i in range(100): print(&quot;No:&quot;,i+1) if i == 10 : break print(&quot;Hellow World&quot;) print(&quot;안녕하세요&quot;) No: 1 Hellow World 안녕하세요 No: 2 Hellow World 안녕하세요 No: 3 Hellow World 안녕하세요 No: 4 Hellow World 안녕하세요 No: 5 Hellow World 안녕하세요 No: 6 Hellow World 안녕하세요 No: 7 Hellow World 안녕하세요 No: 8 Hellow World 안녕하세요 No: 9 Hellow World 안녕하세요 No: 10 Hellow World 안녕하세요 No: 11 12345&quot;k&quot; in &quot;kaggle&quot;if &quot;k&quot; == &quot;a&quot;: print(&quot;출력이 되나요?&quot;)else: print(&quot;출력이 안됨&quot;) 출력이 안됨 123456a = &quot;Kaggle&quot;for i in a: print(i) if i == &quot;a&quot;: break K a 리스트의 값이 존재 전체 총합구하기 12345678910numbers = [3,2,3,4,5]sum = 0for num in numbers: print(&quot;number:&quot;,num) sum = sum + num print(&quot;total:&quot;,sum)print(&quot;-----최종결괏값------&quot;)print(sum) number: 3 total: 3 number: 2 total: 5 number: 3 total: 8 number: 4 total: 12 number: 5 total: 17 -----최종결괏값------ 17 12345678910111213fruits = ['apple', 'kiwi', 'mango']newlist = []# apple : a가 있나요? 있으면 newlist에 추가하세요# kiwi에는 a가 있나요 없으면 그냥 넘어가요# mango : a가 있나요? 그럼 newlist에 추가하세요for fruit in fruits: print(&quot;조건문 밖:&quot;, fruit) if &quot;a&quot; in fruit: print(&quot;조건문 안:&quot;,fruit) newlist.append(fruit)print(newlist) 조건문 밖: apple 조건문 안: apple 조건문 밖: kiwi 조건문 밖: mango 조건문 안: mango ['apple', 'mango'] While Loop 분석할 때는 거의 사용안되고 개발할 때 사용 123456i=1while i&lt;10: #조건식 :참일때만 반복문 코드가 돔 (무한루프가 될 수 있다) # 코드 print(i) i+=1 #1씩 증감 하여 10이 되면 거짓이 되면 멈춤 #1-=1 #1씩 감소 1 2 3 4 5 6 7 8 9 사용자 정의 함수 내가 필요에 의해 직접 함수를 작성 함수 문서화 키워드 : Docsting -&gt;””” 내용 “”” 함수에는 반드시 설명이 들어가야 된다. 오늘 배운 것 중에 두번째로 중요한 내용!!! 123def 함수명(param1, param2): #코드 return None 1234567def add(a = 0,b = 1): # c = a + b # return c return a + bprint(add(a = 5, b = 4))print(add()) 9 1 사칙연산 사용자 정의 함수 만들기 123456789101112131415def de(a, b) : return a + bprint(de(1,2))def bbe(a, b) : return a - bprint(bbe(1,2))def kop(a, b) : return a * bprint(kop(1,2))def na(a, b) : return a / bprint(na(1,2))def mean(a , b): return (a + b) / 2print(mean(5,10)) 3 -1 2 0.5 7.5 123456a = int(input('첫번째수를 넣으세요'))b = int(input('두번째수를 넣으세요'))def mean (a, b): return (a + b)/2print(mean(a,b)) 첫번째수를 넣으세요5 두번째수를 넣으세요10 7.5 123456789101112131415def subtract(a, b): &quot;&quot;&quot; a,b를 빼는 함수 parameters: a(int): int형 숫자 a가 입력 b(int): int형 숫자 b가 입력 return: int :반환값 &quot;&quot;&quot; return a-bprint(subtract(a= 5, b= 10))print(subtract.__doc__) -5 a,b를 빼는 함수 parameters: a(int): int형 숫자 a가 입력 b(int): int형 숫자 b가 입력 return: int :반환값 Numpy 내장 모듈(=라이브러리=R의 패키지)이 아닌 별도 라이브러리를 설치해야 함 별도 라이브러리 설치가 필요(구글코랩은 불필요) 12import numpy as npprint(np.__version__) 1.21.6 12345temp = [1,2,3]temp_array = np.array(temp) # 리스트를 배열로 변환print (temp_array)print (type(temp_array)) [1 2 3] &lt;class 'numpy.ndarray'&gt; 사칙연산1234567891011math_score = [90, 80, 100]eng_score= [80, 90,100]#print(math_score+eng_score)math_array = np.array(math_score)eng_array= np.array(eng_score)total= math_array + eng_arrayprint(total)print(type(total)) [170 170 200] &lt;class 'numpy.ndarray'&gt; 집계함수123print(np.min(total))print(np.max(total))print(np.sum(total)) 170 200 540 차원 확인 배열의 차원 확인 필요 12345# 1차원 배열 생성temp_arr= np.array([1,2,3])print(temp_arr.shape)print(temp_arr.ndim) #차원을 나타냄print(temp_arr) (3,) 1 [1 2 3] 12345# 2차원 배열 생성temp_arr = np.array([[1,2,3],[4,5,6]])print(temp_arr.shape)print(temp_arr.ndim) #차원을 나타냄print(temp_arr) (2, 3) 2 [[1 2 3] [4 5 6]] 12345# 3차원 배열-&gt; 이미지temp_arr = np.array([[[1,2,3],[4,5,6]],[[7,8,9],[10,11,12]]])print(temp_arr.shape)print(temp_arr.ndim)print(temp_arr) (2, 2, 3) 3 [[[ 1 2 3] [ 4 5 6]] [[ 7 8 9] [10 11 12]]] 배열 생성의 다양한 방법들 모두 0으로 채운다 12import numpy as np print(np.__version__) 1.21.6 12temp_arr = np.zeros((4,2,3))temp_arr array([[[0., 0., 0.], [0., 0., 0.]], [[0., 0., 0.], [0., 0., 0.]], [[0., 0., 0.], [0., 0., 0.]], [[0., 0., 0.], [0., 0., 0.]]]) 모두 1로 채운다. 12temp_arr = np.ones((2,3))temp_arr array([[1., 1., 1.], [1., 1., 1.]]) 임의의 상수값으로 채운다(3개의 1행 3열자료에 100을 넣었다)3x1x3배열이라고 말한다. 12temp_arr = np.full((3,3),100.1)temp_arr array([[100.1, 100.1, 100.1], [100.1, 100.1, 100.1], [100.1, 100.1, 100.1]]) 2개의 3행 4열 자료에 100을 넣었다. 12temp_arr = np.full((2,3,4),100)temp_arr array([[[100, 100, 100, 100], [100, 100, 100, 100], [100, 100, 100, 100]], [[100, 100, 100, 100], [100, 100, 100, 100], [100, 100, 100, 100]]]) 최소, 최대 숫자의 범위를 정해두고,각 구간별로 값을 생성 .linspace(시작값, 종료값, 간격수) 12temp_arr = np.linspace(5,10,8)temp_arr array([ 5. , 5.71428571, 6.42857143, 7.14285714, 7.85714286, 8.57142857, 9.28571429, 10. ]) 반복문 시 , 자주 등장하는 배열 12temp_arr = np.arange(1,11,1)temp_arr array([ 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]) 난수 생성123from numpy import randomx = random.rand()print(x) 0.09081438142620213 123import numpyx = numpy.random.rand()print(x) 0.17530497899866515 위의 두가지는 같은 내용이다.랜덤함수의 위치를 명시해준다. 랜덤 정수값 추출 12345from numpy import random#x = random.randint(100, size = (5))x = random.randint(100, size = (3,5))print(x)print(type(x)) [[ 4 26 62 97 49] [68 51 15 95 55] [37 91 98 47 8]] &lt;class 'numpy.ndarray'&gt; 12345from numpy import random#x = random.randint(100, size = (5))x = random.randint(100, size = (3,2,4))print(x)print(type(x)) [[[10 16 7 84] [49 15 85 24]] [[78 15 34 79] [95 66 42 17]] [[69 9 8 4] [41 78 93 37]]] &lt;class 'numpy.ndarray'&gt; 랜덤 배열 실숫값 추출 1234from numpy import randomx = random.rand(2,5)print(x)print(type(x)) [[0.4704299 0.23477923 0.28244335 0.71846304 0.75368955] [0.41210023 0.91448093 0.15992748 0.33859588 0.52543335]] &lt;class 'numpy.ndarray'&gt; Numpy 사칙 연산123import numpy as nparray_01 = np.array([1,2,3])array_02 = np.array([10,20,30]) 123456789101112131415161718192021# 덧셈newArr = np.add(array_01,array_02)print(newArr)# 뺄셈newArr = np.subtract(array_01,array_02)print(newArr)# 곱셈newArr = np.multiply(array_01,array_02)print(newArr)# 나눗셈newArr = np.divide(array_01,array_02)print(newArr)# 거듭제곱array_01 = np.array([1,2,3])array_02 = np.array([2,4,2])newArr = np.power(array_01,array_02)print(newArr) [11 22 33] [ -9 -18 -27] [10 40 90] [0.1 0.1 0.1] [ 1 16 9] 소숫점 정렬 소숫점을 정렬하는 다양한 방법 1234567# 소숫점 제거import numpy as nptemp_arr =np.trunc([-1.91,1.9])print(temp_arr) temp_arr =np.fix([-1.23,1.9])print(temp_arr) [-1. 1.] [-1. 1.] 123# 임의의 소숫점 자리에서 반올림temp_arr = np.around([-1.2345667,1.23232323],5)print(temp_arr) [-1.23457 1.23232] 123#소숫점 모두 내림temp_arr = np.floor([-1.2345667,1.23232323])print(temp_arr) [-2. 1.] 123#소숫점 모두 올림temp_arr = np.ceil([-1.2345667,1.23232323])print(temp_arr) [-1. 2.] 조건식 pandas 가공 numpy 조건식 하나(단일)의 조건식(-&gt;np.where) 다중 조건식(이번 시간에 가장 중요함!!!!) 12temp_arr = np.arange(10)temp_arr array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]) 12# 5보다 작으면 원 값 유지# 5보다 크면 곱하기 10을 유지해줌 12#np.where(조건식, 참일 때, 거짓일 때)np.where(temp_arr&lt; 5, temp_arr, temp_arr*10) array([ 0, 1, 2, 3, 4, 50, 60, 70, 80, 90]) 1234567temp_arr = np.arange(10)# temp_arrcond_list = [temp_arr &gt; 5, temp_arr &lt; 2]choice_list = [temp_arr * 2, temp_arr + 100]#np.select(조건식 리스트, 결과값 리스트, default = )np.select(cond_list, choice_list, default = temp_arr) array([100, 101, 2, 3, 4, 5, 12, 14, 16, 18]) Reshape 배열의 차원 또는 크기를 바꿈 곱셈이 중요 1234import numpy as nptemp_array = np.ones((3,4))print(temp_array.shape)print(temp_array) (3, 4) [[1. 1. 1. 1.] [1. 1. 1. 1.] [1. 1. 1. 1.]] 값을 임으로 바꿔서 순서를 확인바란다. 12temp_array= random.randint(100, size = (3,1,4))temp_array array([[[34, 28, 61, 38]], [[49, 30, 47, 52]], [[78, 13, 72, 62]]]) 3개의 4열을 만들었다. 그렇기에 아래도 12의 공배수로 적어줘야 된다. 123after_reshape = temp_array.reshape(2,2,3)print(after_reshape.shape)print(after_reshape) (2, 2, 3) [[[34 28 61] [38 49 30]] [[47 52 78] [13 72 62]]] (2,2,?)를 했을 때 나머지 내용을 모르겠으면 (-1)을 적어주면 편하다. 123after_reshape = temp_array.reshape(2,2,-1)print(after_reshape.shape)print(after_reshape) (2, 2, 3) [[[34 28 61] [38 49 30]] [[47 52 78] [13 72 62]]] 브로드 캐스팅 서로 다른 크기의 배열을 계산할 때의 기본적인 규칙을 의미 판다스12import pandas as pdprint(pd.__version__) 1.3.5 1234567temp_dict = { 'col1' : [1,2], 'col2' : [3,4] }df = pd.DataFrame(temp_dict)print(df)print(type(df)) col1 col2 0 1 3 1 2 4 &lt;class 'pandas.core.frame.DataFrame'&gt; 구글 드라이브 연동12from google.colab import drivedrive.mount('/content/drive') Mounted at /content/drive 1234567DATA_PATH= '/content/drive/MyDrive/Colab Notebooks/Human_ai/Basic/Chapter 3. pandas/data/'print(DATA_PATH + 'Lemonade2016.csv')lemonade = pd.read_csv(DATA_PATH + 'Lemonade2016.csv')#covid_df = pd.read_csv(DATA_PATH + 'owid-covid-data.csv')lemonade.info() /content/drive/MyDrive/Colab Notebooks/Human_ai/Basic/Chapter 3. pandas/data/Lemonade2016.csv &lt;class 'pandas.core.frame.DataFrame'&gt; RangeIndex: 32 entries, 0 to 31 Data columns (total 7 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 Date 31 non-null object 1 Location 32 non-null object 2 Lemon 32 non-null int64 3 Orange 32 non-null int64 4 Temperature 32 non-null int64 5 Leaflets 31 non-null float64 6 Price 32 non-null float64 dtypes: float64(2), int64(3), object(2) memory usage: 1.9+ KB 1","link":"/2022/06/28/day0628/"},{"title":"","text":"title: ‘머신러닝’ date: ‘2022-06-29 16:00’ 파이썬 주요 라이브러리 Machine learning 정형데이터 사이킷런(Scikit-Learn) Deep Learming 비정형데이터 TensorFlow(구글)vs Pytorch(페이스북) 혼공머신은 텐서플로우 실제 상용 서비스(텐서플로우) Vs R&amp;D(파이터치-&gt;넘파이와 비슷한 용어) 생선분류(p.45) 도미, 곤들매기, 농어 등등 목표: 이생선들을 프록그램을 통해 분류한다 30cm 이상이면 도미라고 알려줘라. 12345fish_length = int(input('길이의 숫자로 넣어주세요:'))if fish_length &gt;=30: print(&quot;도미&quot;)else: print(&quot;몰라&quot;) 길이의 숫자로 넣어주세요:20 몰라 도미데이터(p.47) 데이터 수집1234# 도미 길이bream_length = [25.4, 26.3, 26.5, 29.0, 29.0, 29.7, 29.7, 30.0, 30.0, 30.7, 31.0, 31.0, 31.5, 32.0, 32.0, 32.0, 33.0, 33.0, 33.5, 33.5, 34.0, 34.0, 34.5, 35.0, 35.0, 35.0, 35.0, 36.0, 36.0, 37.0, 38.5, 38.5, 39.5, 41.0, 41.0]# 도미 무게bream_weight = [242.0, 290.0, 340.0, 363.0, 430.0, 450.0, 500.0, 390.0, 450.0, 500.0, 475.0, 500.0, 500.0, 340.0, 600.0, 600.0, 700.0, 700.0, 610.0, 650.0, 575.0, 685.0, 620.0, 680.0, 700.0, 725.0, 720.0, 714.0, 850.0, 1000.0, 920.0, 955.0, 925.0, 975.0, 950.0] 데이터 가공 여기서는 생략 데이터 시각화 여러 인사이트를 확인하기 위해 시각화, 통계 수치 계산 탐색적 자료 분석(EDA : Exploratory Data Analysis) 123456import matplotlib.pyplot as pltplt.scatter(bream_length, bream_weight)plt.xlabel('length')plt.ylabel('weight')plt.show() 파이썬 시각화는 객체 지향으로 한다. 왜냐하면 좀 더 이쁘고 아름답게 다듬기 위해서 캐글 시각화 참고할 때 아래와 같이 하는 사람이 많음 1234567import matplotlib.pyplot as pltfig, ax = plt.subplots()ax.scatter(bream_length, bream_weight)ax.set_xlabel('length')ax.set_ylabel('weight')plt.show() 빙어 데이터 준비하기 12smelt_length = [9.8, 10.5, 10.6, 11.0, 11.2, 11.3, 11.8, 11.8, 12.0, 12.2, 12.4, 13.0, 14.3, 15.0]smelt_weight = [6.7, 7.5, 7.0, 9.7, 9.8, 8.7, 10.0, 9.9, 9.8, 12.2, 13.4, 12.2, 19.7, 19.9] 123456fig, ax = plt.subplots()ax.scatter(bream_length,bream_weight)ax.scatter(smelt_length,smelt_weight)ax.set_xlabel('length')ax.set_ylabel('weight')plt.show() 두개의 리스트 합치기 12length = bream_length + smelt_lengthweight = bream_weight + smelt_weight 2차원 리스트로 만든다. 12fish_data = [[l,w] for l,w in zip(length, weight)]fish_data [0:5] #5개만 추출 [[25.4, 242.0], [26.3, 290.0], [26.5, 340.0], [29.0, 363.0], [29.0, 430.0]] 생선의 길이와 무게를 보고 빙어와 도미를 구분하고 싶다. 라벨링을 해준다=지도해준다=지도학습 12fish_target = [1]* 35 + [0]*14print(fish_taget) [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] 123456from sklearn.neighbors import KNeighborsClassifier# 클래스 인스턴스화kn = KNeighborsClassifier()# 모형학습kn.fit(fish_data, fish_target) KNeighborsClassifier() 12# 예측 정확도kn.score(fish_data, fish_target) 1.0 실제 예측을 해보자 새로운 물고기 도착 길이 :30 몸무게:600 123456789ac_length = int(input(&quot;물고기 길이를 입력하세요..&quot;))ac_weight= int(input(&quot;물고기 무게를 입력하세요..&quot;))preds = int(kn.predict([[ac_length, ac_weight]]))print(preds)if preds == 1: print(&quot;도미&quot;)else: print(&quot;빙어&quot;) 물고기 길이를 입력하세요..35 물고기 무게를 입력하세요..100 0 빙어","link":"/2022/06/29/day29_ml/"},{"title":"판다스","text":"라이브러리 불러오기1234import pandas as pdimport numpy as npprint(&quot;pandas version:&quot;,pd.__version__)print(&quot;numpy verson:&quot;,np.__version__) pandas version: 1.3.5 numpy verson: 1.21.6 데이터 불러오기 구글 드라이브에 있는 데이터를 불러올 때 데이터는 존재해야 함 12from google.colab import drivedrive.mount('/content/drive') Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(&quot;/content/drive&quot;, force_remount=True). 레모네이드 데이터를 불렀는데 결측치가 있음. 123456DATA_PATH= '/content/drive/MyDrive/Colab Notebooks/Human_ai/Basic/Chapter 3. pandas/data/Lemonade2016.csv'print(DATA_PATH)lemonade = pd.read_csv(DATA_PATH)lemonade.info() /content/drive/MyDrive/Colab Notebooks/Human_ai/Basic/Chapter 3. pandas/data/Lemonade2016.csv &lt;class 'pandas.core.frame.DataFrame'&gt; RangeIndex: 32 entries, 0 to 31 Data columns (total 7 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 Date 31 non-null object 1 Location 32 non-null object 2 Lemon 32 non-null int64 3 Orange 32 non-null int64 4 Temperature 32 non-null int64 5 Leaflets 31 non-null float64 6 Price 32 non-null float64 dtypes: float64(2), int64(3), object(2) memory usage: 1.9+ KB 데이터 맛보기 헤드 테일은 숫자를 안넣어주면 5개만 보임 1print(lemonade.head()) Date Location Lemon Orange Temperature Leaflets Price 0 7/1/2016 Park 97 67 70 90.0 0.25 1 7/2/2016 Park 98 67 72 90.0 0.25 2 7/3/2016 Park 110 77 71 104.0 0.25 3 7/4/2016 Beach 134 99 76 98.0 0.25 4 7/5/2016 Beach 159 118 78 135.0 0.25 1print(lemonade. tail()) Date Location Lemon Orange Temperature Leaflets Price 27 7/27/2016 Park 104 68 80 99.0 0.35 28 7/28/2016 Park 96 63 82 90.0 0.35 29 7/29/2016 Park 100 66 81 95.0 0.35 30 7/30/2016 Beach 88 57 82 81.0 0.35 31 7/31/2016 Beach 76 47 82 68.0 0.35 기술통계량 보는 함수 describe() 1print(lemonade.describe()) Lemon Orange Temperature Leaflets Price count 32.000000 32.000000 32.000000 31.000000 32.000000 mean 116.156250 80.000000 78.968750 108.548387 0.354688 std 25.823357 21.863211 4.067847 20.117718 0.113137 min 71.000000 42.000000 70.000000 68.000000 0.250000 25% 98.000000 66.750000 77.000000 90.000000 0.250000 50% 113.500000 76.500000 80.500000 108.000000 0.350000 75% 131.750000 95.000000 82.000000 124.000000 0.500000 max 176.000000 129.000000 84.000000 158.000000 0.500000 레몬보다 오랜지가 표준편차가 작으니 판매차가 거의 없다.(leaflets전단지) 범주형 데이터 빈도수 구하기 데이터[‘컬럼’].갯수함수 -&gt; 시리즈 함수였다!! 1lemonade['Location'].value_counts() Beach 17 Park 15 Name: Location, dtype: int64 1print(type(lemonade['Location'])) &lt;class 'pandas.core.series.Series'&gt; 행과 열 다루기 sold(판매량) 컬럼(=피처=feature)을 추가 12lemonade['Sold'] = 0print(lemonade.head(3)) Date Location Lemon Orange Temperature Leaflets Price Sold 0 7/1/2016 Park 97 67 70 90.0 0.25 0 1 7/2/2016 Park 98 67 72 90.0 0.25 0 2 7/3/2016 Park 110 77 71 104.0 0.25 0 12lemonade['Sold'] = lemonade['Lemon'] + lemonade['Orange']print(lemonade.head(3)) Date Location Lemon Orange Temperature Leaflets Price Sold 0 7/1/2016 Park 97 67 70 90.0 0.25 164 1 7/2/2016 Park 98 67 72 90.0 0.25 165 2 7/3/2016 Park 110 77 71 104.0 0.25 187 Revenue = 단가 x 판매량 12lemonade['Revenue'] = 0print(lemonade[['Sold','Price']].head(3)) Sold Price 0 164 0.25 1 165 0.25 2 187 0.25 12lemonade['Revenue'] = lemonade['Price'] * lemonade['Sold']print(lemonade[['Revenue', 'Price', 'Sold']].head()) Revenue Price Sold 0 41.00 0.25 164 1 41.25 0.25 165 2 46.75 0.25 187 3 58.25 0.25 233 4 69.25 0.25 277 drop 함수 행은 axis=1 열은 axis=0 을 넣어줘야 한다 1234#컬럼 제거col_drop = lemonade.drop('Sold', axis=1)print(col_drop.head()) Date Location Lemon Orange Temperature Leaflets Price Revenue 0 7/1/2016 Park 97 67 70 90.0 0.25 41.00 1 7/2/2016 Park 98 67 72 90.0 0.25 41.25 2 7/3/2016 Park 110 77 71 104.0 0.25 46.75 3 7/4/2016 Beach 134 99 76 98.0 0.25 58.25 4 7/5/2016 Beach 159 118 78 135.0 0.25 69.25 123#행 제거row_drop = lemonade.drop([0,2], axis =0)print(row_drop.head()) Date Location Lemon Orange Temperature Leaflets Price Sold \\ 1 7/2/2016 Park 98 67 72 90.0 0.25 165 3 7/4/2016 Beach 134 99 76 98.0 0.25 233 4 7/5/2016 Beach 159 118 78 135.0 0.25 277 5 7/6/2016 Beach 103 69 82 90.0 0.25 172 6 7/6/2016 Beach 103 69 82 90.0 0.25 172 Revenue 1 41.25 3 58.25 4 69.25 5 43.00 6 43.00 데이터 인덱싱1print(lemonade[4:7]) Date Location Lemon Orange Temperature Leaflets Price Sold \\ 4 7/5/2016 Beach 159 118 78 135.0 0.25 277 5 7/6/2016 Beach 103 69 82 90.0 0.25 172 6 7/6/2016 Beach 103 69 82 90.0 0.25 172 Revenue 4 69.25 5 43.00 6 43.00 행의 인덱스자체가 제거됨 특정 값만 추출하는데 함수filter를 사용할 수 있지만 조건식을 이용하는 것이 더 편하다. 참 거짓으로 구분한 뒤 참만을 뽑도록 한다. lemonade[조건식] 1234#데이터 [데이터 컬럼 == 특정값]lemonade_L=lemonade[lemonade['Location'] == 'Beach']print(lemonade_L)#lemonade['Location'] == 'Beach'-&gt;참 거짓으로 구분한 뒤 참인 그 값(Beach)을 불러내면 lemonade[lemonade['Location'] == 'Beach'] 그값(Beach)만 나옴 Date Location Lemon Orange Temperature Leaflets Price Sold \\ 3 7/4/2016 Beach 134 99 76 98.0 0.25 233 4 7/5/2016 Beach 159 118 78 135.0 0.25 277 5 7/6/2016 Beach 103 69 82 90.0 0.25 172 6 7/6/2016 Beach 103 69 82 90.0 0.25 172 7 7/7/2016 Beach 143 101 81 135.0 0.25 244 8 NaN Beach 123 86 82 113.0 0.25 209 9 7/9/2016 Beach 134 95 80 126.0 0.25 229 10 7/10/2016 Beach 140 98 82 131.0 0.25 238 11 7/11/2016 Beach 162 120 83 135.0 0.25 282 12 7/12/2016 Beach 130 95 84 99.0 0.25 225 13 7/13/2016 Beach 109 75 77 99.0 0.25 184 14 7/14/2016 Beach 122 85 78 113.0 0.25 207 15 7/15/2016 Beach 98 62 75 108.0 0.50 160 16 7/16/2016 Beach 81 50 74 90.0 0.50 131 17 7/17/2016 Beach 115 76 77 126.0 0.50 191 30 7/30/2016 Beach 88 57 82 81.0 0.35 145 31 7/31/2016 Beach 76 47 82 68.0 0.35 123 Revenue 3 58.25 4 69.25 5 43.00 6 43.00 7 61.00 8 52.25 9 57.25 10 59.50 11 70.50 12 56.25 13 46.00 14 51.75 15 80.00 16 65.50 17 95.50 30 50.75 31 43.05 1print(lemonade[lemonade['Temperature'] &gt;= 80])#온도가 80이 넘는 것을 뽑아내라 Date Location Lemon Orange Temperature Leaflets Price Sold \\ 5 7/6/2016 Beach 103 69 82 90.0 0.25 172 6 7/6/2016 Beach 103 69 82 90.0 0.25 172 7 7/7/2016 Beach 143 101 81 135.0 0.25 244 8 NaN Beach 123 86 82 113.0 0.25 209 9 7/9/2016 Beach 134 95 80 126.0 0.25 229 10 7/10/2016 Beach 140 98 82 131.0 0.25 238 11 7/11/2016 Beach 162 120 83 135.0 0.25 282 12 7/12/2016 Beach 130 95 84 99.0 0.25 225 18 7/18/2016 Park 131 92 81 122.0 0.50 223 22 7/22/2016 Park 112 75 80 108.0 0.50 187 23 7/23/2016 Park 120 82 81 117.0 0.50 202 24 7/24/2016 Park 121 82 82 117.0 0.50 203 25 7/25/2016 Park 156 113 84 135.0 0.50 269 26 7/26/2016 Park 176 129 83 158.0 0.35 305 27 7/27/2016 Park 104 68 80 99.0 0.35 172 28 7/28/2016 Park 96 63 82 90.0 0.35 159 29 7/29/2016 Park 100 66 81 95.0 0.35 166 30 7/30/2016 Beach 88 57 82 81.0 0.35 145 31 7/31/2016 Beach 76 47 82 68.0 0.35 123 Revenue 5 43.00 6 43.00 7 61.00 8 52.25 9 57.25 10 59.50 11 70.50 12 56.25 18 111.50 22 93.50 23 101.00 24 101.50 25 134.50 26 106.75 27 60.20 28 55.65 29 58.10 30 50.75 31 43.05 1print(lemonade[(lemonade['Temperature'] &gt;= 80) &amp; (lemonade['Orange'] &gt;= 100)]) #lemonade[(조건식)&amp;(조건식2)]온도가 80넘고 오랜지가100이상인 것 Date Location Lemon Orange Temperature Leaflets Price Sold \\ 7 7/7/2016 Beach 143 101 81 135.0 0.25 244 11 7/11/2016 Beach 162 120 83 135.0 0.25 282 25 7/25/2016 Park 156 113 84 135.0 0.50 269 26 7/26/2016 Park 176 129 83 158.0 0.35 305 Revenue 7 61.00 11 70.50 25 134.50 26 106.75 1print(lemonade[(lemonade['Temperature'] &gt;= 80) &amp; (lemonade['Orange'] &gt;= 100) &amp; (lemonade['Location'] == &quot;Park&quot;)]) Date Location Lemon Orange Temperature Leaflets Price Sold \\ 25 7/25/2016 Park 156 113 84 135.0 0.50 269 26 7/26/2016 Park 176 129 83 158.0 0.35 305 Revenue 25 134.50 26 106.75 iloc와 loc의 차이 iloc는 인덱스의 “숫자”로 loc는 라벨의 “이름”으로 내용을 선택한다. loc가 사용하기 편하다. 1print(lemonade.loc[lemonade['Temperature'] &gt;= 80,['Date','Sold']]) #온도가 80이 넘는 것 중 라벨명이 날자와 판매량을 찾아라(중요) Date Sold 5 7/6/2016 172 6 7/6/2016 172 7 7/7/2016 244 8 NaN 209 9 7/9/2016 229 10 7/10/2016 238 11 7/11/2016 282 12 7/12/2016 225 18 7/18/2016 223 22 7/22/2016 187 23 7/23/2016 202 24 7/24/2016 203 25 7/25/2016 269 26 7/26/2016 305 27 7/27/2016 172 28 7/28/2016 159 29 7/29/2016 166 30 7/30/2016 145 31 7/31/2016 123 문법상의 차이 확인 숫자(ilot) 라벨(lot)=글자 숫자 문자 동시 1print(lemonade.iloc[0:3,0:2]) # [행-인덱스번호부터 시작, 열] Date Location 0 7/1/2016 Park 1 7/2/2016 Park 2 7/3/2016 Park 1print(lemonade.loc[0:2,['Date','Location']])# 똑같은 결과이지만 라벨은 행의 시작 위치 컬럼부터 시작 Date Location 0 7/1/2016 Park 1 7/2/2016 Park 2 7/3/2016 Park 데이터 정렬 sort_values() 12#lemonade.head()print(lemonade.sort_values(by=['Revenue']).head(5)) Date Location Lemon Orange Temperature Leaflets Price Sold \\ 0 7/1/2016 Park 97 67 70 90.0 0.25 164 1 7/2/2016 Park 98 67 72 90.0 0.25 165 6 7/6/2016 Beach 103 69 82 90.0 0.25 172 5 7/6/2016 Beach 103 69 82 90.0 0.25 172 31 7/31/2016 Beach 76 47 82 68.0 0.35 123 Revenue 0 41.00 1 41.25 6 43.00 5 43.00 31 43.05 1print(lemonade[['Date','Temperature','Revenue']].sort_values(by=['Temperature','Revenue']).head(5)) Date Temperature Revenue 0 7/1/2016 70 41.00 20 7/20/2016 70 56.50 2 7/3/2016 71 46.75 1 7/2/2016 72 41.25 16 7/16/2016 74 65.50 1print(lemonade[['Date','Temperature','Revenue']].sort_values(by=['Temperature','Revenue'],ascending=[True,False]).head(5)) Date Temperature Revenue 20 7/20/2016 70 56.50 0 7/1/2016 70 41.00 2 7/3/2016 71 46.75 1 7/2/2016 72 41.25 16 7/16/2016 74 65.50 1print(lemonade[['Date','Temperature','Revenue']].sort_values(by=['Temperature','Revenue'],ascending=[False,True]).head(5)) Date Temperature Revenue 12 7/12/2016 84 56.25 25 7/25/2016 84 134.50 11 7/11/2016 83 70.50 26 7/26/2016 83 106.75 5 7/6/2016 82 43.00 Group by cf)인덱스가 숫자에서 라벨로 바뀜 1234df = lemonade.groupby(by='Location').count()print(df)print(&quot;&quot;)print(type(df)) Date Lemon Orange Temperature Leaflets Price Sold Revenue Location Beach 16 17 17 17 17 17 17 17 Park 15 15 15 15 14 15 15 15 &lt;class 'pandas.core.frame.DataFrame'&gt; 12df[['Date','Lemon']]print(df[['Date','Lemon']]) Date Lemon Location Beach 16 17 Park 15 15 1print(df.iloc[0:1,0:2]) Date Lemon Location Beach 16 17 1print(df.loc['Park', ['Date', 'Lemon']]) Date 15 Lemon 15 Name: Park, dtype: int64 간단한 피벗 테이블 만들기 1print(lemonade.groupby('Location')['Revenue'].agg([max,min,sum,np.mean])) max min sum mean Location Beach 95.5 43.0 1002.8 58.988235 Park 134.5 41.0 1178.2 78.546667 지역별로 매출액의 최대값, 최소값, 합계, 평균을 구한다. 1print(lemonade.groupby('Location')['Revenue','Sold','Temperature'].agg([max,min,sum,np.mean])) Revenue Sold \\ max min sum mean max min sum mean Location Beach 95.5 43.0 1002.8 58.988235 282 123 3422 201.294118 Park 134.5 41.0 1178.2 78.546667 305 113 2855 190.333333 Temperature max min sum mean Location Beach 84 74 1355 79.705882 Park 84 70 1172 78.133333 /usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:1: FutureWarning: Indexing with multiple keys (implicitly converted to a tuple of keys) will be deprecated, use a list instead. &quot;&quot;&quot;Entry point for launching an IPython kernel. 1print(lemonade.groupby(['Location', 'Price'])['Orange'].agg([max, min, sum, np.mean])) max min sum mean Location Price Beach 0.25 120 69 1110 92.500000 0.35 57 47 104 52.000000 0.50 76 50 188 62.666667 Park 0.25 77 67 211 70.333333 0.35 129 63 326 81.500000 0.50 113 42 621 77.625000 &lt;–지역에 따라 오렌지의 단가별 최대판매량, 최소판매량, 총합, 평균","link":"/2022/06/29/day0629/"},{"title":"","text":"title: ‘머신러닝3 회귀알고리즘’ date: ‘2022-06-30 14:00’ K- 최근접 이웃 회귀 지도학습 알고리즘은 크게 분류와 회귀 지도 학습 : 종속변수 존재 분류 : 도미와 빙어 분류 문제 해결 회귀 : 통계 회귀분석 y = ax + b 데이터 불러오기12import numpy as npprint(np.__version__) 1.21.6 1234567891011121314151617perch_length = np.array( [8.4, 13.7, 15.0, 16.2, 17.4, 18.0, 18.7, 19.0, 19.6, 20.0, 21.0, 21.0, 21.0, 21.3, 22.0, 22.0, 22.0, 22.0, 22.0, 22.5, 22.5, 22.7, 23.0, 23.5, 24.0, 24.0, 24.6, 25.0, 25.6, 26.5, 27.3, 27.5, 27.5, 27.5, 28.0, 28.7, 30.0, 32.8, 34.5, 35.0, 36.5, 36.0, 37.0, 37.0, 39.0, 39.0, 39.0, 40.0, 40.0, 40.0, 40.0, 42.0, 43.0, 43.0, 43.5, 44.0] )perch_weight = np.array( [5.9, 32.0, 40.0, 51.5, 70.0, 100.0, 78.0, 80.0, 85.0, 85.0, 110.0, 115.0, 125.0, 130.0, 120.0, 120.0, 130.0, 135.0, 110.0, 130.0, 150.0, 145.0, 150.0, 170.0, 225.0, 145.0, 188.0, 180.0, 197.0, 218.0, 300.0, 260.0, 265.0, 250.0, 250.0, 300.0, 320.0, 514.0, 556.0, 840.0, 685.0, 700.0, 700.0, 690.0, 900.0, 650.0, 820.0, 850.0, 900.0, 1015.0, 820.0, 1100.0, 1000.0, 1100.0, 1000.0, 1000.0] ) 1234567import matplotlib.pyplot as pltfig, ax= plt.subplots() # 객체지향의 시작ax.scatter(perch_length,perch_weight)ax.set_xlabel('length')ax.set_ylabel('weight')plt.show &lt;function matplotlib.pyplot.show&gt; 1234567from sklearn.model_selection import train_test_splittrain_input, test_input, train_target, test_target = train_test_split( perch_length, perch_weight, random_state = 42)train_input.shape, test_input.shape, train_target.shape, test_target.shapeprint(train_input.ndim) 1 1차원 배열-&gt; 2차원 배열 1234train_input = train_input. reshape(-1,1)test_input = test_input.reshape(-1,1)print(train_input.shape, test_input.shape)print(train_input.ndim) (42, 1) (14, 1) 2 결정계수 Adjusted -R Squared 정확한 지표(0~1) 1에 가까울수록 예측 모형이 예측을 잘한다. 123456789from sklearn.neighbors import KNeighborsRegressorknr = KNeighborsRegressor()#모형학습knr.fit(train_input,train_target)#테스트 세트의 점수를 확인한다print(knr.score(test_input,test_target)) 0.992809406101064 12345678from sklearn.metrics import mean_absolute_error#예측 데이터test_prediction = knr.predict(test_input)#테스트 세트에 대한 평균 절댓값 오차를 계산mae = mean_absolute_error(test_target,test_prediction)print(mae) 19.157142857142862 예측이 평균적으로 19g정도 다르다. 확실한 것은 오차가 존재하는데 19g이 의미하는 것은 무엇인가? 오차를 줄일 필요가 있음(더 많은 데이터를 모으거나 알고리즘을 바꿔야 함) 개선을 지속적으로 하여 0g이 될때까지 1print(knr.score(train_input, train_target)) 0.9698823289099254 과대적합 vs 과소적합 매우 힘듬. 도망가고 싶음(모형 설계가 잘못됨) 과대적합 : 훈련세트는 점수 좋으나 테스트 점수가 매우 안좋음 과소적합 : 테스트세트의 점수가 매우 좋음 결론 : 제대로 모형이 훈련이 안된 것이기에 모형 서비스에 탑재 불가. 12print(&quot;훈련평가:&quot;,knr.score(train_input,train_target))print(&quot;테스트평가:&quot;,knr.score(test_input,test_target))# 테스트세트 점수가 좋기에 과소적합 훈련평가: 0.9698823289099254 테스트평가: 0.992809406101064 모형개선 1234567# 이웃의 갯수를 3으로 재 지정knr.n_neighbors = 3# 모형 다시 훈련knr.fit(train_input, train_target)print(&quot;훈련 평가:&quot;,knr.score(train_input,train_target))print(&quot;테스트 평가:&quot;,knr.score(test_input, test_target)) 훈련 평가: 0.9804899950518966 테스트 평가: 0.9746459963987609","link":"/2022/06/30/day0630_ch3/"},{"title":"","text":"title: ‘머신러닝2 데이터다루기’ date: ‘2022-06-30 09:00’ 인공지능인공지능&gt;머신러닝&gt;딥러닝 딥러닝 알고리즘→인공신경망 알고리즘 이미지,자연어(=음성인식) 판별하는 성능이 중요 머신러닝 알고리즘→선형회귀, 결정트리 결과에 대한 해석 요구 통계적 분석이 중요 정형데이터(=엑셀 데이터, 테이블) 분석의 흐름 1.데이터 수집 2.데이터 가공 3.데이터 시각화 4.데이터(예측)모델링 예측평가지표 cf)R:,데이터(통계)모델링 변수(=컬럼=피쳐)간의 관계 가설 검정이 중요 공통점 : 결과를 해석 5.보고서를 작성 모형학습 fish_data-&gt; 독립변수, fish_target-&gt;종속변수 kn.fit(fish_data, fish_target) 새로운 모델 제안의 위험성 어제 머신 러닝 공부에 이어서 Default(내정값) : 정확도 1(100%) 하이퍼 파라미터 세팅 n_neighbprs = 49 로 했을 시 정확도 0.7(70%) 튜닝을 하면 이상해질 수 있다. 따라서 완벽하게 파악하지 못한 내용을 변동시키지 말것. 머신러닝 알고리즘의 흐름 선형모델 : 선형회귀,로지스틱 회귀,서포트벡터 머신 의사결정트리 모델 : 1975년 의사결정트리모델, KNN 랜덤포레스트 부스팅계열 : LightGBM(2017), XGBoost(2016) 추천: LightGBM 훈련 세트와 테스트 세트(P68)12345678fish_length = [25.4, 26.3, 26.5, 29.0, 29.0, 29.7, 29.7, 30.0, 30.0, 30.7, 31.0, 31.0, 31.5, 32.0, 32.0, 32.0, 33.0, 33.0, 33.5, 33.5, 34.0, 34.0, 34.5, 35.0, 35.0, 35.0, 35.0, 36.0, 36.0, 37.0, 38.5, 38.5, 39.5, 41.0, 41.0, 9.8, 10.5, 10.6, 11.0, 11.2, 11.3, 11.8, 11.8, 12.0, 12.2, 12.4, 13.0, 14.3, 15.0]fish_weight = [242.0, 290.0, 340.0, 363.0, 430.0, 450.0, 500.0, 390.0, 450.0, 500.0, 475.0, 500.0, 500.0, 340.0, 600.0, 600.0, 700.0, 700.0, 610.0, 650.0, 575.0, 685.0, 620.0, 680.0, 700.0, 725.0, 720.0, 714.0, 850.0, 1000.0, 920.0, 955.0, 925.0, 975.0, 950.0, 6.7, 7.5, 7.0, 9.7, 9.8, 8.7, 10.0, 9.9, 9.8, 12.2, 13.4, 12.2, 19.7, 19.9] 2차원 리스트를 만들고 라벨링을 한다. 12345fish_data = [[l, w] for l, w in zip(fish_length, fish_weight)]fish_target = [1] * 35 + [0] * 14print(fish_target[0:40:5])#1-40까지의 데이터에서 5간격으로 표시print(fish_data[0:40:5])print(fish_target) [1, 1, 1, 1, 1, 1, 1, 0] [[25.4, 242.0], [29.7, 450.0], [31.0, 475.0], [32.0, 600.0], [34.0, 575.0], [35.0, 725.0], [38.5, 920.0], [9.8, 6.7]] [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] 전체 데이터에서 일부분 샘플을 추출했다. 도미35마리 빙어 14마리 처음 35개를 훈련/ 나머지 14개를 테스트 해본다. 123456789101112131415from sklearn.neighbors import KNeighborsClassifier# 클래스 인스턴스화kn = KNeighborsClassifier()# 훈련세트로 0:34를 인덱스로 활용train_input = fish_data[:35]train_target =fish_target[:35]# 테스트 세트로 35:마지막까지를 인덱스로 활용test_input = fish_data[35:]test_target =fish_target[35:]# 모형학습kn = kn.fit(train_input,train_target)print(kn.score(test_input, test_target)) 0.0 -&gt; 훈련된 데이터는 도미인데 테스트한 데이터는 빙어이기에 이상한 결과가 나옴 샘플링 편향 훈련세트와 테스트 세트가 골고루 섞이지 않음 샘플링 작업 넘파이를 사용하여 골고루 섞어 준다. 123456import numpy as npinput_arr = np.array(fish_data)target_arr = np.array(fish_target)print(input_arr[0:49:7])print(input_arr.shape, target_arr.shape)#입력한 것의 (샘플수,특성수) 타겟은 특성수가 없음 [[ 25.4 242. ] [ 30. 390. ] [ 32. 600. ] [ 34. 685. ] [ 36. 850. ] [ 9.8 6.7] [ 11.8 9.9]] (49, 2) (49,) 123print(target_arr.shape)print(target_arr.ndim)#차원을 확인-&gt;1차원print(target_arr) (49,) 1 [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0] 12345#random으로 무작위 배열을 만드는 설정np.random.seed(42)index = np.arange(49)np.random.shuffle(index)print(index) [13 45 47 44 17 27 26 25 31 19 12 4 34 8 3 6 40 41 46 15 9 16 24 33 30 0 43 32 5 29 11 36 1 21 2 37 35 23 39 10 22 18 48 20 7 42 14 28 38] 12345train_input = input_arr[index[:35]]train_target= target_arr[index[:35]]test_input = input_arr[index[35:]]test_target= target_arr[index[35:]] 12print(train_input[:1])print(train_input[:,0])#전체길이 [[ 32. 340.]] [32. 12.4 14.3 12.2 33. 36. 35. 35. 38.5 33.5 31.5 29. 41. 30. 29. 29.7 11.3 11.8 13. 32. 30.7 33. 35. 41. 38.5 25.4 12. 39.5 29.7 37. 31. 10.5 26.3 34. 26.5] 시각화12345678import matplotlib.pyplot as plt fig, ax = plt.subplots()ax.scatter(train_input[:, 0], train_input[:, 1])ax.scatter(test_input[:, 0], test_input[:, 1])ax.set_xlabel(&quot;length&quot;)ax.set_ylabel(&quot;weight&quot;)fig.show() 두번째 머신러닝 프로그램12kn.fit(train_input, train_target)kn.score(test_input, test_target) 1.0 1kn.predict(test_input) # 예측데이터 array([0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0]) 1test_target #실제 데이터 array([0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0]) 데이터 전처리 머신러닝 시 데이터 전처리 결측치 처리, 이상치 처리 12345678fish_length = [25.4, 26.3, 26.5, 29.0, 29.0, 29.7, 29.7, 30.0, 30.0, 30.7, 31.0, 31.0, 31.5, 32.0, 32.0, 32.0, 33.0, 33.0, 33.5, 33.5, 34.0, 34.0, 34.5, 35.0, 35.0, 35.0, 35.0, 36.0, 36.0, 37.0, 38.5, 38.5, 39.5, 41.0, 41.0, 9.8, 10.5, 10.6, 11.0, 11.2, 11.3, 11.8, 11.8, 12.0, 12.2, 12.4, 13.0, 14.3, 15.0]fish_weight = [242.0, 290.0, 340.0, 363.0, 430.0, 450.0, 500.0, 390.0, 450.0, 500.0, 475.0, 500.0, 500.0, 340.0, 600.0, 600.0, 700.0, 700.0, 610.0, 650.0, 575.0, 685.0, 620.0, 680.0, 700.0, 725.0, 720.0, 714.0, 850.0, 1000.0, 920.0, 955.0, 925.0, 975.0, 950.0, 6.7, 7.5, 7.0, 9.7, 9.8, 8.7, 10.0, 9.9, 9.8, 12.2, 13.4, 12.2, 19.7, 19.9] 12## column_stack()활용np.column_stack(([1,2,3],[4,5,6])) array([[1, 4], [2, 5], [3, 6]]) 1234fish_data = np.column_stack((fish_length, fish_weight))print(fish_data[:5])fish_data.shape [[ 25.4 242. ] [ 26.3 290. ] [ 26.5 340. ] [ 29. 363. ] [ 29. 430. ]] (49, 2) 종속변수 = Y = 타깃데이터 = Target &lt;-&gt;독립변수(X) 12fish_target = np.concatenate((np.ones(35),np.zeros(14)))print(fish_target.shape) (49,) scikit-learn 훈련세트와 테스트 세트 나누기1234567from sklearn.model_selection import train_test_splittrain_input, test_input, train_target, test_target = train_test_split( #독립변수, 종속변수 fish_data, fish_target, random_state =42)train_input.shape, test_input.shape, train_target.shape, test_target.shape ((36, 2), (13, 2), (36,), (13,)) P92 도미와 빙어가 잘 섞여 있나? 1print(test_target) [1. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1.] 35(도미) :14(빙어) 2.5:1 테스트 셋(비율) 3.3:1 층화샘플링 기초 통계, 설문조사 비율이 중요 예)남성 속옷을 구매하는 비율은 남자9: 여자1이지만 조사는 남자5: 여자 5로 조사됨으로 비율이 맞지 않음 123456train_input, test_input, train_target, test_target = train_test_split( # 독립변수, 종속변수 fish_data, fish_target, stratify=fish_target, random_state = 42)train_input.shape, test_input.shape, train_target.shape, test_target.shape ((36, 2), (13, 2), (36,), (13,)) stratify = fish_target를 넣어주어 빙어가 한마리 더 늘도록 해서 테스트 세트 비율이 2.55:1 로 근접하게 됨 1print(test_target) [0. 0. 1. 0. 1. 0. 1. 1. 1. 1. 1. 1. 1.] 수상한 도미 한마리 1234from sklearn.neighbors import KNeighborsClassifierkn = KNeighborsClassifier()kn.fit(train_input, train_target)kn.score(test_input, test_target) 1.0 도미사이즈 20cm 이상 = 1 빙어사이즈 10cm 이하 = 0 인 문제가 발생 알고리즘에 문제가 있음 1print(kn.predict([[25,150]])) [0.] 123456import matplotlib.pyplot as plt fig, ax = plt.subplots()ax.scatter(train_input[:, 0], train_input[:, 1])ax.scatter(25, 150, marker = '^')plt.show() 이웃 샘플이 누구인지 확인해보니 알고리즘이 맞지 않음 빙어에 4개의 이웃 샘플이 있어 빙어로 인식 1234567distances, indexes = kn.kneighbors([[25, 150]])plt.scatter(train_input[:,0], train_input[:,1])plt.scatter(25, 150, marker='^')plt.scatter(train_input[indexes,0], train_input[indexes,1], marker='D')plt.xlabel('length')plt.ylabel('weight')plt.show() 떨어진 거리 비율을 맞추기 위해 스케일의 크기를 동일하게 함.즉 무게와 길이의 길이를 1000으로 맞춤 1234567plt.scatter(train_input[:,0], train_input[:,1])plt.scatter(25, 150, marker='^')plt.scatter(train_input[indexes,0], train_input[indexes,1], marker='D')plt.xlim((0, 1000))plt.xlabel('length')plt.ylabel('weight')plt.show() -p98 그러나 두 특성(길이와 무게)의 값이 놓인 범위가 매우 다름 두 특성의 스케일이 다름 스케일이 같도록 통계처리 필요 Feature Engineering(피처 엔지니어링) 머신 러닝 전체 데이터 전처리(결측지 처리, 이상치 처리) 데이터 분리 Feature Engineering(피처 엔지니어링) 표준점수 z 점수 1234mean = np.mean(train_input, axis =0)std = np.std(train_input,axis=0)print(mean, std) [ 27.29722222 454.09722222] [ 9.98244253 323.29893931] 표준 점수 구하기 123# 브로드 캐스팅- 서로 다른 배열을 계산할 때print(train_input.shape, mean.shape, std.shape)train_scaled = (train_input - mean)/std (36, 2) (2,) (2,) 1train_input[0:5] array([[ 29.7, 500. ], [ 12.2, 12.2], [ 33. , 700. ], [ 11.3, 8.7], [ 39.5, 925. ]]) 1train_scaled[0:5] array([[ 0.24070039, 0.14198246], [-1.51237757, -1.36683783], [ 0.5712808 , 0.76060496], [-1.60253587, -1.37766373], [ 1.22242404, 1.45655528]]) 12345plt.scatter(train_scaled[:,0], train_scaled[:,1])plt.scatter(25, 150, marker='^')plt.xlabel('length')plt.ylabel('weight')plt.show() 123456new = ([25, 150] - mean) / stdplt.scatter(train_scaled[:,0], train_scaled[:,1])plt.scatter(new[0], new[1], marker='^')plt.xlabel('length')plt.ylabel('weight')plt.show() 통계처리 전 : KNN –&gt; 예측이 틀림통계처리 후 : KNN –&gt; 예측이 정확하게 맞음– 통계처리 –&gt; Feature Engineering 모형학습 1kn.fit(train_scaled, train_target) KNeighborsClassifier() 123#kn.score(test_input, test_target)test_scaled = (test_input - mean)/ stdkn.score(test_scaled, test_target) 1.0 예측 1print(kn.predict([new])) [1.] 1234567distances, indexes = kn.kneighbors([new])plt.scatter(train_scaled[:,0], train_scaled[:,1])plt.scatter(new[0], new[1], marker='^')plt.scatter(train_scaled[indexes,0], train_scaled[indexes,1], marker='D')plt.xlabel('length')plt.ylabel('weight')plt.show()","link":"/2022/06/30/day0630_ml/"},{"title":"","text":"title: ‘머신러닝3 회귀알고리즘’date: ‘2022-07-01 09:00’ 데이터 불러오기12import numpy as npprint(np.__version__) 1.21.6 123456789101112131415161718perch_length = np.array( [8.4, 13.7, 15.0, 16.2, 17.4, 18.0, 18.7, 19.0, 19.6, 20.0, 21.0, 21.0, 21.0, 21.3, 22.0, 22.0, 22.0, 22.0, 22.0, 22.5, 22.5, 22.7, 23.0, 23.5, 24.0, 24.0, 24.6, 25.0, 25.6, 26.5, 27.3, 27.5, 27.5, 27.5, 28.0, 28.7, 30.0, 32.8, 34.5, 35.0, 36.5, 36.0, 37.0, 37.0, 39.0, 39.0, 39.0, 40.0, 40.0, 40.0, 40.0, 42.0, 43.0, 43.0, 43.5, 44.0] )perch_weight = np.array( [5.9, 32.0, 40.0, 51.5, 70.0, 100.0, 78.0, 80.0, 85.0, 85.0, 110.0, 115.0, 125.0, 130.0, 120.0, 120.0, 130.0, 135.0, 110.0, 130.0, 150.0, 145.0, 150.0, 170.0, 225.0, 145.0, 188.0, 180.0, 197.0, 218.0, 300.0, 260.0, 265.0, 250.0, 250.0, 300.0, 320.0, 514.0, 556.0, 840.0, 685.0, 700.0, 700.0, 690.0, 900.0, 650.0, 820.0, 850.0, 900.0, 1015.0, 820.0, 1100.0, 1000.0, 1100.0, 1000.0, 1000.0] )print(perch_length.shape,perch_weight.shape) (56,) (56,) 데이터 가공 1차원 데이터를 가공 train_test_split로 훈련 세트와 테스트 세트로 나눈 후 1-&gt;2차원배열로 변환 12345678from sklearn.model_selection import train_test_splittrain_input, test_input, train_target, test_target = train_test_split( # 독립변수, 종속변수 perch_length, perch_weight, random_state = 42)print(train_input.shape, test_input.shape, train_target.shape, test_target.shape) (42,) (14,) (42,) (14,) 123#1차원 -&gt;2차원:넘파이배열은 크기를 바꾸는 reshape()메서드가 있다.자동으로 바꾸는 식 (-1,1)을 이용한다train_input = train_input.reshape(-1,1)test_input = test_input.reshape(-1,1) 데이터 시각화-&gt;데이터 재가공모델링12345from sklearn.neighbors import KNeighborsRegressorknr = KNeighborsRegressor(n_neighbors=3)#모형훈련knr.fit(train_input, train_target) KNeighborsRegressor(n_neighbors=3) 모델평가모델 예측123# 농어의 50cm --&gt; 농어의 무게print(knr.predict([[50]])) [1033.33333333] 모형 평가를 위한 시각화12345678910111213141516from scipy.spatial import distanceimport matplotlib.pyplot as plt# 50cm 농어의 이웃을 3개distances, indexes = knr.kneighbors([[50]])# 훈련세트의 산점도를 그립니다.fig, ax = plt.subplots()ax.scatter(train_input, train_target)# 훈련세트 중에서 이웃 샘플만 다시 그립니다ax.scatter(train_input[indexes], train_target[indexes],marker='D')# 농어의 길이 #농어의 무게ax.scatter(50, 1033, marker='^')ax.set_xlabel('length')ax.set_ylabel('weight')plt.show() 맞는 것처럼 보이지만 길이를 100cm으로 해도 똑같은 결과(1033)가 나온다. 멀리있는 데이터를 가지고 왔다. 잘못된 알고리즘이다. 123456789101112131415# 100cm 농어의 이웃을 3개distances, indexes = knr.kneighbors([[100]])print(distances, indexes)# 훈련세트의 산점도를 그립니다.fig, ax = plt.subplots()ax.scatter(train_input, train_target)# 훈련세트 중에서 이웃 샘플만 다시 그립니다ax.scatter(train_input[indexes], train_target[indexes],marker='D')# 농어의 길이 #농어의 무게ax.scatter(100, 1033, marker='^')ax.set_xlabel('length')ax.set_ylabel('weight')plt.show()print(knr.predict([[100]]))# 100cm도 똑같이 1033g 나온다 [[56. 57. 57.]] [[34 8 14]] [1033.33333333] 선형 회귀(p.136) 사이킷에서 선형회귀 알고리즘을 사용해보자. 12345678# 파이썬from sklearn.linear_model import LinearRegressionlr= LinearRegression()# 선형회귀 모델을 훈련lr.fit(train_input, train_target) LinearRegression() 1print(lr.predict([[50]])) [1241.83860323] 1print(lr.predict([[1000]])) [38308.12631868] 1print(lr.coef_,lr.intercept_) # lr.coef_는 기울기(계수, 가중치) 값,lr.intercept_(절편) [39.01714496] -709.0186449535477 선형회귀에서 다항회귀로 바꾸자 농어 1cm가 -650g은 이상하다. 직선의 기울기 대신 곡선의 기울기를 쓰자. 직선은 1차방정식, 곡선은 2차방정식 $ 무게 =a \\times\\ 길이^2 + b \\times\\ 길이 + 절편 $ 1234#p.140train_poly = np.column_stack((train_input **2, train_input))test_poly = np.column_stack((test_input **2, test_input))print(train_poly.shape, test_poly.shape) (42, 2) (14, 2) 123lr = LinearRegression()lr.fit(train_poly,train_target)print(lr.predict([[50 ** 2, 50]])) [1573.98423528] 1print(lr.coef_, lr.intercept_) [ 1.01433211 -21.55792498] 116.0502107827827 $무게 = 1.01 \\times\\ 길이^2 - 21.6 \\times\\ 길이 +116.05$","link":"/2022/07/01/day0701_1/"},{"title":"","text":"title: ‘머신러닝4 로지스틱 회귀’date: ‘2022-07-01 11:00’ 로지스틱 회귀 선형회귀에서 출발 이진 분류 문제 해결 클래스 확률 예측 딥러닝에서도 사용됨 P177 X가 사격형일 확율 30% X가 삼각형일 확률 50% X가 원일 확률 20% 데이터 불러오기 Species(종속변수 = Y) Weight,Length, Diagonal,Height,Width(독립변수들) 1234import pandas as pdfish = pd.read_csv('https://bit.ly/fish_csv_data')fish.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Species Weight Length Diagonal Height Width 0 Bream 242.0 25.4 30.0 11.5200 4.0200 1 Bream 290.0 26.3 31.2 12.4800 4.3056 2 Bream 340.0 26.5 31.1 12.3778 4.6961 3 Bream 363.0 29.0 33.5 12.7300 4.4555 4 Bream 430.0 29.0 34.0 12.4440 5.1340 &lt;svg xmlns=”http://www.w3.org/2000/svg&quot; height=”24px”viewBox=”0 0 24 24” width=”24px”&gt; .colab-df-container { display:flex; flex-wrap:wrap; gap: 12px; } .colab-df-convert { background-color: #E8F0FE; border: none; border-radius: 50%; cursor: pointer; display: none; fill: #1967D2; height: 32px; padding: 0 0 0 0; width: 32px; } .colab-df-convert:hover { background-color: #E2EBFA; box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15); fill: #174EA6; } [theme=dark] .colab-df-convert { background-color: #3B4455; fill: #D2E3FC; } [theme=dark] .colab-df-convert:hover { background-color: #434B5C; box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15); filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3)); fill: #FFFFFF; } &lt;script&gt; const buttonEl = document.querySelector('#df-1e28b899-1483-4eec-b037-95410a050afe button.colab-df-convert'); buttonEl.style.display = google.colab.kernel.accessAllowed ? 'block' : 'none'; async function convertToInteractive(key) { const element = document.querySelector('#df-1e28b899-1483-4eec-b037-95410a050afe'); const dataTable = await google.colab.kernel.invokeFunction('convertToInteractive', [key], {}); if (!dataTable) return; const docLinkHtml = 'Like what you see? Visit the ' + '&lt;a target=&quot;_blank&quot; href=https://colab.research.google.com/notebooks/data_table.ipynb&gt;data table notebook&lt;/a&gt;' + ' to learn more about interactive tables.'; element.innerHTML = ''; dataTable['output_type'] = 'display_data'; await google.colab.output.renderOutput(dataTable, element); const docLink = document.createElement('div'); docLink.innerHTML = docLinkHtml; element.appendChild(docLink); } &lt;/script&gt; &lt;/div&gt; 데이터 탐색1234# 종속변수print(pd.unique(fish['Species']))#유니크함수로 스피시스열의 고유값 추출print(&quot;&quot;)print(fish['Species'].value_counts()) ['Bream' 'Roach' 'Whitefish' 'Parkki' 'Perch' 'Pike' 'Smelt'] Perch 56 Bream 35 Roach 20 Pike 17 Smelt 14 Parkki 11 Whitefish 6 Name: Species, dtype: int64 데이터 가공12345# 판다스 데이터 프레임에서 넘파이 배열로 변환fish_input = fish[['Weight','Length','Diagonal','Height','Width']].to_numpy() # fish 데이터 프레임에서 여러열을 선택해 새로운 데이터 프레임을 넘파이 배열로 바꾸어 저장print(fish_input.shape) (159, 5) 1print(fish_input[:5]) [[242. 25.4 30. 11.52 4.02 ] [290. 26.3 31.2 12.48 4.3056] [340. 26.5 31.1 12.3778 4.6961] [363. 29. 33.5 12.73 4.4555] [430. 29. 34. 12.444 5.134 ]] 타킷데이터, 종속변수, Y 123fish_target = fish['Species'].to_numpy()print(fish_target.shape)print(fish_target[:5]) (159,) ['Bream' 'Bream' 'Bream' 'Bream' 'Bream'] 데이터 분리 훈련 데이터 테스트 데이터 분리 123456789from sklearn.model_selection import train_test_split#임의 샘플링train_input, test_input,train_target,test_target = train_test_split( fish_input, fish_target, random_state= 42)print(train_input.shape) # 훈련데이터 값#층화 샘플링 (119, 5) 표준화 전처리 여기에서도 훈련 세트의 통계 값으로 테스트 세트를 변환해야 한다는 점을 잊지 마세요!!(중요) 훈련 세트의 평균값과 테스트 세트의 평균값는 다르다. 따라서 테스트 세트의 평균값(통계값)을 훈련세트의 평균값(통계값)으로 대체해줘야 한다. 데이터 가공 숫자 결측치가 존재, 평균값으로 대체 원본 데이터 평균으로 대치하면 안됨 훈련 데이터와 테스트 데이터 분리 데이터 누수(Data Leakage) 훈련데이터 평균값 70을 대치(기준) 테스트 데이터 평균값(75)과 모든 데이터 평균값(72.5)은 기준이 안됨 참조: https://scikit-learn.org/stable/common_pitfalls.html cf) 기준을 맞춰라 –&gt;데이터 표준화(표준점수) p97~100는 수동으로 mean,std 을 -‘# train_scaled = (train_input - mean)/ std 라는 수식을 만들어 사용했으나 StandardScaler 라는 매소드가 있으니 이를 이용하면 된다. 123456from sklearn.preprocessing import StandardScalerss = StandardScaler()ss.fit(train_input)#ss.fit(test_input)을 하면 안됨!!-&gt; 훈련테스트 통계값으로 통일train_scaled = ss.transform(train_input)test_scaled = ss.transform(test_input) 모형 만들기 K-최근접 이웃 123456from sklearn.neighbors import KNeighborsClassifierkn = KNeighborsClassifier(n_neighbors = 3)kn.fit(train_scaled, train_target)print(kn.score(train_scaled, train_target))print(kn.score(test_scaled, test_target)) 0.8907563025210085 0.85 타깃값 확인 알파벳 순으로 정렬 1print(kn.classes_) ['Bream' 'Parkki' 'Perch' 'Pike' 'Roach' 'Smelt' 'Whitefish'] 다중분율 5개 샘플에 대한 예측은 어떤 확률이냐? 12345import numpy as npproba = kn.predict_proba(test_scaled[:5])print(kn.classes_)print(np.round(proba,decimals= 4)) ['Bream' 'Parkki' 'Perch' 'Pike' 'Roach' 'Smelt' 'Whitefish'] [[0. 0. 1. 0. 0. 0. 0. ] [0. 0. 0. 0. 0. 1. 0. ] [0. 0. 0. 1. 0. 0. 0. ] [0. 0. 0.6667 0. 0.3333 0. 0. ] [0. 0. 0.6667 0. 0.3333 0. 0. ]] 첫번째 클래스는 Perch 100% 확률로 Perch로 예측 네번째 클래스는 Perch 66.7%확률로 Perch로 예측 33.3%확률로 Roach로 예측 회귀식 y= ax + b 양변에 로그를 취함 원래 값으로 돌리기 위해 양변을 다시 지수로 변환-&gt;로지스틱 회귀 로지스틱 회귀로 이진분류 수행12char_arr = np.array(['A','B','C','D','E'])print(char_arr[[True,False,True,False,False]]) ['A' 'C'] 도미와 빙어의 행만 골라냄 (bream,smelt) 123456bream_smelt_indexes =(train_target == 'Bream') | (train_target == 'Smelt')print(bream_smelt_indexes)train_bream_smelt = train_scaled[bream_smelt_indexes]target_bream_smelt = train_target[bream_smelt_indexes]print(train_scaled.shape, train_bream_smelt.shape) [ True False True False False False False True False False False True False False False True True False False True False True False False False True False False True False False False False True False False True True False False False False False True False False False False False True False True False False True False False False True False False False False False False True False True False False False False False False False False False True False True False False True True False False False True False False False False False True False False False True False True False False True True False False False False False False False False True True False False True False False] (119, 5) (33, 5) 총 119마리에서 참인 값은 33마리만 추출 모델 만들기123from sklearn.linear_model import LogisticRegressionlr = LogisticRegression()lr.fit(train_bream_smelt,target_bream_smelt) LogisticRegression() 1print(lr.predict(train_bream_smelt[:5]))#훈련한 모델로 5개 샘플 예측 ['Bream' 'Smelt' 'Bream' 'Bream' 'Bream'] 1print(lr.predict_proba(train_bream_smelt[:5]))# 예측 확율을 출력 두번째만 도미가 아님 [[0.99759855 0.00240145] [0.02735183 0.97264817] [0.99486072 0.00513928] [0.98584202 0.01415798] [0.99767269 0.00232731]] 1print(lr.classes_) # 음성클라스 도미(0): 양성크라스 빙어(1) ['Bream' 'Smelt'] cf. 분류기준 : threshold 임계값 설정(경계선 설정) 도미 Vs 빙어 [0.51,0.49]-&gt; 이런값은 도미인가 빙어인가? [0.90,0.10] 계수와 절편 1print(lr.coef_, lr.intercept_)#로지스틱 회귀는 선형회귀와 비슷 [[-0.4037798 -0.57620209 -0.66280298 -1.01290277 -0.73168947]] [-2.16155132] 12decisions = lr.decision_function(train_bream_smelt[:5])#decision_function()메서드로 Z값 출력print(decisions) [[ 13.07724442 5.67940163 -3.35341274 -3.31343798 2.17367082 -20.94258142 6.67911528] [-11.87101288 2.30253045 5.38260123 -3.16152122 3.19003127 8.30344773 -4.14607657] [ 12.33862012 5.65079591 -4.66939988 -2.1462105 1.70362799 -17.38222731 4.50479367] [ 10.54150945 6.10969846 -4.81186721 -2.96238906 2.29032761 -14.96402558 3.79674632] [ 13.67852112 5.73152066 -4.25491239 -2.55085968 1.73528849 -20.24827704 5.90871883]] z값을 확율값으로 변환시켜야 함. 지수변환(p188)시켜야 함 expit() 12from scipy.special import expitprint(expit(decisions)) [0.00240145 0.97264817 0.00513928 0.01415798 0.00232731] 다중 분류 수행하기 2진분류의 확장판 1234567# 하이퍼 파라메터 세팅# 모형을 튜닝(잘모르면 건들지 않는게 좋음, defult값 사용)# 모형 결과의 과대적합 또는 과소적합을 방지하기 위한 것lr = LogisticRegression(C =20 , max_iter = 1000)lr.fit(train_scaled, train_target)print(lr.score(train_scaled, train_target))print(lr.score(test_scaled, test_target)) 0.9327731092436975 0.925 1print(lr.predict(test_scaled[:5])) ['Perch' 'Smelt' 'Pike' 'Roach' 'Perch'] 123proba = lr.predict_proba(test_scaled[:5])print(np.round(proba, decimals = 3))print(lr.classes_) [[0. 0.014 0.841 0. 0.136 0.007 0.003] [0. 0.003 0.044 0. 0.007 0.946 0. ] [0. 0. 0.034 0.935 0.015 0.016 0. ] [0.011 0.034 0.306 0.007 0.567 0. 0.076] [0. 0. 0.904 0.002 0.089 0.002 0.001]] ['Bream' 'Parkki' 'Perch' 'Pike' 'Roach' 'Smelt' 'Whitefish'] 다중 분류일 경우 선형 방정식은 어떤 모습일까? 분류 7개 컬럼 값 5개 123print(lr.coef_,lr.intercept_)print(&quot;&quot;)print(lr.coef_.shape, lr.intercept_.shape) [[-1.49002087 -1.02912886 2.59345551 7.70357682 -1.2007011 ] [ 0.19618235 -2.01068181 -3.77976834 6.50491489 -1.99482722] [ 3.56279745 6.34357182 -8.48971143 -5.75757348 3.79307308] [-0.10458098 3.60319431 3.93067812 -3.61736674 -1.75069691] [-1.40061442 -6.07503434 5.25969314 -0.87220069 1.86043659] [-1.38526214 1.49214574 1.39226167 -5.67734118 -4.40097523] [ 0.62149861 -2.32406685 -0.90660867 1.71599038 3.6936908 ]] [-0.09205179 -0.26290885 3.25101327 -0.14742956 2.65498283 -6.78782948 1.38422358] (7, 5) (7,) 평가지표 회귀 평가지표-&gt; 결정계수($R^2$)P.121 $1-[(타깃-예측)^2의 합/(타깃-평균)^2합]$ MAE, MSE, RMSE (실제 - 예측) =오차 MAE(mean absolute errer): 오차의 절댓값의 평균 MSE(m Squared e): 오차의 제곱의 평균 RMSE(Root MSE): MSE에 제곱근을 취한값 좋은 모델이란 결정계수 :1에 수렴하면 좋은 모델 MAE외 :0에 수렴하면 좋은 모델 123456789101112131415161718import numpy as npfrom sklearn.metrics import mean_absolute_error, mean_squared_error, r2_scoretrue = np.array([1,2,3,2,3,5,4,6,5,6,7,8,8]) #실제값preds = np.array([1,1,2,2,3,4,4,5,5,7,7,6,8])#예측값#절대값 오차의 평균mae = mean_absolute_error(true, preds)print(&quot;mae=&quot;,mae)#제곱 오차의 평균mse = mean_absolute_error(true, preds)print(&quot;mse=&quot;,mse)#mse제곱근rmse =np.sqrt(mse)print(&quot;rmse=&quot;,rmse)#결정계수r2 = r2_score(true, preds)print(&quot;r2=&quot;,r2) mae= 0.5384615384615384 mse= 0.5384615384615384 rmse= 0.7337993857053428 r2= 0.8617021276595744 분류 오차 행렬 오차 행렬 실제 값 [빙어, 도미, 도미, 빙어, 도미] 예측 값 [빙어, 빙어, 도미, 빙어, 빙어] TP(빙어를 빙어로 예측):2 TN(도미를 도미로 예측):1 FP(실제도미,예측 빙어):2 FN(실제빙어,예측 도미):0 모형의 정확도 3/5 =60% 사이킷런에 분류오차행렬 함수가 있다. TP,TN,FP,FN(5,4,3,7) 정확도(5+4/5+5+3+7) 정밀도(precision:5/5+3):양성이라 예측(TP+FP)중 실제 양성값(TP)의 비율(스팸메일)-&gt;실수를 옳다고 생각하면 안되는 값 재현율(5/5+7):실제 양성(TP+FN) 값 중 양성으로 예측한 값(TP)의 비율 (암진단)-&gt;사실을 거짓으로 판단하면 큰일나는 값 로그손실 ROC Curve(=AUC) 코로나 검사 양성(1) : 음성(99) 머신러닝 모형 :98%/ 정밀도 99 인간 음성진단 :99%/ 정밀도 95 검사자가 실제는 양성이나 진단은 음성으로 내릴 가능성이 높음(의료사고)-재현율로 파악하는 것이 옳다. 123456from sklearn.metrics import confusion_matrixtrue = [0,1,1,0,0]preds = [1,0,0,0,0]confusion_matrix(true, preds) array([[2, 1], [2, 0]])","link":"/2022/07/01/day0701_2/"},{"title":"","text":"title: ‘머신러닝4 확률적 경사 하강법’date: ‘2022-07-04 09:00’ 확률적 경사 하강법 점진적 학습 (step, 보폭) 학습률 XGBoost, Light GBM, 딥러닝(이미지 분류, 자연어처리, 옵티마이져) 샘플 신경망 이미지 데이터, 자연어 자율주행 하루 데이터 1TB –&gt;학습량이 너무 큼 한꺼번에 모델을 학습하기 어려움 샘플링, 배치, 에포크, 오차(=손실=loss)가 가장 작은 지점을 찾아야 함. 이러한 방법이 확률적 경사 하강법 빠른 시간내에 손실을 줄일 수 있는 방법을 찾는것= 손실함수를 이용. 10시간 걸려 정확도95% 1시간 걸려 정확도 80% 어느 것이 좋은가? 최적화. 손실함수 로지스틱 손실 함수 123import pandas as pdfish = pd.read_csv(&quot;http://bit.ly/fish_csv_data&quot;)fish.info() &lt;class 'pandas.core.frame.DataFrame'&gt; RangeIndex: 159 entries, 0 to 158 Data columns (total 6 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 Species 159 non-null object 1 Weight 159 non-null float64 2 Length 159 non-null float64 3 Diagonal 159 non-null float64 4 Height 159 non-null float64 5 Width 159 non-null float64 dtypes: float64(5), object(1) memory usage: 7.6+ KB 1234fish_input =fish[['Weight','Length','Diagonal','Height','Width']].to_numpy()fish_target =fish['Species'].to_numpy()fish_input.shape,fish_target.shape ((159, 5), (159,)) 훈련세트와 테스트 데이터 분리 1234from sklearn.model_selection import train_test_splittrain_input,test_input, train_target, test_target =train_test_split( fish_input, fish_target, random_state =42) 훈련세트와 테스트 세트의 특성 표준화 무게, 길이, 대각선 길이, 높이, 너비-표준화 처리진행 1234567from sklearn.preprocessing import StandardScalerss = StandardScaler()ss.fit(train_input)train_scaled = ss.transform(train_input)test_scaled = ss.transform(test_input)train_scaled[:5] array([[ 0.91965782, 0.60943175, 0.81041221, 1.85194896, 1.00075672], [ 0.30041219, 1.54653445, 1.45316551, -0.46981663, 0.27291745], [-1.0858536 , -1.68646987, -1.70848587, -1.70159849, -2.0044758 ], [-0.79734143, -0.60880176, -0.67486907, -0.82480589, -0.27631471], [-0.71289885, -0.73062511, -0.70092664, -0.0802298 , -0.7033869 ]]) 모델링 확률적 경사 하강법 123456from sklearn.linear_model import SGDClassifiersc = SGDClassifier(loss = 'log',max_iter =10, random_state =42)#에포크 10회는 좀 적으니 더 숫자를 넣어라~sc.fit(train_scaled, train_target)print(sc.score(train_scaled,train_target))print(sc.score(test_scaled, test_target)) 0.773109243697479 0.775 /usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_stochastic_gradient.py:700: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit. ConvergenceWarning, partial_fit()메서드 사용하면 추가학습 가능 123sc.partial_fit(train_scaled, train_target)print(sc.score(train_scaled, train_target))print(sc.score(test_scaled, test_target)) 0.8151260504201681 0.85 에포크와 과대/과소적합 에포크 숫자가 적으면 덜학습 됨 early_stopping 에포크 숫자를 1000회로 주어졌을 때 손실 10,9,8…..3,3,3 3에 도달한 시점에서 학습을 몇번하고 그만 둠 123456789101112import numpy as npsc = SGDClassifier(loss='log',random_state=42)train_score =[]test_score= []classes = np.unique(train_target)#300번 에포크 훈련을 반복#훈련할 때마다, train_score, test_score추가를 한다.for _ in range(0,300): sc.partial_fit(train_scaled,train_target, classes= classes) train_score.append(sc.score(train_scaled,train_target)) test_score.append(sc.score(test_scaled,test_target)) 123456import matplotlib.pyplot as plt fig,ax = plt.subplots()ax.plot(train_score)ax.plot(test_score)ax.legend([&quot;train&quot;, &quot;test&quot;])plt.show() XGBoost, LightGBM코드 train-loss, train-accuaray,test-loss,test-accurary","link":"/2022/07/04/day0704_1/"},{"title":"","text":"title: ‘머신러닝5 결정트리’date: ‘2022-07-04 11:00’ 결정트리(아주 중요)123import pandas as pdwine = pd.read_csv('https://bit.ly/wine_csv_data')print(wine.head()) alcohol sugar pH class 0 9.4 1.9 3.51 0.0 1 9.8 2.6 3.20 0.0 2 9.8 2.3 3.26 0.0 3 9.8 1.9 3.16 0.0 4 9.4 1.9 3.51 0.0 데이터 가공하기. 12data = wine[['alcohol','sugar','pH']].to_numpy()target = wine['class'].to_numpy() 훈련데이터 분리 123456from sklearn.model_selection import train_test_splittrain_input, test_input, train_target, test_target =train_test_split( data, target, test_size = 0.2, random_state=42)train_input.shape, test_input.shape, train_target.shape, test_target.shape ((5197, 3), (1300, 3), (5197,), (1300,)) 123456from sklearn.preprocessing import StandardScalerss = StandardScaler()ss.fit(train_input)train_scaled = ss.transform(train_input)test_scaled = ss.transform(test_input) 12345from sklearn.linear_model import LogisticRegressionlr = LogisticRegression()lr.fit(train_scaled, train_target)print(lr.score(train_scaled,train_target))print(lr.score(test_scaled,test_target)) 0.7808350971714451 0.7776923076923077 1print(lr.coef_,lr.intercept_) [[ 0.51270274 1.6733911 -0.68767781]] [1.81777902] 모델만들기 123456789101112from sklearn.tree import DecisionTreeClassifierimport matplotlib.pyplot as pltfrom sklearn.tree import plot_treedt = DecisionTreeClassifier(max_depth= 6,random_state=42)#깊이를 줄여보자dt.fit(train_scaled,train_target)print(dt.score(train_scaled,train_target)) #훈련 셋트print(dt.score(test_scaled,test_target)) #테스트 셋트plt.figure(figsize=(10,7))plot_tree(dt)plt.show() 0.8766596113142198 0.8523076923076923 훈련정확도는 99.6% 테스트 정확도는 85.9%+-&gt;과대적합이 일어남-max_depth= 7값을 조정하여 비슷하게 만듬 노드란 무엇인가? 0이면 레드 와인(1599) 1이면 화이트 와인(4898) 1wine['class'].value_counts() 1.0 4898 0.0 1599 Name: class, dtype: int64 123456plt.figure(figsize=(10,7))plot_tree(dt, max_depth=1, filled=True, feature_names=['alcohol','sugar','pH'])plt.show() 불순도(Gini impurity) 비율(0~0.5) 레드와인:화이트 와인 이 5:5 일때 불순도가 가장 높은 상태(0.5) 한범주안에서 서로 다른 데이터가 얼마나 섞여 있는지를 나타냄. 흰색과 검은색이 각각 반반이면 불순도 최대 0.5 흰색과 검은색이 완전 분리가 되면 흰색 노드 불순도 최소 0 검은색 노드 불순도 최소 0 엔트로피(Entropy) 불확실한 정도를 의미(0~1) 흰색과 검은색이 각각 반이면 엔트로피 최대 1 흰색과 검은색이 완전 분리가 되면 흰색 노드 엔트로피도 0 검은색 노드 엔트로피도 0 1234567891011121314from sklearn.tree import DecisionTreeClassifierimport matplotlib.pyplot as pltfrom sklearn.tree import plot_treedt = DecisionTreeClassifier(criterion = 'entropy', max_depth = 42, random_state=42)dt.fit(train_scaled,train_target)dt.score(train_scaled,train_target) #훈련 셋트dt.score(test_scaled,test_target) #테스트 셋트plt.figure(figsize=(10,7))plot_tree(dt, max_depth=1, filled=True, feature_names=['alcohol','sugar','pH'])plt.show() 특성 중요도 어떤 특성이 결정 트리 모델에 영향을 주었는가? 1print(dt.feature_importances_) [0.23739824 0.5051808 0.25742097] 현업에서의 적용 현업에서 DecisionTreeClassifier을 사용하기에는 오래되었다.(1970년대) 렘덤포르세트, XGBoost 하이퍼 파라미터가 매우 많음 검증 세트 훈련세트와 테스트세트 훈련 : 교과서로 공부하는 것 훈련세트 : 모의평가 검증 : 강남대성 모의고사 테스트 : 중간고사, 기말고사 실전 :수능 123456789import pandas as pdwine = pd.read_csv('https://bit.ly/wine_csv_data')data = wine[['alcohol','sugar','pH']].to_numpy()target = wine['class'].to_numpy()#훈련 80%, 테스트 20%train_input, test_input, train_target, test_target =train_test_split( data, target, test_size = 0.2, random_state=42)train_input.shape, test_input.shape, train_target.shape, test_target.shape ((5197, 3), (1300, 3), (5197,), (1300,)) 12345#훈련80%, 검증20%sub_input, val_input,sub_target,val_target = train_test_split( train_input, train_target, test_size =0.2, random_state=42)sub_input.shape,val_input.shape,sub_target.shape,val_target.shape ((4157, 3), (1040, 3), (4157,), (1040,)) 훈련데이터: sub_input, sub_target 검증데이터: val_input, val_target 테스트데이터: test_input, test_target 123456from sklearn.tree import DecisionTreeClassifierdt =DecisionTreeClassifier(random_state =42)dt.fit(sub_input,sub_target)print(&quot;훈련성과:&quot;,dt.score(sub_input,sub_target))print(&quot;검증성과:&quot;,dt.score(val_input,val_target))print(&quot;최종:&quot;,dt.score(test_input,test_target)) 훈련성과: 0.9971133028626413 검증성과: 0.864423076923077 최종: 0.8569230769230769 훈련 : 87% 검증 : 86%-&gt;과대적합 최종 : 85% 교차 검증 데이터 셋을 반복분할 For loop 샘플링의 편향성을 방지 교차검증을 한다고 해서 정확도가 무조건 올라가는 것은 아님. 모형을 안정적으로 만들어줌(과대적합 방지) 12345678from sklearn.model_selection import KFoldimport numpy as npdf = np.array([1,2,3,4,5,6,7,8,9,10])#데이터를 K폴드로 나눈다.folds = KFold(n_splits=5, shuffle = True)for train_idx,valid_idx in folds.split(df): print(f'훈련데이터:{df[train_idx]},검증데이터:{df[valid_idx]}') 훈련데이터:[1 2 3 4 5 6 8 9],검증데이터:[ 7 10] 훈련데이터:[ 1 2 3 4 6 7 8 10],검증데이터:[5 9] 훈련데이터:[ 1 2 4 5 7 8 9 10],검증데이터:[3 6] 훈련데이터:[ 1 3 4 5 6 7 9 10],검증데이터:[2 8] 훈련데이터:[ 2 3 5 6 7 8 9 10],검증데이터:[1 4] 교차 검증 함수 cross_validate() 1234from sklearn.model_selection import cross_validatescores =cross_validate(dt,train_input,train_target)print(scores)print(&quot;평균:&quot;,np.mean(scores['test_score'])) {'fit_time': array([0.01610422, 0.00758529, 0.00780439, 0.00830793, 0.00751185]), 'score_time': array([0.00120473, 0.00096321, 0.00094819, 0.00098753, 0.00134945]), 'test_score': array([0.86923077, 0.84615385, 0.87680462, 0.84889317, 0.83541867])} 평균: 0.855300214703487 StratifiedKFold 사용 타깃클래스를 골고루 나누기 위함 1234from sklearn.model_selection import StratifiedKFold scores = cross_validate(dt, train_input, train_target, cv = StratifiedKFold())print(scores)print(&quot;평균 : &quot;, np.mean(scores['test_score'])) {'fit_time': array([0.01418257, 0.00769162, 0.00846624, 0.00809741, 0.00809479]), 'score_time': array([0.00103283, 0.00094843, 0.00096679, 0.00104737, 0.00102282]), 'test_score': array([0.86923077, 0.84615385, 0.87680462, 0.84889317, 0.83541867])} 평균 : 0.855300214703487 10-폴드 교차 검증 12345from sklearn.model_selection import StratifiedKFoldsplitter =StratifiedKFold(n_splits = 10,shuffle =True, random_state = 42)#10번 더 교차 검증을 수행scores =cross_validate(dt,train_input,train_target,cv =splitter)print(scores)print(&quot;평균:&quot;,np.mean(scores['test_score'])) {'fit_time': array([0.01752782, 0.00863743, 0.00879025, 0.0088315 , 0.00837684, 0.00851107, 0.00831246, 0.00826931, 0.00829077, 0.00844049]), 'score_time': array([0.00099802, 0.00086832, 0.00085974, 0.00183129, 0.00091791, 0.0008316 , 0.00078368, 0.00092673, 0.00080705, 0.00082994]), 'test_score': array([0.83461538, 0.87884615, 0.85384615, 0.85384615, 0.84615385, 0.87307692, 0.85961538, 0.85549133, 0.85163776, 0.86705202])} 평균: 0.8574181117533719 하이퍼 파라미터 튜닝 그리드 서치(사람이 수동입력) max_depth: [1.3.5…] 랜덤 서치(사람이 범위만 지정) max_depth: 1~10/by random 베이지안 옵티마이제이션 사람의 개입없이 하이퍼파라미터 튜닝을 자동적으로 수행하는 기술을 AutoML이라고 함 예)PyCaret 각 모델마다 적게는 1-2개에서 많게는 5-6개의 매개 변수를 제공한다 하이퍼파라미터와 동시에 교차검증을 수행(불가능하다) 교차검증 5번 교차검증 1번 돌때, Max Depth3번 적용 총 결과값 3x5x2나옴 Max Death =1,3,7 criterion= gini.entropy 1234567from sklearn.model_selection import GridSearchCVparams = { 'min_impurity_decrease' : [0.0001,0.0002,0.0003,0.0004,0.0005] }gs = GridSearchCV(DecisionTreeClassifier(random_state =42),params, n_jobs= -1)gs.fit(train_input,train_target) GridSearchCV(estimator=DecisionTreeClassifier(random_state=42), n_jobs=-1, param_grid={'min_impurity_decrease': [0.0001, 0.0002, 0.0003, 0.0004, 0.0005]}) 123print(&quot;best:&quot;,gs.best_estimator_)dt = gs.best_estimator_print(dt.score(train_input, train_target)) best: DecisionTreeClassifier(min_impurity_decrease=0.0001, random_state=42) 0.9615162593804117 123456789101112from sklearn.model_selection import GridSearchCVparams = { 'criterion' : ['gini', 'entropy'], 'max_depth ': [1,3,7], 'min_impurity_decrease' : [0.0001,0.0002,0.0003,0.0004,0.0005] }gs = GridSearchCV(DecisionTreeClassifier(random_state =42),params, n_jobs= -1)gs.fit(train_input,train_target)print(&quot;best:&quot;,gs.best_estimator_)dt = gs.best_estimator_print(dt.score(train_input, train_target)) best: DecisionTreeClassifier(max_depth=7, min_impurity_decrease=0.0005, random_state=42) 0.8830094285164518","link":"/2022/07/04/day0704_2/"}],"tags":[],"categories":[]}
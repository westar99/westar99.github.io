{"pages":[],"posts":[{"title":"복습day0617","text":"R MarkdownThis is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see http://rmarkdown.rstudio.com. When you click the Knit button a document will be generated that includes both content as well as the output of any embedded R code chunks within the document. You can embed an R code chunk like this: 1summary(cars) 1234567## speed dist ## Min. : 4.0 Min. : 2.00 ## 1st Qu.:12.0 1st Qu.: 26.00 ## Median :15.0 Median : 36.00 ## Mean :15.4 Mean : 42.98 ## 3rd Qu.:19.0 3rd Qu.: 56.00 ## Max. :25.0 Max. :120.00 Including PlotsYou can also embed plots, for example: Note that the echo = FALSE parameter was added to the code chunk to prevent printing of the R code that generated the plot. ggplot2 시각화 다음과 같이 시각화를 작성한다. 1234library(ggplot2)ggplot(data = iris, aes(x = Sepal.Length, y = Sepal.Width)) + geom_point()","link":"/2022/06/20/day0617_1/"},{"title":"Hello World","text":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new &quot;My New Post&quot; More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment","link":"/2022/06/20/hello-world/"},{"title":"test","text":"##csv파일 불러오기 csv파일을 불러옵니다 12mpg1 &lt;- read.csv(&quot;mpg1.csv&quot;)str(mpg1) 123456## 'data.frame': 234 obs. of 5 variables:## $ manufacturer: chr &quot;audi&quot; &quot;audi&quot; &quot;audi&quot; &quot;audi&quot; ...## $ trans : chr &quot;auto&quot; &quot;manual&quot; &quot;manual&quot; &quot;auto&quot; ...## $ drv : chr &quot;f&quot; &quot;f&quot; &quot;f&quot; &quot;f&quot; ...## $ cty : int 18 21 20 21 16 18 18 18 16 20 ...## $ hwy : int 29 29 31 30 26 26 27 26 25 28 ... ##데이터 시각화 하기-cty, hwy산점도를 그려본다 123library(ggplot2)ggplot(mpg1,aes(x =cty, y = hwy))+ geom_point()","link":"/2022/06/20/test/"},{"title":"220621","text":"#220621에 한 일들.데이터 불러오기#경로 설정이 매우 중요#getwd()#현재 경로를 확인하는 함수#setwd(“C:/Users/human/Desktop/R_lecure/source/data”)#csv파일/엑셀파일 불어오기#오늘 할 일 P91 12mpg1&lt;-read.csv(&quot;mpg1.csv&quot;)mpg1 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235## manufacturer trans drv cty hwy## 1 audi auto f 18 29## 2 audi manual f 21 29## 3 audi manual f 20 31## 4 audi auto f 21 30## 5 audi auto f 16 26## 6 audi manual f 18 26## 7 audi auto f 18 27## 8 audi manual 4 18 26## 9 audi auto 4 16 25## 10 audi manual 4 20 28## 11 audi auto 4 19 27## 12 audi auto 4 15 25## 13 audi manual 4 17 25## 14 audi auto 4 17 25## 15 audi manual 4 15 25## 16 audi auto 4 15 24## 17 audi auto 4 17 25## 18 audi auto 4 16 23## 19 chevrolet auto r 14 20## 20 chevrolet auto r 11 15## 21 chevrolet auto r 14 20## 22 chevrolet auto r 13 17## 23 chevrolet auto r 12 17## 24 chevrolet manual r 16 26## 25 chevrolet auto r 15 23## 26 chevrolet manual r 16 26## 27 chevrolet auto r 15 25## 28 chevrolet manual r 15 24## 29 chevrolet auto 4 14 19## 30 chevrolet auto 4 11 14## 31 chevrolet auto 4 11 15## 32 chevrolet auto 4 14 17## 33 chevrolet auto f 19 27## 34 chevrolet auto f 22 30## 35 chevrolet auto f 18 26## 36 chevrolet auto f 18 29## 37 chevrolet auto f 17 26## 38 dodge auto f 18 24## 39 dodge auto f 17 24## 40 dodge auto f 16 22## 41 dodge auto f 16 22## 42 dodge auto f 17 24## 43 dodge auto f 17 24## 44 dodge auto f 11 17## 45 dodge auto f 15 22## 46 dodge auto f 15 21## 47 dodge auto f 16 23## 48 dodge auto f 16 23## 49 dodge manual 4 15 19## 50 dodge auto 4 14 18## 51 dodge auto 4 13 17## 52 dodge manual 4 14 17## 53 dodge auto 4 14 19## 54 dodge auto 4 14 19## 55 dodge auto 4 9 12## 56 dodge manual 4 11 17## 57 dodge auto 4 11 15## 58 dodge auto 4 13 17## 59 dodge auto 4 13 17## 60 dodge auto 4 9 12## 61 dodge auto 4 13 17## 62 dodge auto 4 11 16## 63 dodge auto 4 13 18## 64 dodge auto 4 11 15## 65 dodge manual 4 12 16## 66 dodge auto 4 9 12## 67 dodge auto 4 13 17## 68 dodge auto 4 13 17## 69 dodge manual 4 12 16## 70 dodge manual 4 9 12## 71 dodge auto 4 11 15## 72 dodge manual 4 11 16## 73 dodge auto 4 13 17## 74 dodge auto 4 11 15## 75 ford auto r 11 17## 76 ford auto r 11 17## 77 ford auto r 12 18## 78 ford auto 4 14 17## 79 ford manual 4 15 19## 80 ford auto 4 14 17## 81 ford auto 4 13 19## 82 ford auto 4 13 19## 83 ford auto 4 13 17## 84 ford auto 4 14 17## 85 ford manual 4 14 17## 86 ford manual 4 13 16## 87 ford auto 4 13 16## 88 ford auto 4 13 17## 89 ford auto 4 11 15## 90 ford auto 4 13 17## 91 ford manual r 18 26## 92 ford auto r 18 25## 93 ford manual r 17 26## 94 ford auto r 16 24## 95 ford auto r 15 21## 96 ford manual r 15 22## 97 ford manual r 15 23## 98 ford auto r 15 22## 99 ford manual r 14 20## 100 honda manual f 28 33## 101 honda auto f 24 32## 102 honda manual f 25 32## 103 honda manual f 23 29## 104 honda auto f 24 32## 105 honda manual f 26 34## 106 honda auto f 25 36## 107 honda auto f 24 36## 108 honda manual f 21 29## 109 hyundai auto f 18 26## 110 hyundai manual f 18 27## 111 hyundai auto f 21 30## 112 hyundai manual f 21 31## 113 hyundai auto f 18 26## 114 hyundai manual f 18 26## 115 hyundai auto f 19 28## 116 hyundai auto f 19 26## 117 hyundai manual f 19 29## 118 hyundai manual f 20 28## 119 hyundai auto f 20 27## 120 hyundai auto f 17 24## 121 hyundai manual f 16 24## 122 hyundai manual f 17 24## 123 jeep auto 4 17 22## 124 jeep auto 4 15 19## 125 jeep auto 4 15 20## 126 jeep auto 4 14 17## 127 jeep auto 4 9 12## 128 jeep auto 4 14 19## 129 jeep auto 4 13 18## 130 jeep auto 4 11 14## 131 land rover auto 4 11 15## 132 land rover auto 4 12 18## 133 land rover auto 4 12 18## 134 land rover auto 4 11 15## 135 lincoln auto r 11 17## 136 lincoln auto r 11 16## 137 lincoln auto r 12 18## 138 mercury auto 4 14 17## 139 mercury auto 4 13 19## 140 mercury auto 4 13 19## 141 mercury auto 4 13 17## 142 nissan manual f 21 29## 143 nissan auto f 19 27## 144 nissan auto f 23 31## 145 nissan manual f 23 32## 146 nissan manual f 19 27## 147 nissan auto f 19 26## 148 nissan auto f 18 26## 149 nissan manual f 19 25## 150 nissan auto f 19 25## 151 nissan auto 4 14 17## 152 nissan manual 4 15 17## 153 nissan auto 4 14 20## 154 nissan auto 4 12 18## 155 pontiac auto f 18 26## 156 pontiac auto f 16 26## 157 pontiac auto f 17 27## 158 pontiac auto f 18 28## 159 pontiac auto f 16 25## 160 subaru manual 4 18 25## 161 subaru auto 4 18 24## 162 subaru manual 4 20 27## 163 subaru manual 4 19 25## 164 subaru auto 4 20 26## 165 subaru auto 4 18 23## 166 subaru auto 4 21 26## 167 subaru manual 4 19 26## 168 subaru manual 4 19 26## 169 subaru auto 4 19 26## 170 subaru auto 4 20 25## 171 subaru auto 4 20 27## 172 subaru manual 4 19 25## 173 subaru manual 4 20 27## 174 toyota manual 4 15 20## 175 toyota auto 4 16 20## 176 toyota auto 4 15 19## 177 toyota manual 4 15 17## 178 toyota auto 4 16 20## 179 toyota auto 4 14 17## 180 toyota manual f 21 29## 181 toyota auto f 21 27## 182 toyota manual f 21 31## 183 toyota auto f 21 31## 184 toyota auto f 18 26## 185 toyota manual f 18 26## 186 toyota auto f 19 28## 187 toyota auto f 21 27## 188 toyota manual f 21 29## 189 toyota manual f 21 31## 190 toyota auto f 22 31## 191 toyota auto f 18 26## 192 toyota manual f 18 26## 193 toyota auto f 18 27## 194 toyota auto f 24 30## 195 toyota auto f 24 33## 196 toyota manual f 26 35## 197 toyota manual f 28 37## 198 toyota auto f 26 35## 199 toyota auto 4 11 15## 200 toyota auto 4 13 18## 201 toyota manual 4 15 20## 202 toyota auto 4 16 20## 203 toyota manual 4 17 22## 204 toyota manual 4 15 17## 205 toyota auto 4 15 19## 206 toyota manual 4 15 18## 207 toyota auto 4 16 20## 208 volkswagen manual f 21 29## 209 volkswagen auto f 19 26## 210 volkswagen manual f 21 29## 211 volkswagen auto f 22 29## 212 volkswagen manual f 17 24## 213 volkswagen manual f 33 44## 214 volkswagen manual f 21 29## 215 volkswagen auto f 19 26## 216 volkswagen auto f 22 29## 217 volkswagen manual f 21 29## 218 volkswagen auto f 21 29## 219 volkswagen manual f 21 29## 220 volkswagen auto f 16 23## 221 volkswagen manual f 17 24## 222 volkswagen manual f 35 44## 223 volkswagen auto f 29 41## 224 volkswagen manual f 21 29## 225 volkswagen auto f 19 26## 226 volkswagen manual f 20 28## 227 volkswagen auto f 20 29## 228 volkswagen manual f 21 29## 229 volkswagen auto f 18 29## 230 volkswagen auto f 19 28## 231 volkswagen manual f 21 29## 232 volkswagen auto f 16 26## 233 volkswagen manual f 18 26## 234 volkswagen auto f 17 26 1str(mpg1) 123456## 'data.frame': 234 obs. of 5 variables:## $ manufacturer: chr &quot;audi&quot; &quot;audi&quot; &quot;audi&quot; &quot;audi&quot; ...## $ trans : chr &quot;auto&quot; &quot;manual&quot; &quot;manual&quot; &quot;auto&quot; ...## $ drv : chr &quot;f&quot; &quot;f&quot; &quot;f&quot; &quot;f&quot; ...## $ cty : int 18 21 20 21 16 18 18 18 16 20 ...## $ hwy : int 29 29 31 30 26 26 27 26 25 28 ... 1mean(mtcars$mpg)#평균 1## [1] 20.09062 1var(mtcars$mpg)#분산 1## [1] 36.3241 1sd(mtcars$mpg)#표준편차 1## [1] 6.026948 #기술통계-표준편차를 이용하여 그 데이터의 생김새를 상상할 수 있는 통계법#사분위수#IQR:1사분위수와 3사분위수#통계분석/머신러닝-&gt;왜곡된 이상치를 제거할 때 사용 1quantile(mtcars$mpg) 12## 0% 25% 50% 75% 100% ## 10.400 15.425 19.200 22.800 33.900 #통계 요약 구하기(p90) 1summary(iris) 1234567891011121314## Sepal.Length Sepal.Width Petal.Length Petal.Width ## Min. :4.300 Min. :2.000 Min. :1.000 Min. :0.100 ## 1st Qu.:5.100 1st Qu.:2.800 1st Qu.:1.600 1st Qu.:0.300 ## Median :5.800 Median :3.000 Median :4.350 Median :1.300 ## Mean :5.843 Mean :3.057 Mean :3.758 Mean :1.199 ## 3rd Qu.:6.400 3rd Qu.:3.300 3rd Qu.:5.100 3rd Qu.:1.800 ## Max. :7.900 Max. :4.400 Max. :6.900 Max. :2.500 ## Species ## setosa :50 ## versicolor:50 ## virginica :50 ## ## ## 1summary(iris$Sepal.Length) 12## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 4.300 5.100 5.800 5.843 6.400 7.900 1table(mpg1$trans) 123## ## auto manual ## 157 77 1table(mpg1$manufacturer) 1234567## ## audi chevrolet dodge ford honda hyundai jeep ## 18 19 37 25 9 14 8 ## land rover lincoln mercury nissan pontiac subaru toyota ## 4 3 4 13 5 14 34 ## volkswagen ## 27 1table(mpg1$trans,mpg1$manufacturer) 12345678## ## audi chevrolet dodge ford honda hyundai jeep land rover lincoln## auto 11 16 30 17 4 7 8 4 3## manual 7 3 7 8 5 7 0 0 0## ## mercury nissan pontiac subaru toyota volkswagen## auto 4 8 5 7 20 13## manual 0 5 0 7 14 14 #빈도의 비율 구하기 12a&lt;-table(mpg1$trans)prop.table(a) 123## ## auto manual ## 0.6709402 0.3290598 12b&lt;-table(mpg1$trans,mpg1$drv)prop.table(b) 1234## ## 4 f r## auto 0.32051282 0.27777778 0.07264957## manual 0.11965812 0.17521368 0.03418803 1prop.table(table(mpg1$manufacturer)) 1234567## ## audi chevrolet dodge ford honda hyundai jeep ## 0.07692308 0.08119658 0.15811966 0.10683761 0.03846154 0.05982906 0.03418803 ## land rover lincoln mercury nissan pontiac subaru toyota ## 0.01709402 0.01282051 0.01709402 0.05555556 0.02136752 0.05982906 0.14529915 ## volkswagen ## 0.11538462 #행과 열의 비율 형식 맞추기(각각의 행열을 1이되게) 1?prop.table 1## httpd 도움말 서버를 시작합니다 ... 완료 1prop.table(b,margin = 1)#행의 합이 1 1234## ## 4 f r## auto 0.4777070 0.4140127 0.1082803## manual 0.3636364 0.5324675 0.1038961 1prop.table(b,margin = 2)#열의 합이 1 1234## ## 4 f r## auto 0.7281553 0.6132075 0.6800000## manual 0.2718447 0.3867925 0.3200000 #소수점 아래 자리 지정 1round(0.322323,2)#round(정보값,자릿수) 1## [1] 0.32 1round(prop.table(table(mpg1$manufacturer)),2) 1234567## ## audi chevrolet dodge ford honda hyundai jeep ## 0.08 0.08 0.16 0.11 0.04 0.06 0.03 ## land rover lincoln mercury nissan pontiac subaru toyota ## 0.02 0.01 0.02 0.06 0.02 0.06 0.15 ## volkswagen ## 0.12 12a= table(mpg1$trans)a 123## ## auto manual ## 157 77 12b= prop.table(a)b 123## ## auto manual ## 0.6709402 0.3290598 1round(b,2) 123## ## auto manual ## 0.67 0.33 #시각화 이미지##보통은 ggplot2 패키지를 사용하지만 테이블 이미지를 이쁘게 사용하고 싶으면 gt table 패키지도 있다.고급","link":"/2022/06/21/220621/"},{"title":"220622","text":"라이브러리 불러오기1library(dplyr) 12## ## 다음의 패키지를 부착합니다: 'dplyr' 123## The following objects are masked from 'package:stats':## ## filter, lag 123## The following objects are masked from 'package:base':## ## intersect, setdiff, setequal, union 1library(ggplot2) 데이터 불러오기 데이터를 불러오세요 12exam_na &lt;- read.csv(&quot;data/exam_na.csv&quot;)str(exam_na) 123456## 'data.frame': 5 obs. of 5 variables:## $ id : int 1 2 3 4 5## $ sex : chr &quot;M&quot; &quot;F&quot; &quot;F&quot; &quot;M&quot; ...## $ korean : int 87 92 95 NA 87## $ english: int NA 95 92 84 NA## $ math : int 82 93 90 80 88 결측치 확인 후 빈도 구하기(1) 결측치 확인1is.na(exam_na) 123456## id sex korean english math## [1,] FALSE FALSE FALSE TRUE FALSE## [2,] FALSE FALSE FALSE FALSE FALSE## [3,] FALSE FALSE FALSE FALSE FALSE## [4,] FALSE FALSE TRUE FALSE FALSE## [5,] FALSE FALSE FALSE TRUE FALSE (2)결측치 빈도 구하기-table():빈도구하기 1table(is.na(exam_na)) 123## ## FALSE TRUE ## 22 3 1table(is.na(exam_na$korean)) 123## ## FALSE TRUE ## 4 1 1summary(is.na(exam_na)) 12345678## id sex korean english ## Mode :logical Mode :logical Mode :logical Mode :logical ## FALSE:5 FALSE:5 FALSE:4 FALSE:3 ## TRUE :1 TRUE :2 ## math ## Mode :logical ## FALSE:5 ## 1summary(exam_na) 12345678## id sex korean english math ## Min. :1 Length:5 Min. :87.00 Min. :84.00 Min. :80.0 ## 1st Qu.:2 Class :character 1st Qu.:87.00 1st Qu.:88.00 1st Qu.:82.0 ## Median :3 Mode :character Median :89.50 Median :92.00 Median :88.0 ## Mean :3 Mean :90.25 Mean :90.33 Mean :86.6 ## 3rd Qu.:4 3rd Qu.:92.75 3rd Qu.:93.50 3rd Qu.:90.0 ## Max. :5 Max. :95.00 Max. :95.00 Max. :93.0 ## NA's :1 NA's :2 결측치 처리 방법-제거하고 처리하기 -다른 값으로 대체하기 +평균입력 (1)결측치를 제외하고 분석하기 -p160 na.rm=T1mean(exam_na$korean,na.rm = T) 1## [1] 90.25 -na.omit() -결측치가 있는 행을 모두 제거 +가급적 쓰지 말것 -filter()활용 +is.na(korea) 1exam_na %&gt;% filter(is.na(korean)) 12## id sex korean english math## 1 4 M NA 84 80 -이번에는 !is.na(korean)을 적용한다 1exam_na %&gt;% filter(!is.na(korean)) 12345## id sex korean english math## 1 1 M 87 NA 82## 2 2 F 92 95 93## 3 3 F 95 92 90## 4 5 F 87 NA 88 (2) 결측치를 다른 값으로 대체하기 imputation 참고자료 A Solution to Missing Data: Imputation Using R R 결측값(NA) 제거, 대체 방법 이상치-데이터의 특정 값이 뭔가 “이상”이 있다. -case 1: 정해진 범주에서 벗어난 데이터 ex)2월 31일 -case 2: 숫자 /아웃라이어(outlier)/극단값 ex) 평균임금에서 삼성 이재용이 들어가면 안됨 12mpg1_out&lt;-read.csv(&quot;data/mpg1_out.csv&quot;)glimpse(mpg1_out) 12345## Rows: 234## Columns: 3## $ trans &lt;int&gt; 1, 2, 2, 1, 1, 2, 1, 2, 1, 2, 1, 1, 2, 1, 2, 1, 1, 1, 1, 3, 1, 1…## $ drv &lt;chr&gt; &quot;f&quot;, &quot;f&quot;, &quot;f&quot;, &quot;f&quot;, &quot;f&quot;, &quot;f&quot;, &quot;f&quot;, &quot;4&quot;, &quot;4&quot;, &quot;4&quot;, &quot;4&quot;, &quot;4&quot;, &quot;5&quot;,…## $ cty &lt;int&gt; 18, 21, 20, 21, 16, 18, 18, 18, 16, 20, 19, 15, 17, 17, 15, 15, … -trans의 빈도의갯수를 구하면 1이 몇개 2가 몇개 3이 몇개가 나온다 1table(mpg1_out$trans) 123## ## 1 2 3 ## 154 76 4 -만약 3을 그냥 제거하고 싶으면 다음과 같이 해도 된다. -mpg1_out %&gt;% filter(trans !=3) -ifelse란? -만약~라면 내가 무엇을 할 것이다.(가정법) -만약 trans의 값이 3이라면 결측치로 바꿔주세요.나머지는 그대로 유지하세요. 123mpg1_out$trans&lt;-ifelse(mpg1_out$trans == 3 ,NA ,mpg1_out$trans)table(is.na(mpg1_out$trans)) 123## ## FALSE TRUE ## 230 4 -결측치 제거 12result &lt;-mpg1_out %&gt;% filter(!is.na(trans))table(is.na(result$trans)) 123## ## FALSE ## 230 극단치 처리-숫자 데이터 boxplot() -boxlot() 함수를 통해서 극단치가 있는지 없는지 확인 가능 -IQR:3사분위-1사분위 -경계값:IQR+IQR1.5 상한/IQR-IQR1.5 123mpg1&lt;- read.csv(&quot;data/mpg1.csv&quot;)boxplot(mpg1$cty)boxplot(mpg1$cty)$stats 123456## [,1]## [1,] 9## [2,] 14## [3,] 17## [4,] 19## [5,] 26","link":"/2022/06/22/day0622/"},{"title":"0623","text":"복습-iris 데이터, sepal.length, sepal. width활용해서 종별로 산점도를 그리세요-제목과 x축 y축 제목을 변경하세요. +x축 길이 , y축 너비 12library(ggplot2)str(iris) 123456## 'data.frame': 150 obs. of 5 variables:## $ Sepal.Length: num 5.1 4.9 4.7 4.6 5 5.4 4.6 5 4.4 4.9 ...## $ Sepal.Width : num 3.5 3 3.2 3.1 3.6 3.9 3.4 3.4 2.9 3.1 ...## $ Petal.Length: num 1.4 1.4 1.3 1.5 1.4 1.7 1.4 1.5 1.4 1.5 ...## $ Petal.Width : num 0.2 0.2 0.2 0.2 0.2 0.4 0.3 0.2 0.2 0.1 ...## $ Species : Factor w/ 3 levels &quot;setosa&quot;,&quot;versicolor&quot;,..: 1 1 1 1 1 1 1 1 1 1 ... 1234567891011121314ggplot(iris, aes( x = Sepal.Length, y = Sepal.Width, colour =Species))+ geom_point()+ labs( title =&quot; 제목&quot;, x = &quot;길이&quot;, y = &quot;너비&quot;, )+ scale_color_manual( labels = c(&quot;setosa&quot;,&quot;versicolor&quot;,&quot;virginica&quot;), values = c(&quot;pink&quot;,&quot;orange&quot;,&quot;blue&quot;) )+ theme_classic() – 색깔을 부꿀 수 있는 명령어 scale_color_manual– 테마를 바꾸는 옵션 theme_classic()-&gt; 농도가 각각 다르게 함.– ggstatsplot라는 것도 통계그림 그리기에 좋음 통계 기술통계 :평균, 최촛값, 최댓값, 중간값 추론통계 : 변수간의 관계를 파악/새로운 사실을 발견(추정)+평균차이 검정 -수치데이터 -가설검정 : 평균의 차이를 검정 -남자의 평균 키와 여자의 평균 키는 차이가 있을 것이다. (남자 여자라는 그룹을 평균을 기준으로 비교 ) +교차분석(빈도분석) -범주 데이터 -가설검정 : 각 범주별 빈도를 활용하여 관계성을 검정 +상관관계 분석 -수치 데이터 -변수사이의 연관성을 수치로 표현 +단순 회귀분석 -y(종속변수)= a(기울기=회기계수)x(독립변수)+b(절편=상수항:의미없음) -가설검정-&gt;기온x(독립변수)이 판매량y(종속변수)에 긍정적(+)a영향을 준다.+다중 회귀분석 -y=a1x1+a2x2+a3x3….+b 통계 검정 P176 가설 -hypothesis –&gt;공부방법: 선행연구, 해당분석방법의 가설 설정 예시를 보고 연습 -연구:내가 궁금한 것을 찾는 것 ex)남자와 여자의 평균키가 차이가 있다. -귀무가설:궁금한 것의 반대내용(ex.남여 평균키 차이가 없다.) -대립가설:궁금한 사항이 여기로 옴(ex.남여 평균키 차이가 있다) -가설 검정에서 인정하는 유의 수준:5%,1%,0.1% 또는 10% -오차 범위 내에 있으면 차이가 크지 않음. 오차 범위 밖에 있으면 결과가 결정됨.(신뢰수준&lt;-&gt;유의수준) -유의수준&gt;0.05가 넘으면 데이터의 의미가 없어진다.귀무가설 실패 t.test-어떻게 데이터를 입력하는지 확인-p-value, 유의수준 0.05이상: 귀무가설(p&gt;0.05) 0.05이내: 대립가설(p&lt;0.05) 12mpg1&lt;-read.csv(&quot;data/mpg1.csv&quot;,stringsAsFactors = F)str(mpg1) 123456## 'data.frame': 234 obs. of 5 variables:## $ manufacturer: chr &quot;audi&quot; &quot;audi&quot; &quot;audi&quot; &quot;audi&quot; ...## $ trans : chr &quot;auto&quot; &quot;manual&quot; &quot;manual&quot; &quot;auto&quot; ...## $ drv : chr &quot;f&quot; &quot;f&quot; &quot;f&quot; &quot;f&quot; ...## $ cty : int 18 21 20 21 16 18 18 18 16 20 ...## $ hwy : int 29 29 31 30 26 26 27 26 25 28 ... -시각화 +여기서는 큰 차이를 알 수가 없다. 123library(ggplot2)ggplot(mpg1, aes(x = trans, y = cty)) + geom_boxplot() -t.test 검정 +귀무가설 :auto와 manual의 cty평균은 차이가 없다. +대립가설 :auto와 manual의 cty평균은 차이가 있다. 1t.test(data = mpg1, cty ~ trans) 1234567891011## ## Welch Two Sample t-test## ## data: cty by trans## t = -4.5375, df = 132.32, p-value = 1.263e-05## alternative hypothesis: true difference in means between group auto and group manual is not equal to 0## 95 percent confidence interval:## -3.887311 -1.527033## sample estimates:## mean in group auto mean in group manual ## 15.96815 18.67532 1#종속변수 ~ 독립변수 Y ~ X 반응변수 ~ 설명변수 –시각화한 것은 차이를 모르겠지만 통계를 통해 차이가 있음을 알 수 있다.(대립가설 성립)P-value&lt;0.5 -cf..두 그룹의 평균 차이 검정하기 전에 +사전 필수 검증요소가 있다.바로 등분산 검정 +등분산 검정 -&gt;두 그룹간의 분산이 비슷하면 –&gt;t.test(모수검정) -&gt;두 그룹간의 분산이 다르면–&gt;(비모수검정) -&gt;귀무가설: 두 그룹간의 분산이 비슷하다 p.value&gt;0.05 -&gt;대립가설: 두 그룹간의 분산이 다르다. 1var.test(data =mpg1, cty~trans) 1234567891011## ## F test to compare two variances## ## data: cty by trans## F = 0.73539, num df = 156, denom df = 76, p-value = 0.1101## alternative hypothesis: true ratio of variances is not equal to 1## 95 percent confidence interval:## 0.4912917 1.0719468## sample estimates:## ratio of variances ## 0.7353887 p.value&gt;0.05이므로 귀무가설 성립. 등분산으로 본다. -시각화 123ggplot(mpg1, aes(x = cty, fill = trans)) + # geom_histogram() + geom_density(alpha = 0.1) 교차 분석 -범주형 변수들이 관계가 있다는 것을 검정 -비율에 차이가 있는 지 검정 -교차분석 검정은 R의 chisq.test()함수로 진행 -귀무가설: tras에 따라 drv(4,f,r)의 (비율)차이가 없다. -대립가설: tras에 따라 drv의 차이가 있다. -빈도표/비율 -1빈도표 1table(mpg1$trans,mpg1$drv)#교차분석석 1234## ## 4 f r## auto 75 65 17## manual 28 41 8 -2비율 1prop.table(table(mpg1$trans,mpg1$drv),1) 1234## ## 4 f r## auto 0.4777070 0.4140127 0.1082803## manual 0.3636364 0.5324675 0.1038961 -auto 4륜구동이 47% -manual 전륜구동이 53% 가장 많음 -실제로 통계적으로 봤을 때 차이가 있는지 검정 -귀무가설: tras에 따라 drv(4,f,r)의 (비율)차이가 없다. -대립가설: tras에 따라 drv의 차이가 있다. 1chisq.test(mpg1$trans,mpg1$drv) 12345## ## Pearson's Chi-squared test## ## data: mpg1$trans and mpg1$drv## X-squared = 3.1368, df = 2, p-value = 0.2084 p.value&gt;0.05이므로 귀무가설 성립. 차이가 없다.","link":"/2022/06/23/day0623/"},{"title":"0624","text":"복습 -통계검정 평균차이검정 :(두그룹간의)수치데이터&amp;범주데이터-&gt;세그룹이상 평균 차이 검정(중급이상이라면) 비율차이검정(교차분석) :범주 데이터 상관관계 :수치 데이터 회귀 -통계검정을 할 때는 분석을 위한 데이터가 적절한지 검정 등분산 검정, 수치 데이터가 정규분포를 이루는가?(정규성 검정) -귀무가설 , 대립가설을 적절하게 설정 서울이 부산보다 잘산다(X)-&gt;서울의 평균임금과 부산의 평균임금이 차이가 있다. 선행연구(논문등)을 찾아 응용 -테스트 실시 +함수 t.test, chisq.test, cor.test를 통해 P.value를 찾음 +P.value -&gt;P.value&gt;0.5–&gt;귀무가설 지지 -&gt;P.value&lt;0.5–&gt;대립가설 지지 ##회귀분석(p.184) 회귀가 재일 중요하다~ 기초통계 : 특정한 결과에 영향을 주는 주 요인이 무엇인가? 이걸 찾는 것이 회귀 회귀분석과 종류 1세대 회귀 방법론: 다항회귀분석, 다중회귀분석, 포아송 회귀분석 등 2세대 회귀 방법론: 구조방정식 귀무가설 &amp; 대립가설 존재 귀무가설 : x(=독립변수)가 y(=종속변수)에 영향을 주지 않는다. 대립가설 : x가 y에 영향을 준다 -lm(종속변수~독립변수, data)-&gt;p.185 p.value로 대립가설을 확인 anova(분산분석)y=ax+b+a1x1+a2x2+a3x3….. 실무에서는 독립변수x1를 계속 변경해봐서 대립가설이 되는지 여부를 찾아본다.-&gt; 독립변수가 너무 많으면 전진소거법,후진소거법을 사용한다. R_Squared(결정계수)=설명력 = 0~1 -&gt;1로 수렴할 수록 설명력이 좋다. 123RA &lt;- lm(data=mtcars,mpg ~ disp)summary(RA) 123456789101112131415161718## ## Call:## lm(formula = mpg ~ disp, data = mtcars)## ## Residuals:## Min 1Q Median 3Q Max ## -4.8922 -2.2022 -0.9631 1.6272 7.2305 ## ## Coefficients:## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 29.599855 1.229720 24.070 &lt; 2e-16 ***## disp -0.041215 0.004712 -8.747 9.38e-10 ***## ---## Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1## ## Residual standard error: 3.251 on 30 degrees of freedom## Multiple R-squared: 0.7183, Adjusted R-squared: 0.709 ## F-statistic: 76.51 on 1 and 30 DF, p-value: 9.38e-10 -머신러닝, 인공지능 주 목적은 예측 Y= aX+b","link":"/2022/06/24/day0624/"},{"title":"파이선 기초문법:::","text":"Hello World 1print (&quot;Hello World&quot;) Hello World 주석처리 1줄 주석, 여러줄 주석처리: 여러줄 주석 처리 함수 또는 클래스를 문서화 할때 주로 사용 프로잭트 할 때 전체 공정100 코드 / 코드 문서화 / 한글작업 문서화 12345678910# print()함수 사용print(&quot;1줄주석&quot;)&quot;&quot;&quot;&quot;여러 줄의 주석에는 쌍따옴표 3개를 입력해주세요앞과 뒤로&quot;&quot;&quot;print(&quot;여러줄 주석&quot;) 1줄주석 여러줄 주석 변수 (Scalar) 자료형 Scalar형 Non-Scalar형 수치형 자료형 int, float 1234num_int =1print(num_int)print(type(num_int)) 1 &lt;class 'int'&gt; 1234num_float =0.1print(num_float)print(type(num_float)) 0.1 &lt;class 'float'&gt; Bool형 True,False cf)R: TRUE, FALSE 123bool_true =Trueprint(bool_true)print(type(bool_true)) True &lt;class 'bool'&gt; None 자료형 Null값- 값이 정해지지 않은 자료형 123none_x= Noneprint(none_x)print(type(none_x)) None &lt;class 'NoneType'&gt; 사칙연산 정수형 사칙연산, 실수형 사칙연산 결괏값의 자료형 정수형 사칙연산 +.-,*,/ 단 정수형 나누기는 실수형으로 바뀐다 123456a = 3b = 2print('a+b=',a+b)print('a-b=',a-b)print('a*b=',a*b)print('a/b=',a/b) a+b= 5 a-b= 1 a*b= 6 a/b= 1.5 실수형 사칙연산123456a = 1.5b = 2.5print('a+b=', a+b)print('a-b=',a-b)print('a*b=',a*b)print('a/b=',a/b) a+b= 4.0 a-b= -1.0 a*b= 3.75 a/b= 0.6 논리형 연산자 bool형 True와 False 값으로 정의 조건식 교집합(=and), 합집합(=or) 1234print(True and True)print(True and False)print(False and True)print(False and False) True False False False 1234print(True or True)print(True or False)print(False or True)print(False or False) True True True False 비교 연산자 비교연산자는 부등호를 의미한다. 1234print(4&gt;3) #참 = Trueprint(4&lt;3) #거짓 = Falseprint(4&gt;3 or 4&lt;3) # False True False True 논리형 &amp; 비교 연산자 응용 input() 형변환 데이터 타입을 바꾸는 것 123var = input(&quot;숫자를 입력하세요!&quot;)print(var)print(type(var)) 숫자를 입력하세요!3 3 &lt;class 'str'&gt; 123var = int(input(&quot;숫자를 입력하세요!&quot;))print(var)print(type(var)) 숫자를 입력하세요!1 1 &lt;class 'int'&gt; –계산이 안될 때 타입을 확인해보고 숫자로 형변환 해주세요 1234num1 = int(input(&quot;첫번째 숫자를 입력하세요.&quot;))num2 = int(input(&quot;두번째 숫자를 입력하세요.&quot;))num3 = int(input(&quot;세번째 숫자를 입력하세요.&quot;))num4 = int(input(&quot;네번째 숫자를 입력하세요.&quot;)) 첫번째 숫자를 입력하세요.40 두번째 숫자를 입력하세요.50 세번째 숫자를 입력하세요.60 네번째 숫자를 입력하세요.70 12var1 = num1 &lt;= num2 var2 = num3 &gt; num4 12print( var1 and var2) False String Non Scalar 12345print('hello World')print(&quot;hello World&quot;)print(&quot;'hello World'&quot;)print('&quot;hello World&quot;') hello World hello World 'hello World' &quot;hello World&quot; String Operators 문자열 연산자 +, * 가능 123str1 = &quot;Hello &quot;str2 = &quot;World &quot;print(str1 + str2) Hello World 12greet = str1 + str2print(greet * 2) Hello World Hello World 문자열 인덱싱 인덱싱은 0번째부터 시작 123greeting='hello kaggle'print(greeting)print(greeting[0]) hello kaggle h 123greeting= &quot;hello kabble&quot;i=int(input(&quot;숫자를 입력하세요&quot;))print(greeting[i]) 숫자를 입력하세요3 l 슬라이싱123456789greeting = &quot;hello kaggle&quot;#print(greeting[시작인덱스:끝인덱스-1])print(greeting[0:2])print(greeting[0:8])print(greeting[:8])print(greeting[6:8])print(greeting[0:10:2])print(greeting[0:10:3])print(greeting[0:10:4]) he hello ka hello ka ka hlokg hlkg hog 123alphabet_letter = &quot;1234567890qbcde&quot;print(alphabet_letter[0:14:2])print(alphabet_letter[0:14:3]) 13579qc 1470c 인덱스가 넘어갈때 뜨는 에러 메세지 12greeting = &quot;hello Kabble&quot;print(greeting[100]) --------------------------------------------------------------------------- IndexError Traceback (most recent call last) &lt;ipython-input-84-40fb216685e4&gt; in &lt;module&gt;() 1 greeting = &quot;hello Kabble&quot; ----&gt; 2 print(greeting[100]) IndexError: string index out of range 문자열 관련 메서드 split() sort() etc 리스트 [] (대괄호)로 표시 [item1, item2, item3..] 1234567a = []#빈 리스트a_func = list() # 빈 리스트 생성b= [1] #숫자요소c=['apple'] #문자요소d = [1,2,['apple'],'apple']#리스트 자체를 리스트 요소로 만들 수도 있다print(a,b,c)print(d) [] [1] ['apple'] [1, 2, ['apple'], 'apple'] 리스트 값 수정하기 리스트 값 수정 123a =[ 0,1,2]a[0] = &quot;아무값&quot;print (a) ['아무값', 1, 2] 메서드 사용 123456789a = [100, 200, 300]a.append(400)print(a)a.append([500,600])print(a)a.extend([700,800])print(a) [100, 200, 300, 400] [100, 200, 300, 400, [500, 600]] [100, 200, 300, 400, [500, 600], 700, 800] insert(인덱스 위치, 값) 123a = [100,200,300 ]a.insert(1,1000)print(a) [100, 1000, 200, 300] remover() 리스트에서 첫 번째로 나오는 x를 삭제하는 함수이다 123a = [1,2,1,2]a.remove(1)print(a) [2, 1, 2] del 123456a = [0,1,2,3,4,5,6,7,8,9]del a[1]print(a)del a[0:2]print(a) [0, 2, 3, 4, 5, 6, 7, 8, 9] [3, 4, 5, 6, 7, 8, 9] pop() : pop()은 리스트의 맨 마지막 요소를 돌려주고 그 요소는 삭제한다 1234567a = [1,2,3,4,5]rem = a.pop(1)print(a)print(rem)x = a.pop()print(a)print(x) [1, 3, 4, 5] 2 [1, 3, 4] 5 clear() : 리스트 내 모든 값 삭제 index(“값”) : 값의 위치를 불러옴 1234a= [1,4,5,2,3]b= [&quot;철수&quot;,&quot;영희&quot;,&quot;길동&quot;]print(a.index(3))print(b.index(&quot;영희&quot;)) 4 1 sort : 리스트의 정렬 123456a= [1,4,5,2,3]a.sort()print(a)help(list.sort)#help(list.index) [1, 2, 3, 4, 5] Help on method_descriptor: sort(self, /, *, key=None, reverse=False) Stable sort *IN PLACE*. 튜플 면접질문 :리스트와 튜플의 차이가 뭔가요? 형태로 리스트는 [ ]으로 둘러싸지만 튜플은 ( )으로 둘러싼다. 리스트는 그 값의 생성, 삭제, 수정이 가능하지만 튜플은 그 값을 바꿀 수 없다 1234567891011tuple1 = (0)tuple2 = (0,)tuple3 = 0, 1, 2print(type(tuple1))print(type(tuple2))print(type(tuple2))print(tuple1)print(tuple2)print(tuple3) &lt;class 'int'&gt; &lt;class 'tuple'&gt; &lt;class 'tuple'&gt; 0 (0,) (0, 1, 2) 튜플은 소가로()가 있고 ,(쉼표)가 있다. 정수는 숫자만 있다. 123a = (0,1,2,3,'a')#del a[4]#print(a)-&gt;에러가 뜨며 tuple' object doesn't support item deletion 튜플(=리스트)연산자 문자열 연산자 +, * 1234t1 = (0,1,2)t2 = (3,4,5)print(t1 +t2)print(t1 * 2) (0, 1, 2, 3, 4, 5) (0, 1, 2, 0, 1, 2) 딕셔너리 Dictionary 키(Key)와 값(value)으로 구성됨 슬라이싱!= (값의 순서가 존재해야 됨) 그러나 딕셔너리는 순서라는 개념자체가 존재하지 않기에 슬라이싱이 안됨(cf.0부터 시작된다는 개념 자체도 없으니 조심) 1234567891011temp_dict = { #'키' : '값' 'teacher' : 'evan', 'class' : 15, 'students' : ['s1', 's2','s3']}print(temp_dict[&quot;teacher&quot;])print(temp_dict['class'])print(temp_dict['students']) evan 15 ['s1', 's2', 's3'] key()값만 출 1list(temp_dict.keys()) ['teacher', 'class', 'students'] values()값만 출 1temp_dict.values() dict_values(['evan', 15, ['s1', 's2', 's3']]) items() key-value 쌍으로, list와 tuple 형태로 반환 1temp_dict.items() dict_items([('teacher', 'evan'), ('class', 15), ('students', ['s1', 's2', 's3'])]) 조건문 잊지말고 :를 찍으세요 123456789a = int(input(&quot;숫자를 입력하세요&quot;))if a&gt;5: print(&quot;a는 5보다 크다&quot;)elif a&gt;0: print(&quot;a는 0보다 크다&quot;)elif a&gt;-5: print(&quot;a는 -5보다 크다&quot;)else: print(&quot;a는 매우 작다&quot;) 숫자를 입력하세요2 a는 0보다 크다 반복문1234#Hello World 3번 출력하세요for idx in range(3): print(idx+1) print(&quot;Hello World, &quot;) 1 Hello World, 2 Hello World, 3 Hello World, 123#가독성이 좋게 하려고 인덱스의 숫자를 앞에 넣을 수도 있다.for idx in range(3): print(idx+1,&quot;Hello World, &quot;) 1 Hello World, 2 Hello World, 3 Hello World, for loop if 조건문 사용 문자열 ,리스트 등–&gt;시퀀스 데이터(반복문 사용가능) 123456a= &quot;kaggle&quot;# g가 시작하면 반복문을 멈추세요&quot;for x in a: print(x) if x== 'g': break k a g 123456a= &quot;kaggle&quot;# print(x)의 위치에 따라 출력이 달라진다&quot;for x in a: if x== 'g': breakprint(x) g enumerate() 123alphabets = ['A','B','c']for index, value in enumerate(alphabets): print(index,value) 0 A 1 B 2 c 리스트 컴프리헨션(list comprehension) 1&quot;a&quot; in &quot;kiwi&quot; False 123456789fruits = ['apple','kiwi','mango']newlists= []#알파벳 a가 있는 과일만 추출 후 새로운 리스트에 담기for fruit in fruits: #print(fruit) if &quot;a&quot; in fruit : newlists.append(fruit)print(newlists) ['apple', 'mango'] 123#리스트 컴프리헨션으로 표현하면newlist = [fruit for fruit in fruits if 'a' in fruit]print(newlist) ['apple', 'mango']","link":"/2022/06/27/day0627/"},{"title":"파이선 기초문법2+Numpy","text":"##복습-반복문 연습 for loop and while loop 123for i in range(3): print(&quot;Hellow World&quot;) print(&quot;안녕하세요&quot;) Hellow World 안녕하세요 Hellow World 안녕하세요 Hellow World 안녕하세요 123456for i in range(100): print(&quot;No:&quot;,i+1) if i == 10 : break print(&quot;Hellow World&quot;) print(&quot;안녕하세요&quot;) No: 1 Hellow World 안녕하세요 No: 2 Hellow World 안녕하세요 No: 3 Hellow World 안녕하세요 No: 4 Hellow World 안녕하세요 No: 5 Hellow World 안녕하세요 No: 6 Hellow World 안녕하세요 No: 7 Hellow World 안녕하세요 No: 8 Hellow World 안녕하세요 No: 9 Hellow World 안녕하세요 No: 10 Hellow World 안녕하세요 No: 11 12345&quot;k&quot; in &quot;kaggle&quot;if &quot;k&quot; == &quot;a&quot;: print(&quot;출력이 되나요?&quot;)else: print(&quot;출력이 안됨&quot;) 출력이 안됨 123456a = &quot;Kaggle&quot;for i in a: print(i) if i == &quot;a&quot;: break K a 리스트의 값이 존재 전체 총합구하기 12345678910numbers = [3,2,3,4,5]sum = 0for num in numbers: print(&quot;number:&quot;,num) sum = sum + num print(&quot;total:&quot;,sum)print(&quot;-----최종결괏값------&quot;)print(sum) number: 3 total: 3 number: 2 total: 5 number: 3 total: 8 number: 4 total: 12 number: 5 total: 17 -----최종결괏값------ 17 12345678910111213fruits = ['apple', 'kiwi', 'mango']newlist = []# apple : a가 있나요? 있으면 newlist에 추가하세요# kiwi에는 a가 있나요 없으면 그냥 넘어가요# mango : a가 있나요? 그럼 newlist에 추가하세요for fruit in fruits: print(&quot;조건문 밖:&quot;, fruit) if &quot;a&quot; in fruit: print(&quot;조건문 안:&quot;,fruit) newlist.append(fruit)print(newlist) 조건문 밖: apple 조건문 안: apple 조건문 밖: kiwi 조건문 밖: mango 조건문 안: mango ['apple', 'mango'] While Loop 분석할 때는 거의 사용안되고 개발할 때 사용 123456i=1while i&lt;10: #조건식 :참일때만 반복문 코드가 돔 (무한루프가 될 수 있다) # 코드 print(i) i+=1 #1씩 증감 하여 10이 되면 거짓이 되면 멈춤 #1-=1 #1씩 감소 1 2 3 4 5 6 7 8 9 사용자 정의 함수 내가 필요에 의해 직접 함수를 작성 함수 문서화 키워드 : Docsting -&gt;””” 내용 “”” 함수에는 반드시 설명이 들어가야 된다. 오늘 배운 것 중에 두번째로 중요한 내용!!! 123def 함수명(param1, param2): #코드 return None 1234567def add(a = 0,b = 1): # c = a + b # return c return a + bprint(add(a = 5, b = 4))print(add()) 9 1 사칙연산 사용자 정의 함수 만들기 123456789101112131415def de(a, b) : return a + bprint(de(1,2))def bbe(a, b) : return a - bprint(bbe(1,2))def kop(a, b) : return a * bprint(kop(1,2))def na(a, b) : return a / bprint(na(1,2))def mean(a , b): return (a + b) / 2print(mean(5,10)) 3 -1 2 0.5 7.5 123456a = int(input('첫번째수를 넣으세요'))b = int(input('두번째수를 넣으세요'))def mean (a, b): return (a + b)/2print(mean(a,b)) 첫번째수를 넣으세요5 두번째수를 넣으세요10 7.5 123456789101112131415def subtract(a, b): &quot;&quot;&quot; a,b를 빼는 함수 parameters: a(int): int형 숫자 a가 입력 b(int): int형 숫자 b가 입력 return: int :반환값 &quot;&quot;&quot; return a-bprint(subtract(a= 5, b= 10))print(subtract.__doc__) -5 a,b를 빼는 함수 parameters: a(int): int형 숫자 a가 입력 b(int): int형 숫자 b가 입력 return: int :반환값 Numpy 내장 모듈(=라이브러리=R의 패키지)이 아닌 별도 라이브러리를 설치해야 함 별도 라이브러리 설치가 필요(구글코랩은 불필요) 12import numpy as npprint(np.__version__) 1.21.6 12345temp = [1,2,3]temp_array = np.array(temp) # 리스트를 배열로 변환print (temp_array)print (type(temp_array)) [1 2 3] &lt;class 'numpy.ndarray'&gt; 사칙연산1234567891011math_score = [90, 80, 100]eng_score= [80, 90,100]#print(math_score+eng_score)math_array = np.array(math_score)eng_array= np.array(eng_score)total= math_array + eng_arrayprint(total)print(type(total)) [170 170 200] &lt;class 'numpy.ndarray'&gt; 집계함수123print(np.min(total))print(np.max(total))print(np.sum(total)) 170 200 540 차원 확인 배열의 차원 확인 필요 12345# 1차원 배열 생성temp_arr= np.array([1,2,3])print(temp_arr.shape)print(temp_arr.ndim) #차원을 나타냄print(temp_arr) (3,) 1 [1 2 3] 12345# 2차원 배열 생성temp_arr = np.array([[1,2,3],[4,5,6]])print(temp_arr.shape)print(temp_arr.ndim) #차원을 나타냄print(temp_arr) (2, 3) 2 [[1 2 3] [4 5 6]] 12345# 3차원 배열-&gt; 이미지temp_arr = np.array([[[1,2,3],[4,5,6]],[[7,8,9],[10,11,12]]])print(temp_arr.shape)print(temp_arr.ndim)print(temp_arr) (2, 2, 3) 3 [[[ 1 2 3] [ 4 5 6]] [[ 7 8 9] [10 11 12]]] 배열 생성의 다양한 방법들 모두 0으로 채운다 12import numpy as np print(np.__version__) 1.21.6 12temp_arr = np.zeros((4,2,3))temp_arr array([[[0., 0., 0.], [0., 0., 0.]], [[0., 0., 0.], [0., 0., 0.]], [[0., 0., 0.], [0., 0., 0.]], [[0., 0., 0.], [0., 0., 0.]]]) 모두 1로 채운다. 12temp_arr = np.ones((2,3))temp_arr array([[1., 1., 1.], [1., 1., 1.]]) 임의의 상수값으로 채운다(3개의 1행 3열자료에 100을 넣었다)3x1x3배열이라고 말한다. 12temp_arr = np.full((3,3),100.1)temp_arr array([[100.1, 100.1, 100.1], [100.1, 100.1, 100.1], [100.1, 100.1, 100.1]]) 2개의 3행 4열 자료에 100을 넣었다. 12temp_arr = np.full((2,3,4),100)temp_arr array([[[100, 100, 100, 100], [100, 100, 100, 100], [100, 100, 100, 100]], [[100, 100, 100, 100], [100, 100, 100, 100], [100, 100, 100, 100]]]) 최소, 최대 숫자의 범위를 정해두고,각 구간별로 값을 생성 .linspace(시작값, 종료값, 간격수) 12temp_arr = np.linspace(5,10,8)temp_arr array([ 5. , 5.71428571, 6.42857143, 7.14285714, 7.85714286, 8.57142857, 9.28571429, 10. ]) 반복문 시 , 자주 등장하는 배열 12temp_arr = np.arange(1,11,1)temp_arr array([ 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]) 난수 생성123from numpy import randomx = random.rand()print(x) 0.09081438142620213 123import numpyx = numpy.random.rand()print(x) 0.17530497899866515 위의 두가지는 같은 내용이다.랜덤함수의 위치를 명시해준다. 랜덤 정수값 추출 12345from numpy import random#x = random.randint(100, size = (5))x = random.randint(100, size = (3,5))print(x)print(type(x)) [[ 4 26 62 97 49] [68 51 15 95 55] [37 91 98 47 8]] &lt;class 'numpy.ndarray'&gt; 12345from numpy import random#x = random.randint(100, size = (5))x = random.randint(100, size = (3,2,4))print(x)print(type(x)) [[[10 16 7 84] [49 15 85 24]] [[78 15 34 79] [95 66 42 17]] [[69 9 8 4] [41 78 93 37]]] &lt;class 'numpy.ndarray'&gt; 랜덤 배열 실숫값 추출 1234from numpy import randomx = random.rand(2,5)print(x)print(type(x)) [[0.4704299 0.23477923 0.28244335 0.71846304 0.75368955] [0.41210023 0.91448093 0.15992748 0.33859588 0.52543335]] &lt;class 'numpy.ndarray'&gt; Numpy 사칙 연산123import numpy as nparray_01 = np.array([1,2,3])array_02 = np.array([10,20,30]) 123456789101112131415161718192021# 덧셈newArr = np.add(array_01,array_02)print(newArr)# 뺄셈newArr = np.subtract(array_01,array_02)print(newArr)# 곱셈newArr = np.multiply(array_01,array_02)print(newArr)# 나눗셈newArr = np.divide(array_01,array_02)print(newArr)# 거듭제곱array_01 = np.array([1,2,3])array_02 = np.array([2,4,2])newArr = np.power(array_01,array_02)print(newArr) [11 22 33] [ -9 -18 -27] [10 40 90] [0.1 0.1 0.1] [ 1 16 9] 소숫점 정렬 소숫점을 정렬하는 다양한 방법 1234567# 소숫점 제거import numpy as nptemp_arr =np.trunc([-1.91,1.9])print(temp_arr) temp_arr =np.fix([-1.23,1.9])print(temp_arr) [-1. 1.] [-1. 1.] 123# 임의의 소숫점 자리에서 반올림temp_arr = np.around([-1.2345667,1.23232323],5)print(temp_arr) [-1.23457 1.23232] 123#소숫점 모두 내림temp_arr = np.floor([-1.2345667,1.23232323])print(temp_arr) [-2. 1.] 123#소숫점 모두 올림temp_arr = np.ceil([-1.2345667,1.23232323])print(temp_arr) [-1. 2.] 조건식 pandas 가공 numpy 조건식 하나(단일)의 조건식(-&gt;np.where) 다중 조건식(이번 시간에 가장 중요함!!!!) 12temp_arr = np.arange(10)temp_arr array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]) 12# 5보다 작으면 원 값 유지# 5보다 크면 곱하기 10을 유지해줌 12#np.where(조건식, 참일 때, 거짓일 때)np.where(temp_arr&lt; 5, temp_arr, temp_arr*10) array([ 0, 1, 2, 3, 4, 50, 60, 70, 80, 90]) 1234567temp_arr = np.arange(10)# temp_arrcond_list = [temp_arr &gt; 5, temp_arr &lt; 2]choice_list = [temp_arr * 2, temp_arr + 100]#np.select(조건식 리스트, 결과값 리스트, default = )np.select(cond_list, choice_list, default = temp_arr) array([100, 101, 2, 3, 4, 5, 12, 14, 16, 18]) Reshape 배열의 차원 또는 크기를 바꿈 곱셈이 중요 1234import numpy as nptemp_array = np.ones((3,4))print(temp_array.shape)print(temp_array) (3, 4) [[1. 1. 1. 1.] [1. 1. 1. 1.] [1. 1. 1. 1.]] 값을 임으로 바꿔서 순서를 확인바란다. 12temp_array= random.randint(100, size = (3,1,4))temp_array array([[[34, 28, 61, 38]], [[49, 30, 47, 52]], [[78, 13, 72, 62]]]) 3개의 4열을 만들었다. 그렇기에 아래도 12의 공배수로 적어줘야 된다. 123after_reshape = temp_array.reshape(2,2,3)print(after_reshape.shape)print(after_reshape) (2, 2, 3) [[[34 28 61] [38 49 30]] [[47 52 78] [13 72 62]]] (2,2,?)를 했을 때 나머지 내용을 모르겠으면 (-1)을 적어주면 편하다. 123after_reshape = temp_array.reshape(2,2,-1)print(after_reshape.shape)print(after_reshape) (2, 2, 3) [[[34 28 61] [38 49 30]] [[47 52 78] [13 72 62]]] 브로드 캐스팅 서로 다른 크기의 배열을 계산할 때의 기본적인 규칙을 의미 판다스12import pandas as pdprint(pd.__version__) 1.3.5 1234567temp_dict = { 'col1' : [1,2], 'col2' : [3,4] }df = pd.DataFrame(temp_dict)print(df)print(type(df)) col1 col2 0 1 3 1 2 4 &lt;class 'pandas.core.frame.DataFrame'&gt; 구글 드라이브 연동12from google.colab import drivedrive.mount('/content/drive') Mounted at /content/drive 1234567DATA_PATH= '/content/drive/MyDrive/Colab Notebooks/Human_ai/Basic/Chapter 3. pandas/data/'print(DATA_PATH + 'Lemonade2016.csv')lemonade = pd.read_csv(DATA_PATH + 'Lemonade2016.csv')#covid_df = pd.read_csv(DATA_PATH + 'owid-covid-data.csv')lemonade.info() /content/drive/MyDrive/Colab Notebooks/Human_ai/Basic/Chapter 3. pandas/data/Lemonade2016.csv &lt;class 'pandas.core.frame.DataFrame'&gt; RangeIndex: 32 entries, 0 to 31 Data columns (total 7 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 Date 31 non-null object 1 Location 32 non-null object 2 Lemon 32 non-null int64 3 Orange 32 non-null int64 4 Temperature 32 non-null int64 5 Leaflets 31 non-null float64 6 Price 32 non-null float64 dtypes: float64(2), int64(3), object(2) memory usage: 1.9+ KB 1","link":"/2022/06/28/day0628/"},{"title":"","text":"title: ‘머신러닝’ date: ‘2022-06-29 16:00’ 파이썬 주요 라이브러리 Machine learning 정형데이터 사이킷런(Scikit-Learn) Deep Learming 비정형데이터 TensorFlow(구글)vs Pytorch(페이스북) 혼공머신은 텐서플로우 실제 상용 서비스(텐서플로우) Vs R&amp;D(파이터치-&gt;넘파이와 비슷한 용어) 생선분류(p.45) 도미, 곤들매기, 농어 등등 목표: 이생선들을 프록그램을 통해 분류한다 30cm 이상이면 도미라고 알려줘라. 12345fish_length = int(input('길이의 숫자로 넣어주세요:'))if fish_length &gt;=30: print(&quot;도미&quot;)else: print(&quot;몰라&quot;) 길이의 숫자로 넣어주세요:20 몰라 도미데이터(p.47) 데이터 수집1234# 도미 길이bream_length = [25.4, 26.3, 26.5, 29.0, 29.0, 29.7, 29.7, 30.0, 30.0, 30.7, 31.0, 31.0, 31.5, 32.0, 32.0, 32.0, 33.0, 33.0, 33.5, 33.5, 34.0, 34.0, 34.5, 35.0, 35.0, 35.0, 35.0, 36.0, 36.0, 37.0, 38.5, 38.5, 39.5, 41.0, 41.0]# 도미 무게bream_weight = [242.0, 290.0, 340.0, 363.0, 430.0, 450.0, 500.0, 390.0, 450.0, 500.0, 475.0, 500.0, 500.0, 340.0, 600.0, 600.0, 700.0, 700.0, 610.0, 650.0, 575.0, 685.0, 620.0, 680.0, 700.0, 725.0, 720.0, 714.0, 850.0, 1000.0, 920.0, 955.0, 925.0, 975.0, 950.0] 데이터 가공 여기서는 생략 데이터 시각화 여러 인사이트를 확인하기 위해 시각화, 통계 수치 계산 탐색적 자료 분석(EDA : Exploratory Data Analysis) 123456import matplotlib.pyplot as pltplt.scatter(bream_length, bream_weight)plt.xlabel('length')plt.ylabel('weight')plt.show() 파이썬 시각화는 객체 지향으로 한다. 왜냐하면 좀 더 이쁘고 아름답게 다듬기 위해서 캐글 시각화 참고할 때 아래와 같이 하는 사람이 많음 1234567import matplotlib.pyplot as pltfig, ax = plt.subplots()ax.scatter(bream_length, bream_weight)ax.set_xlabel('length')ax.set_ylabel('weight')plt.show() 빙어 데이터 준비하기 12smelt_length = [9.8, 10.5, 10.6, 11.0, 11.2, 11.3, 11.8, 11.8, 12.0, 12.2, 12.4, 13.0, 14.3, 15.0]smelt_weight = [6.7, 7.5, 7.0, 9.7, 9.8, 8.7, 10.0, 9.9, 9.8, 12.2, 13.4, 12.2, 19.7, 19.9] 123456fig, ax = plt.subplots()ax.scatter(bream_length,bream_weight)ax.scatter(smelt_length,smelt_weight)ax.set_xlabel('length')ax.set_ylabel('weight')plt.show() 두개의 리스트 합치기 12length = bream_length + smelt_lengthweight = bream_weight + smelt_weight 2차원 리스트로 만든다. 12fish_data = [[l,w] for l,w in zip(length, weight)]fish_data [0:5] #5개만 추출 [[25.4, 242.0], [26.3, 290.0], [26.5, 340.0], [29.0, 363.0], [29.0, 430.0]] 생선의 길이와 무게를 보고 빙어와 도미를 구분하고 싶다. 라벨링을 해준다=지도해준다=지도학습 12fish_target = [1]* 35 + [0]*14print(fish_taget) [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] 123456from sklearn.neighbors import KNeighborsClassifier# 클래스 인스턴스화kn = KNeighborsClassifier()# 모형학습kn.fit(fish_data, fish_target) KNeighborsClassifier() 12# 예측 정확도kn.score(fish_data, fish_target) 1.0 실제 예측을 해보자 새로운 물고기 도착 길이 :30 몸무게:600 123456789ac_length = int(input(&quot;물고기 길이를 입력하세요..&quot;))ac_weight= int(input(&quot;물고기 무게를 입력하세요..&quot;))preds = int(kn.predict([[ac_length, ac_weight]]))print(preds)if preds == 1: print(&quot;도미&quot;)else: print(&quot;빙어&quot;) 물고기 길이를 입력하세요..35 물고기 무게를 입력하세요..100 0 빙어","link":"/2022/06/29/day29_ml/"},{"title":"판다스","text":"라이브러리 불러오기1234import pandas as pdimport numpy as npprint(&quot;pandas version:&quot;,pd.__version__)print(&quot;numpy verson:&quot;,np.__version__) pandas version: 1.3.5 numpy verson: 1.21.6 데이터 불러오기 구글 드라이브에 있는 데이터를 불러올 때 데이터는 존재해야 함 12from google.colab import drivedrive.mount('/content/drive') Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(&quot;/content/drive&quot;, force_remount=True). 레모네이드 데이터를 불렀는데 결측치가 있음. 123456DATA_PATH= '/content/drive/MyDrive/Colab Notebooks/Human_ai/Basic/Chapter 3. pandas/data/Lemonade2016.csv'print(DATA_PATH)lemonade = pd.read_csv(DATA_PATH)lemonade.info() /content/drive/MyDrive/Colab Notebooks/Human_ai/Basic/Chapter 3. pandas/data/Lemonade2016.csv &lt;class 'pandas.core.frame.DataFrame'&gt; RangeIndex: 32 entries, 0 to 31 Data columns (total 7 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 Date 31 non-null object 1 Location 32 non-null object 2 Lemon 32 non-null int64 3 Orange 32 non-null int64 4 Temperature 32 non-null int64 5 Leaflets 31 non-null float64 6 Price 32 non-null float64 dtypes: float64(2), int64(3), object(2) memory usage: 1.9+ KB 데이터 맛보기 헤드 테일은 숫자를 안넣어주면 5개만 보임 1print(lemonade.head()) Date Location Lemon Orange Temperature Leaflets Price 0 7/1/2016 Park 97 67 70 90.0 0.25 1 7/2/2016 Park 98 67 72 90.0 0.25 2 7/3/2016 Park 110 77 71 104.0 0.25 3 7/4/2016 Beach 134 99 76 98.0 0.25 4 7/5/2016 Beach 159 118 78 135.0 0.25 1print(lemonade. tail()) Date Location Lemon Orange Temperature Leaflets Price 27 7/27/2016 Park 104 68 80 99.0 0.35 28 7/28/2016 Park 96 63 82 90.0 0.35 29 7/29/2016 Park 100 66 81 95.0 0.35 30 7/30/2016 Beach 88 57 82 81.0 0.35 31 7/31/2016 Beach 76 47 82 68.0 0.35 기술통계량 보는 함수 describe() 1print(lemonade.describe()) Lemon Orange Temperature Leaflets Price count 32.000000 32.000000 32.000000 31.000000 32.000000 mean 116.156250 80.000000 78.968750 108.548387 0.354688 std 25.823357 21.863211 4.067847 20.117718 0.113137 min 71.000000 42.000000 70.000000 68.000000 0.250000 25% 98.000000 66.750000 77.000000 90.000000 0.250000 50% 113.500000 76.500000 80.500000 108.000000 0.350000 75% 131.750000 95.000000 82.000000 124.000000 0.500000 max 176.000000 129.000000 84.000000 158.000000 0.500000 레몬보다 오랜지가 표준편차가 작으니 판매차가 거의 없다.(leaflets전단지) 범주형 데이터 빈도수 구하기 데이터[‘컬럼’].갯수함수 -&gt; 시리즈 함수였다!! 1lemonade['Location'].value_counts() Beach 17 Park 15 Name: Location, dtype: int64 1print(type(lemonade['Location'])) &lt;class 'pandas.core.series.Series'&gt; 행과 열 다루기 sold(판매량) 컬럼(=피처=feature)을 추가 12lemonade['Sold'] = 0print(lemonade.head(3)) Date Location Lemon Orange Temperature Leaflets Price Sold 0 7/1/2016 Park 97 67 70 90.0 0.25 0 1 7/2/2016 Park 98 67 72 90.0 0.25 0 2 7/3/2016 Park 110 77 71 104.0 0.25 0 12lemonade['Sold'] = lemonade['Lemon'] + lemonade['Orange']print(lemonade.head(3)) Date Location Lemon Orange Temperature Leaflets Price Sold 0 7/1/2016 Park 97 67 70 90.0 0.25 164 1 7/2/2016 Park 98 67 72 90.0 0.25 165 2 7/3/2016 Park 110 77 71 104.0 0.25 187 Revenue = 단가 x 판매량 12lemonade['Revenue'] = 0print(lemonade[['Sold','Price']].head(3)) Sold Price 0 164 0.25 1 165 0.25 2 187 0.25 12lemonade['Revenue'] = lemonade['Price'] * lemonade['Sold']print(lemonade[['Revenue', 'Price', 'Sold']].head()) Revenue Price Sold 0 41.00 0.25 164 1 41.25 0.25 165 2 46.75 0.25 187 3 58.25 0.25 233 4 69.25 0.25 277 drop 함수 행은 axis=1 열은 axis=0 을 넣어줘야 한다 1234#컬럼 제거col_drop = lemonade.drop('Sold', axis=1)print(col_drop.head()) Date Location Lemon Orange Temperature Leaflets Price Revenue 0 7/1/2016 Park 97 67 70 90.0 0.25 41.00 1 7/2/2016 Park 98 67 72 90.0 0.25 41.25 2 7/3/2016 Park 110 77 71 104.0 0.25 46.75 3 7/4/2016 Beach 134 99 76 98.0 0.25 58.25 4 7/5/2016 Beach 159 118 78 135.0 0.25 69.25 123#행 제거row_drop = lemonade.drop([0,2], axis =0)print(row_drop.head()) Date Location Lemon Orange Temperature Leaflets Price Sold \\ 1 7/2/2016 Park 98 67 72 90.0 0.25 165 3 7/4/2016 Beach 134 99 76 98.0 0.25 233 4 7/5/2016 Beach 159 118 78 135.0 0.25 277 5 7/6/2016 Beach 103 69 82 90.0 0.25 172 6 7/6/2016 Beach 103 69 82 90.0 0.25 172 Revenue 1 41.25 3 58.25 4 69.25 5 43.00 6 43.00 데이터 인덱싱1print(lemonade[4:7]) Date Location Lemon Orange Temperature Leaflets Price Sold \\ 4 7/5/2016 Beach 159 118 78 135.0 0.25 277 5 7/6/2016 Beach 103 69 82 90.0 0.25 172 6 7/6/2016 Beach 103 69 82 90.0 0.25 172 Revenue 4 69.25 5 43.00 6 43.00 행의 인덱스자체가 제거됨 특정 값만 추출하는데 함수filter를 사용할 수 있지만 조건식을 이용하는 것이 더 편하다. 참 거짓으로 구분한 뒤 참만을 뽑도록 한다. lemonade[조건식] 1234#데이터 [데이터 컬럼 == 특정값]lemonade_L=lemonade[lemonade['Location'] == 'Beach']print(lemonade_L)#lemonade['Location'] == 'Beach'-&gt;참 거짓으로 구분한 뒤 참인 그 값(Beach)을 불러내면 lemonade[lemonade['Location'] == 'Beach'] 그값(Beach)만 나옴 Date Location Lemon Orange Temperature Leaflets Price Sold \\ 3 7/4/2016 Beach 134 99 76 98.0 0.25 233 4 7/5/2016 Beach 159 118 78 135.0 0.25 277 5 7/6/2016 Beach 103 69 82 90.0 0.25 172 6 7/6/2016 Beach 103 69 82 90.0 0.25 172 7 7/7/2016 Beach 143 101 81 135.0 0.25 244 8 NaN Beach 123 86 82 113.0 0.25 209 9 7/9/2016 Beach 134 95 80 126.0 0.25 229 10 7/10/2016 Beach 140 98 82 131.0 0.25 238 11 7/11/2016 Beach 162 120 83 135.0 0.25 282 12 7/12/2016 Beach 130 95 84 99.0 0.25 225 13 7/13/2016 Beach 109 75 77 99.0 0.25 184 14 7/14/2016 Beach 122 85 78 113.0 0.25 207 15 7/15/2016 Beach 98 62 75 108.0 0.50 160 16 7/16/2016 Beach 81 50 74 90.0 0.50 131 17 7/17/2016 Beach 115 76 77 126.0 0.50 191 30 7/30/2016 Beach 88 57 82 81.0 0.35 145 31 7/31/2016 Beach 76 47 82 68.0 0.35 123 Revenue 3 58.25 4 69.25 5 43.00 6 43.00 7 61.00 8 52.25 9 57.25 10 59.50 11 70.50 12 56.25 13 46.00 14 51.75 15 80.00 16 65.50 17 95.50 30 50.75 31 43.05 1print(lemonade[lemonade['Temperature'] &gt;= 80])#온도가 80이 넘는 것을 뽑아내라 Date Location Lemon Orange Temperature Leaflets Price Sold \\ 5 7/6/2016 Beach 103 69 82 90.0 0.25 172 6 7/6/2016 Beach 103 69 82 90.0 0.25 172 7 7/7/2016 Beach 143 101 81 135.0 0.25 244 8 NaN Beach 123 86 82 113.0 0.25 209 9 7/9/2016 Beach 134 95 80 126.0 0.25 229 10 7/10/2016 Beach 140 98 82 131.0 0.25 238 11 7/11/2016 Beach 162 120 83 135.0 0.25 282 12 7/12/2016 Beach 130 95 84 99.0 0.25 225 18 7/18/2016 Park 131 92 81 122.0 0.50 223 22 7/22/2016 Park 112 75 80 108.0 0.50 187 23 7/23/2016 Park 120 82 81 117.0 0.50 202 24 7/24/2016 Park 121 82 82 117.0 0.50 203 25 7/25/2016 Park 156 113 84 135.0 0.50 269 26 7/26/2016 Park 176 129 83 158.0 0.35 305 27 7/27/2016 Park 104 68 80 99.0 0.35 172 28 7/28/2016 Park 96 63 82 90.0 0.35 159 29 7/29/2016 Park 100 66 81 95.0 0.35 166 30 7/30/2016 Beach 88 57 82 81.0 0.35 145 31 7/31/2016 Beach 76 47 82 68.0 0.35 123 Revenue 5 43.00 6 43.00 7 61.00 8 52.25 9 57.25 10 59.50 11 70.50 12 56.25 18 111.50 22 93.50 23 101.00 24 101.50 25 134.50 26 106.75 27 60.20 28 55.65 29 58.10 30 50.75 31 43.05 1print(lemonade[(lemonade['Temperature'] &gt;= 80) &amp; (lemonade['Orange'] &gt;= 100)]) #lemonade[(조건식)&amp;(조건식2)]온도가 80넘고 오랜지가100이상인 것 Date Location Lemon Orange Temperature Leaflets Price Sold \\ 7 7/7/2016 Beach 143 101 81 135.0 0.25 244 11 7/11/2016 Beach 162 120 83 135.0 0.25 282 25 7/25/2016 Park 156 113 84 135.0 0.50 269 26 7/26/2016 Park 176 129 83 158.0 0.35 305 Revenue 7 61.00 11 70.50 25 134.50 26 106.75 1print(lemonade[(lemonade['Temperature'] &gt;= 80) &amp; (lemonade['Orange'] &gt;= 100) &amp; (lemonade['Location'] == &quot;Park&quot;)]) Date Location Lemon Orange Temperature Leaflets Price Sold \\ 25 7/25/2016 Park 156 113 84 135.0 0.50 269 26 7/26/2016 Park 176 129 83 158.0 0.35 305 Revenue 25 134.50 26 106.75 iloc와 loc의 차이 iloc는 인덱스의 “숫자”로 loc는 라벨의 “이름”으로 내용을 선택한다. loc가 사용하기 편하다. 1print(lemonade.loc[lemonade['Temperature'] &gt;= 80,['Date','Sold']]) #온도가 80이 넘는 것 중 라벨명이 날자와 판매량을 찾아라(중요) Date Sold 5 7/6/2016 172 6 7/6/2016 172 7 7/7/2016 244 8 NaN 209 9 7/9/2016 229 10 7/10/2016 238 11 7/11/2016 282 12 7/12/2016 225 18 7/18/2016 223 22 7/22/2016 187 23 7/23/2016 202 24 7/24/2016 203 25 7/25/2016 269 26 7/26/2016 305 27 7/27/2016 172 28 7/28/2016 159 29 7/29/2016 166 30 7/30/2016 145 31 7/31/2016 123 문법상의 차이 확인 숫자(ilot) 라벨(lot)=글자 숫자 문자 동시 1print(lemonade.iloc[0:3,0:2]) # [행-인덱스번호부터 시작, 열] Date Location 0 7/1/2016 Park 1 7/2/2016 Park 2 7/3/2016 Park 1print(lemonade.loc[0:2,['Date','Location']])# 똑같은 결과이지만 라벨은 행의 시작 위치 컬럼부터 시작 Date Location 0 7/1/2016 Park 1 7/2/2016 Park 2 7/3/2016 Park 데이터 정렬 sort_values() 12#lemonade.head()print(lemonade.sort_values(by=['Revenue']).head(5)) Date Location Lemon Orange Temperature Leaflets Price Sold \\ 0 7/1/2016 Park 97 67 70 90.0 0.25 164 1 7/2/2016 Park 98 67 72 90.0 0.25 165 6 7/6/2016 Beach 103 69 82 90.0 0.25 172 5 7/6/2016 Beach 103 69 82 90.0 0.25 172 31 7/31/2016 Beach 76 47 82 68.0 0.35 123 Revenue 0 41.00 1 41.25 6 43.00 5 43.00 31 43.05 1print(lemonade[['Date','Temperature','Revenue']].sort_values(by=['Temperature','Revenue']).head(5)) Date Temperature Revenue 0 7/1/2016 70 41.00 20 7/20/2016 70 56.50 2 7/3/2016 71 46.75 1 7/2/2016 72 41.25 16 7/16/2016 74 65.50 1print(lemonade[['Date','Temperature','Revenue']].sort_values(by=['Temperature','Revenue'],ascending=[True,False]).head(5)) Date Temperature Revenue 20 7/20/2016 70 56.50 0 7/1/2016 70 41.00 2 7/3/2016 71 46.75 1 7/2/2016 72 41.25 16 7/16/2016 74 65.50 1print(lemonade[['Date','Temperature','Revenue']].sort_values(by=['Temperature','Revenue'],ascending=[False,True]).head(5)) Date Temperature Revenue 12 7/12/2016 84 56.25 25 7/25/2016 84 134.50 11 7/11/2016 83 70.50 26 7/26/2016 83 106.75 5 7/6/2016 82 43.00 Group by cf)인덱스가 숫자에서 라벨로 바뀜 1234df = lemonade.groupby(by='Location').count()print(df)print(&quot;&quot;)print(type(df)) Date Lemon Orange Temperature Leaflets Price Sold Revenue Location Beach 16 17 17 17 17 17 17 17 Park 15 15 15 15 14 15 15 15 &lt;class 'pandas.core.frame.DataFrame'&gt; 12df[['Date','Lemon']]print(df[['Date','Lemon']]) Date Lemon Location Beach 16 17 Park 15 15 1print(df.iloc[0:1,0:2]) Date Lemon Location Beach 16 17 1print(df.loc['Park', ['Date', 'Lemon']]) Date 15 Lemon 15 Name: Park, dtype: int64 간단한 피벗 테이블 만들기 1print(lemonade.groupby('Location')['Revenue'].agg([max,min,sum,np.mean])) max min sum mean Location Beach 95.5 43.0 1002.8 58.988235 Park 134.5 41.0 1178.2 78.546667 지역별로 매출액의 최대값, 최소값, 합계, 평균을 구한다. 1print(lemonade.groupby('Location')['Revenue','Sold','Temperature'].agg([max,min,sum,np.mean])) Revenue Sold \\ max min sum mean max min sum mean Location Beach 95.5 43.0 1002.8 58.988235 282 123 3422 201.294118 Park 134.5 41.0 1178.2 78.546667 305 113 2855 190.333333 Temperature max min sum mean Location Beach 84 74 1355 79.705882 Park 84 70 1172 78.133333 /usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:1: FutureWarning: Indexing with multiple keys (implicitly converted to a tuple of keys) will be deprecated, use a list instead. &quot;&quot;&quot;Entry point for launching an IPython kernel. 1print(lemonade.groupby(['Location', 'Price'])['Orange'].agg([max, min, sum, np.mean])) max min sum mean Location Price Beach 0.25 120 69 1110 92.500000 0.35 57 47 104 52.000000 0.50 76 50 188 62.666667 Park 0.25 77 67 211 70.333333 0.35 129 63 326 81.500000 0.50 113 42 621 77.625000 &lt;–지역에 따라 오렌지의 단가별 최대판매량, 최소판매량, 총합, 평균","link":"/2022/06/29/day0629/"},{"title":"","text":"title: ‘머신러닝3 회귀알고리즘’ date: ‘2022-06-30 14:00’ K- 최근접 이웃 회귀 지도학습 알고리즘은 크게 분류와 회귀 지도 학습 : 종속변수 존재 분류 : 도미와 빙어 분류 문제 해결 회귀 : 통계 회귀분석 y = ax + b 데이터 불러오기12import numpy as npprint(np.__version__) 1.21.6 1234567891011121314151617perch_length = np.array( [8.4, 13.7, 15.0, 16.2, 17.4, 18.0, 18.7, 19.0, 19.6, 20.0, 21.0, 21.0, 21.0, 21.3, 22.0, 22.0, 22.0, 22.0, 22.0, 22.5, 22.5, 22.7, 23.0, 23.5, 24.0, 24.0, 24.6, 25.0, 25.6, 26.5, 27.3, 27.5, 27.5, 27.5, 28.0, 28.7, 30.0, 32.8, 34.5, 35.0, 36.5, 36.0, 37.0, 37.0, 39.0, 39.0, 39.0, 40.0, 40.0, 40.0, 40.0, 42.0, 43.0, 43.0, 43.5, 44.0] )perch_weight = np.array( [5.9, 32.0, 40.0, 51.5, 70.0, 100.0, 78.0, 80.0, 85.0, 85.0, 110.0, 115.0, 125.0, 130.0, 120.0, 120.0, 130.0, 135.0, 110.0, 130.0, 150.0, 145.0, 150.0, 170.0, 225.0, 145.0, 188.0, 180.0, 197.0, 218.0, 300.0, 260.0, 265.0, 250.0, 250.0, 300.0, 320.0, 514.0, 556.0, 840.0, 685.0, 700.0, 700.0, 690.0, 900.0, 650.0, 820.0, 850.0, 900.0, 1015.0, 820.0, 1100.0, 1000.0, 1100.0, 1000.0, 1000.0] ) 1234567import matplotlib.pyplot as pltfig, ax= plt.subplots() # 객체지향의 시작ax.scatter(perch_length,perch_weight)ax.set_xlabel('length')ax.set_ylabel('weight')plt.show &lt;function matplotlib.pyplot.show&gt; 1234567from sklearn.model_selection import train_test_splittrain_input, test_input, train_target, test_target = train_test_split( perch_length, perch_weight, random_state = 42)train_input.shape, test_input.shape, train_target.shape, test_target.shapeprint(train_input.ndim) 1 1차원 배열-&gt; 2차원 배열 1234train_input = train_input. reshape(-1,1)test_input = test_input.reshape(-1,1)print(train_input.shape, test_input.shape)print(train_input.ndim) (42, 1) (14, 1) 2 결정계수 Adjusted -R Squared 정확한 지표(0~1) 1에 가까울수록 예측 모형이 예측을 잘한다. 123456789from sklearn.neighbors import KNeighborsRegressorknr = KNeighborsRegressor()#모형학습knr.fit(train_input,train_target)#테스트 세트의 점수를 확인한다print(knr.score(test_input,test_target)) 0.992809406101064 12345678from sklearn.metrics import mean_absolute_error#예측 데이터test_prediction = knr.predict(test_input)#테스트 세트에 대한 평균 절댓값 오차를 계산mae = mean_absolute_error(test_target,test_prediction)print(mae) 19.157142857142862 예측이 평균적으로 19g정도 다르다. 확실한 것은 오차가 존재하는데 19g이 의미하는 것은 무엇인가? 오차를 줄일 필요가 있음(더 많은 데이터를 모으거나 알고리즘을 바꿔야 함) 개선을 지속적으로 하여 0g이 될때까지 1print(knr.score(train_input, train_target)) 0.9698823289099254 과대적합 vs 과소적합 매우 힘듬. 도망가고 싶음(모형 설계가 잘못됨) 과대적합 : 훈련세트는 점수 좋으나 테스트 점수가 매우 안좋음 과소적합 : 테스트세트의 점수가 매우 좋음 결론 : 제대로 모형이 훈련이 안된 것이기에 모형 서비스에 탑재 불가. 12print(&quot;훈련평가:&quot;,knr.score(train_input,train_target))print(&quot;테스트평가:&quot;,knr.score(test_input,test_target))# 테스트세트 점수가 좋기에 과소적합 훈련평가: 0.9698823289099254 테스트평가: 0.992809406101064 모형개선 1234567# 이웃의 갯수를 3으로 재 지정knr.n_neighbors = 3# 모형 다시 훈련knr.fit(train_input, train_target)print(&quot;훈련 평가:&quot;,knr.score(train_input,train_target))print(&quot;테스트 평가:&quot;,knr.score(test_input, test_target)) 훈련 평가: 0.9804899950518966 테스트 평가: 0.9746459963987609","link":"/2022/06/30/day0630_ch3/"},{"title":"","text":"title: ‘머신러닝2 데이터다루기’ date: ‘2022-06-30 09:00’ 인공지능인공지능&gt;머신러닝&gt;딥러닝 딥러닝 알고리즘→인공신경망 알고리즘 이미지,자연어(=음성인식) 판별하는 성능이 중요 머신러닝 알고리즘→선형회귀, 결정트리 결과에 대한 해석 요구 통계적 분석이 중요 정형데이터(=엑셀 데이터, 테이블) 분석의 흐름 1.데이터 수집 2.데이터 가공 3.데이터 시각화 4.데이터(예측)모델링 예측평가지표 cf)R:,데이터(통계)모델링 변수(=컬럼=피쳐)간의 관계 가설 검정이 중요 공통점 : 결과를 해석 5.보고서를 작성 모형학습 fish_data-&gt; 독립변수, fish_target-&gt;종속변수 kn.fit(fish_data, fish_target) 새로운 모델 제안의 위험성 어제 머신 러닝 공부에 이어서 Default(내정값) : 정확도 1(100%) 하이퍼 파라미터 세팅 n_neighbprs = 49 로 했을 시 정확도 0.7(70%) 튜닝을 하면 이상해질 수 있다. 따라서 완벽하게 파악하지 못한 내용을 변동시키지 말것. 머신러닝 알고리즘의 흐름 선형모델 : 선형회귀,로지스틱 회귀,서포트벡터 머신 의사결정트리 모델 : 1975년 의사결정트리모델, KNN 랜덤포레스트 부스팅계열 : LightGBM(2017), XGBoost(2016) 추천: LightGBM 훈련 세트와 테스트 세트(P68)12345678fish_length = [25.4, 26.3, 26.5, 29.0, 29.0, 29.7, 29.7, 30.0, 30.0, 30.7, 31.0, 31.0, 31.5, 32.0, 32.0, 32.0, 33.0, 33.0, 33.5, 33.5, 34.0, 34.0, 34.5, 35.0, 35.0, 35.0, 35.0, 36.0, 36.0, 37.0, 38.5, 38.5, 39.5, 41.0, 41.0, 9.8, 10.5, 10.6, 11.0, 11.2, 11.3, 11.8, 11.8, 12.0, 12.2, 12.4, 13.0, 14.3, 15.0]fish_weight = [242.0, 290.0, 340.0, 363.0, 430.0, 450.0, 500.0, 390.0, 450.0, 500.0, 475.0, 500.0, 500.0, 340.0, 600.0, 600.0, 700.0, 700.0, 610.0, 650.0, 575.0, 685.0, 620.0, 680.0, 700.0, 725.0, 720.0, 714.0, 850.0, 1000.0, 920.0, 955.0, 925.0, 975.0, 950.0, 6.7, 7.5, 7.0, 9.7, 9.8, 8.7, 10.0, 9.9, 9.8, 12.2, 13.4, 12.2, 19.7, 19.9] 2차원 리스트를 만들고 라벨링을 한다. 12345fish_data = [[l, w] for l, w in zip(fish_length, fish_weight)]fish_target = [1] * 35 + [0] * 14print(fish_target[0:40:5])#1-40까지의 데이터에서 5간격으로 표시print(fish_data[0:40:5])print(fish_target) [1, 1, 1, 1, 1, 1, 1, 0] [[25.4, 242.0], [29.7, 450.0], [31.0, 475.0], [32.0, 600.0], [34.0, 575.0], [35.0, 725.0], [38.5, 920.0], [9.8, 6.7]] [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] 전체 데이터에서 일부분 샘플을 추출했다. 도미35마리 빙어 14마리 처음 35개를 훈련/ 나머지 14개를 테스트 해본다. 123456789101112131415from sklearn.neighbors import KNeighborsClassifier# 클래스 인스턴스화kn = KNeighborsClassifier()# 훈련세트로 0:34를 인덱스로 활용train_input = fish_data[:35]train_target =fish_target[:35]# 테스트 세트로 35:마지막까지를 인덱스로 활용test_input = fish_data[35:]test_target =fish_target[35:]# 모형학습kn = kn.fit(train_input,train_target)print(kn.score(test_input, test_target)) 0.0 -&gt; 훈련된 데이터는 도미인데 테스트한 데이터는 빙어이기에 이상한 결과가 나옴 샘플링 편향 훈련세트와 테스트 세트가 골고루 섞이지 않음 샘플링 작업 넘파이를 사용하여 골고루 섞어 준다. 123456import numpy as npinput_arr = np.array(fish_data)target_arr = np.array(fish_target)print(input_arr[0:49:7])print(input_arr.shape, target_arr.shape)#입력한 것의 (샘플수,특성수) 타겟은 특성수가 없음 [[ 25.4 242. ] [ 30. 390. ] [ 32. 600. ] [ 34. 685. ] [ 36. 850. ] [ 9.8 6.7] [ 11.8 9.9]] (49, 2) (49,) 123print(target_arr.shape)print(target_arr.ndim)#차원을 확인-&gt;1차원print(target_arr) (49,) 1 [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0] 12345#random으로 무작위 배열을 만드는 설정np.random.seed(42)index = np.arange(49)np.random.shuffle(index)print(index) [13 45 47 44 17 27 26 25 31 19 12 4 34 8 3 6 40 41 46 15 9 16 24 33 30 0 43 32 5 29 11 36 1 21 2 37 35 23 39 10 22 18 48 20 7 42 14 28 38] 12345train_input = input_arr[index[:35]]train_target= target_arr[index[:35]]test_input = input_arr[index[35:]]test_target= target_arr[index[35:]] 12print(train_input[:1])print(train_input[:,0])#전체길이 [[ 32. 340.]] [32. 12.4 14.3 12.2 33. 36. 35. 35. 38.5 33.5 31.5 29. 41. 30. 29. 29.7 11.3 11.8 13. 32. 30.7 33. 35. 41. 38.5 25.4 12. 39.5 29.7 37. 31. 10.5 26.3 34. 26.5] 시각화12345678import matplotlib.pyplot as plt fig, ax = plt.subplots()ax.scatter(train_input[:, 0], train_input[:, 1])ax.scatter(test_input[:, 0], test_input[:, 1])ax.set_xlabel(&quot;length&quot;)ax.set_ylabel(&quot;weight&quot;)fig.show() 두번째 머신러닝 프로그램12kn.fit(train_input, train_target)kn.score(test_input, test_target) 1.0 1kn.predict(test_input) # 예측데이터 array([0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0]) 1test_target #실제 데이터 array([0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0]) 데이터 전처리 머신러닝 시 데이터 전처리 결측치 처리, 이상치 처리 12345678fish_length = [25.4, 26.3, 26.5, 29.0, 29.0, 29.7, 29.7, 30.0, 30.0, 30.7, 31.0, 31.0, 31.5, 32.0, 32.0, 32.0, 33.0, 33.0, 33.5, 33.5, 34.0, 34.0, 34.5, 35.0, 35.0, 35.0, 35.0, 36.0, 36.0, 37.0, 38.5, 38.5, 39.5, 41.0, 41.0, 9.8, 10.5, 10.6, 11.0, 11.2, 11.3, 11.8, 11.8, 12.0, 12.2, 12.4, 13.0, 14.3, 15.0]fish_weight = [242.0, 290.0, 340.0, 363.0, 430.0, 450.0, 500.0, 390.0, 450.0, 500.0, 475.0, 500.0, 500.0, 340.0, 600.0, 600.0, 700.0, 700.0, 610.0, 650.0, 575.0, 685.0, 620.0, 680.0, 700.0, 725.0, 720.0, 714.0, 850.0, 1000.0, 920.0, 955.0, 925.0, 975.0, 950.0, 6.7, 7.5, 7.0, 9.7, 9.8, 8.7, 10.0, 9.9, 9.8, 12.2, 13.4, 12.2, 19.7, 19.9] 12## column_stack()활용np.column_stack(([1,2,3],[4,5,6])) array([[1, 4], [2, 5], [3, 6]]) 1234fish_data = np.column_stack((fish_length, fish_weight))print(fish_data[:5])fish_data.shape [[ 25.4 242. ] [ 26.3 290. ] [ 26.5 340. ] [ 29. 363. ] [ 29. 430. ]] (49, 2) 종속변수 = Y = 타깃데이터 = Target &lt;-&gt;독립변수(X) 12fish_target = np.concatenate((np.ones(35),np.zeros(14)))print(fish_target.shape) (49,) scikit-learn 훈련세트와 테스트 세트 나누기1234567from sklearn.model_selection import train_test_splittrain_input, test_input, train_target, test_target = train_test_split( #독립변수, 종속변수 fish_data, fish_target, random_state =42)train_input.shape, test_input.shape, train_target.shape, test_target.shape ((36, 2), (13, 2), (36,), (13,)) P92 도미와 빙어가 잘 섞여 있나? 1print(test_target) [1. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1.] 35(도미) :14(빙어) 2.5:1 테스트 셋(비율) 3.3:1 층화샘플링 기초 통계, 설문조사 비율이 중요 예)남성 속옷을 구매하는 비율은 남자9: 여자1이지만 조사는 남자5: 여자 5로 조사됨으로 비율이 맞지 않음 123456train_input, test_input, train_target, test_target = train_test_split( # 독립변수, 종속변수 fish_data, fish_target, stratify=fish_target, random_state = 42)train_input.shape, test_input.shape, train_target.shape, test_target.shape ((36, 2), (13, 2), (36,), (13,)) stratify = fish_target를 넣어주어 빙어가 한마리 더 늘도록 해서 테스트 세트 비율이 2.55:1 로 근접하게 됨 1print(test_target) [0. 0. 1. 0. 1. 0. 1. 1. 1. 1. 1. 1. 1.] 수상한 도미 한마리 1234from sklearn.neighbors import KNeighborsClassifierkn = KNeighborsClassifier()kn.fit(train_input, train_target)kn.score(test_input, test_target) 1.0 도미사이즈 20cm 이상 = 1 빙어사이즈 10cm 이하 = 0 인 문제가 발생 알고리즘에 문제가 있음 1print(kn.predict([[25,150]])) [0.] 123456import matplotlib.pyplot as plt fig, ax = plt.subplots()ax.scatter(train_input[:, 0], train_input[:, 1])ax.scatter(25, 150, marker = '^')plt.show() 이웃 샘플이 누구인지 확인해보니 알고리즘이 맞지 않음 빙어에 4개의 이웃 샘플이 있어 빙어로 인식 1234567distances, indexes = kn.kneighbors([[25, 150]])plt.scatter(train_input[:,0], train_input[:,1])plt.scatter(25, 150, marker='^')plt.scatter(train_input[indexes,0], train_input[indexes,1], marker='D')plt.xlabel('length')plt.ylabel('weight')plt.show() 떨어진 거리 비율을 맞추기 위해 스케일의 크기를 동일하게 함.즉 무게와 길이의 길이를 1000으로 맞춤 1234567plt.scatter(train_input[:,0], train_input[:,1])plt.scatter(25, 150, marker='^')plt.scatter(train_input[indexes,0], train_input[indexes,1], marker='D')plt.xlim((0, 1000))plt.xlabel('length')plt.ylabel('weight')plt.show() -p98 그러나 두 특성(길이와 무게)의 값이 놓인 범위가 매우 다름 두 특성의 스케일이 다름 스케일이 같도록 통계처리 필요 Feature Engineering(피처 엔지니어링) 머신 러닝 전체 데이터 전처리(결측지 처리, 이상치 처리) 데이터 분리 Feature Engineering(피처 엔지니어링) 표준점수 z 점수 1234mean = np.mean(train_input, axis =0)std = np.std(train_input,axis=0)print(mean, std) [ 27.29722222 454.09722222] [ 9.98244253 323.29893931] 표준 점수 구하기 123# 브로드 캐스팅- 서로 다른 배열을 계산할 때print(train_input.shape, mean.shape, std.shape)train_scaled = (train_input - mean)/std (36, 2) (2,) (2,) 1train_input[0:5] array([[ 29.7, 500. ], [ 12.2, 12.2], [ 33. , 700. ], [ 11.3, 8.7], [ 39.5, 925. ]]) 1train_scaled[0:5] array([[ 0.24070039, 0.14198246], [-1.51237757, -1.36683783], [ 0.5712808 , 0.76060496], [-1.60253587, -1.37766373], [ 1.22242404, 1.45655528]]) 12345plt.scatter(train_scaled[:,0], train_scaled[:,1])plt.scatter(25, 150, marker='^')plt.xlabel('length')plt.ylabel('weight')plt.show() 123456new = ([25, 150] - mean) / stdplt.scatter(train_scaled[:,0], train_scaled[:,1])plt.scatter(new[0], new[1], marker='^')plt.xlabel('length')plt.ylabel('weight')plt.show() 통계처리 전 : KNN –&gt; 예측이 틀림통계처리 후 : KNN –&gt; 예측이 정확하게 맞음– 통계처리 –&gt; Feature Engineering 모형학습 1kn.fit(train_scaled, train_target) KNeighborsClassifier() 123#kn.score(test_input, test_target)test_scaled = (test_input - mean)/ stdkn.score(test_scaled, test_target) 1.0 예측 1print(kn.predict([new])) [1.] 1234567distances, indexes = kn.kneighbors([new])plt.scatter(train_scaled[:,0], train_scaled[:,1])plt.scatter(new[0], new[1], marker='^')plt.scatter(train_scaled[indexes,0], train_scaled[indexes,1], marker='D')plt.xlabel('length')plt.ylabel('weight')plt.show()","link":"/2022/06/30/day0630_ml/"},{"title":"","text":"title: ‘머신러닝3 회귀알고리즘’date: ‘2022-07-01 09:00’ 데이터 불러오기12import numpy as npprint(np.__version__) 1.21.6 123456789101112131415161718perch_length = np.array( [8.4, 13.7, 15.0, 16.2, 17.4, 18.0, 18.7, 19.0, 19.6, 20.0, 21.0, 21.0, 21.0, 21.3, 22.0, 22.0, 22.0, 22.0, 22.0, 22.5, 22.5, 22.7, 23.0, 23.5, 24.0, 24.0, 24.6, 25.0, 25.6, 26.5, 27.3, 27.5, 27.5, 27.5, 28.0, 28.7, 30.0, 32.8, 34.5, 35.0, 36.5, 36.0, 37.0, 37.0, 39.0, 39.0, 39.0, 40.0, 40.0, 40.0, 40.0, 42.0, 43.0, 43.0, 43.5, 44.0] )perch_weight = np.array( [5.9, 32.0, 40.0, 51.5, 70.0, 100.0, 78.0, 80.0, 85.0, 85.0, 110.0, 115.0, 125.0, 130.0, 120.0, 120.0, 130.0, 135.0, 110.0, 130.0, 150.0, 145.0, 150.0, 170.0, 225.0, 145.0, 188.0, 180.0, 197.0, 218.0, 300.0, 260.0, 265.0, 250.0, 250.0, 300.0, 320.0, 514.0, 556.0, 840.0, 685.0, 700.0, 700.0, 690.0, 900.0, 650.0, 820.0, 850.0, 900.0, 1015.0, 820.0, 1100.0, 1000.0, 1100.0, 1000.0, 1000.0] )print(perch_length.shape,perch_weight.shape) (56,) (56,) 데이터 가공 1차원 데이터를 가공 train_test_split로 훈련 세트와 테스트 세트로 나눈 후 1-&gt;2차원배열로 변환 12345678from sklearn.model_selection import train_test_splittrain_input, test_input, train_target, test_target = train_test_split( # 독립변수, 종속변수 perch_length, perch_weight, random_state = 42)print(train_input.shape, test_input.shape, train_target.shape, test_target.shape) (42,) (14,) (42,) (14,) 123#1차원 -&gt;2차원:넘파이배열은 크기를 바꾸는 reshape()메서드가 있다.자동으로 바꾸는 식 (-1,1)을 이용한다train_input = train_input.reshape(-1,1)test_input = test_input.reshape(-1,1) 데이터 시각화-&gt;데이터 재가공모델링12345from sklearn.neighbors import KNeighborsRegressorknr = KNeighborsRegressor(n_neighbors=3)#모형훈련knr.fit(train_input, train_target) KNeighborsRegressor(n_neighbors=3) 모델평가모델 예측123# 농어의 50cm --&gt; 농어의 무게print(knr.predict([[50]])) [1033.33333333] 모형 평가를 위한 시각화12345678910111213141516from scipy.spatial import distanceimport matplotlib.pyplot as plt# 50cm 농어의 이웃을 3개distances, indexes = knr.kneighbors([[50]])# 훈련세트의 산점도를 그립니다.fig, ax = plt.subplots()ax.scatter(train_input, train_target)# 훈련세트 중에서 이웃 샘플만 다시 그립니다ax.scatter(train_input[indexes], train_target[indexes],marker='D')# 농어의 길이 #농어의 무게ax.scatter(50, 1033, marker='^')ax.set_xlabel('length')ax.set_ylabel('weight')plt.show() 맞는 것처럼 보이지만 길이를 100cm으로 해도 똑같은 결과(1033)가 나온다. 멀리있는 데이터를 가지고 왔다. 잘못된 알고리즘이다. 123456789101112131415# 100cm 농어의 이웃을 3개distances, indexes = knr.kneighbors([[100]])print(distances, indexes)# 훈련세트의 산점도를 그립니다.fig, ax = plt.subplots()ax.scatter(train_input, train_target)# 훈련세트 중에서 이웃 샘플만 다시 그립니다ax.scatter(train_input[indexes], train_target[indexes],marker='D')# 농어의 길이 #농어의 무게ax.scatter(100, 1033, marker='^')ax.set_xlabel('length')ax.set_ylabel('weight')plt.show()print(knr.predict([[100]]))# 100cm도 똑같이 1033g 나온다 [[56. 57. 57.]] [[34 8 14]] [1033.33333333] 선형 회귀(p.136) 사이킷에서 선형회귀 알고리즘을 사용해보자. 12345678# 파이썬from sklearn.linear_model import LinearRegressionlr= LinearRegression()# 선형회귀 모델을 훈련lr.fit(train_input, train_target) LinearRegression() 1print(lr.predict([[50]])) [1241.83860323] 1print(lr.predict([[1000]])) [38308.12631868] 1print(lr.coef_,lr.intercept_) # lr.coef_는 기울기(계수, 가중치) 값,lr.intercept_(절편) [39.01714496] -709.0186449535477 선형회귀에서 다항회귀로 바꾸자 농어 1cm가 -650g은 이상하다. 직선의 기울기 대신 곡선의 기울기를 쓰자. 직선은 1차방정식, 곡선은 2차방정식 $ 무게 =a \\times\\ 길이^2 + b \\times\\ 길이 + 절편 $ 1234#p.140train_poly = np.column_stack((train_input **2, train_input))test_poly = np.column_stack((test_input **2, test_input))print(train_poly.shape, test_poly.shape) (42, 2) (14, 2) 123lr = LinearRegression()lr.fit(train_poly,train_target)print(lr.predict([[50 ** 2, 50]])) [1573.98423528] 1print(lr.coef_, lr.intercept_) [ 1.01433211 -21.55792498] 116.0502107827827 $무게 = 1.01 \\times\\ 길이^2 - 21.6 \\times\\ 길이 +116.05$","link":"/2022/07/01/day0701_1/"},{"title":"","text":"title: ‘머신러닝4 로지스틱 회귀’date: ‘2022-07-01 11:00’ 로지스틱 회귀 선형회귀에서 출발 이진 분류 문제 해결 클래스 확률 예측 딥러닝에서도 사용됨 P177 X가 사격형일 확율 30% X가 삼각형일 확률 50% X가 원일 확률 20% 데이터 불러오기 Species(종속변수 = Y) Weight,Length, Diagonal,Height,Width(독립변수들) 1234import pandas as pdfish = pd.read_csv('https://bit.ly/fish_csv_data')fish.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Species Weight Length Diagonal Height Width 0 Bream 242.0 25.4 30.0 11.5200 4.0200 1 Bream 290.0 26.3 31.2 12.4800 4.3056 2 Bream 340.0 26.5 31.1 12.3778 4.6961 3 Bream 363.0 29.0 33.5 12.7300 4.4555 4 Bream 430.0 29.0 34.0 12.4440 5.1340 &lt;svg xmlns=”http://www.w3.org/2000/svg&quot; height=”24px”viewBox=”0 0 24 24” width=”24px”&gt; .colab-df-container { display:flex; flex-wrap:wrap; gap: 12px; } .colab-df-convert { background-color: #E8F0FE; border: none; border-radius: 50%; cursor: pointer; display: none; fill: #1967D2; height: 32px; padding: 0 0 0 0; width: 32px; } .colab-df-convert:hover { background-color: #E2EBFA; box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15); fill: #174EA6; } [theme=dark] .colab-df-convert { background-color: #3B4455; fill: #D2E3FC; } [theme=dark] .colab-df-convert:hover { background-color: #434B5C; box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15); filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3)); fill: #FFFFFF; } &lt;script&gt; const buttonEl = document.querySelector('#df-1e28b899-1483-4eec-b037-95410a050afe button.colab-df-convert'); buttonEl.style.display = google.colab.kernel.accessAllowed ? 'block' : 'none'; async function convertToInteractive(key) { const element = document.querySelector('#df-1e28b899-1483-4eec-b037-95410a050afe'); const dataTable = await google.colab.kernel.invokeFunction('convertToInteractive', [key], {}); if (!dataTable) return; const docLinkHtml = 'Like what you see? Visit the ' + '&lt;a target=&quot;_blank&quot; href=https://colab.research.google.com/notebooks/data_table.ipynb&gt;data table notebook&lt;/a&gt;' + ' to learn more about interactive tables.'; element.innerHTML = ''; dataTable['output_type'] = 'display_data'; await google.colab.output.renderOutput(dataTable, element); const docLink = document.createElement('div'); docLink.innerHTML = docLinkHtml; element.appendChild(docLink); } &lt;/script&gt; &lt;/div&gt; 데이터 탐색1234# 종속변수print(pd.unique(fish['Species']))#유니크함수로 스피시스열의 고유값 추출print(&quot;&quot;)print(fish['Species'].value_counts()) ['Bream' 'Roach' 'Whitefish' 'Parkki' 'Perch' 'Pike' 'Smelt'] Perch 56 Bream 35 Roach 20 Pike 17 Smelt 14 Parkki 11 Whitefish 6 Name: Species, dtype: int64 데이터 가공12345# 판다스 데이터 프레임에서 넘파이 배열로 변환fish_input = fish[['Weight','Length','Diagonal','Height','Width']].to_numpy() # fish 데이터 프레임에서 여러열을 선택해 새로운 데이터 프레임을 넘파이 배열로 바꾸어 저장print(fish_input.shape) (159, 5) 1print(fish_input[:5]) [[242. 25.4 30. 11.52 4.02 ] [290. 26.3 31.2 12.48 4.3056] [340. 26.5 31.1 12.3778 4.6961] [363. 29. 33.5 12.73 4.4555] [430. 29. 34. 12.444 5.134 ]] 타킷데이터, 종속변수, Y 123fish_target = fish['Species'].to_numpy()print(fish_target.shape)print(fish_target[:5]) (159,) ['Bream' 'Bream' 'Bream' 'Bream' 'Bream'] 데이터 분리 훈련 데이터 테스트 데이터 분리 123456789from sklearn.model_selection import train_test_split#임의 샘플링train_input, test_input,train_target,test_target = train_test_split( fish_input, fish_target, random_state= 42)print(train_input.shape) # 훈련데이터 값#층화 샘플링 (119, 5) 표준화 전처리 여기에서도 훈련 세트의 통계 값으로 테스트 세트를 변환해야 한다는 점을 잊지 마세요!!(중요) 훈련 세트의 평균값과 테스트 세트의 평균값는 다르다. 따라서 테스트 세트의 평균값(통계값)을 훈련세트의 평균값(통계값)으로 대체해줘야 한다. 데이터 가공 숫자 결측치가 존재, 평균값으로 대체 원본 데이터 평균으로 대치하면 안됨 훈련 데이터와 테스트 데이터 분리 데이터 누수(Data Leakage) 훈련데이터 평균값 70을 대치(기준) 테스트 데이터 평균값(75)과 모든 데이터 평균값(72.5)은 기준이 안됨 참조: https://scikit-learn.org/stable/common_pitfalls.html cf) 기준을 맞춰라 –&gt;데이터 표준화(표준점수) p97~100는 수동으로 mean,std 을 -‘# train_scaled = (train_input - mean)/ std 라는 수식을 만들어 사용했으나 StandardScaler 라는 매소드가 있으니 이를 이용하면 된다. 123456from sklearn.preprocessing import StandardScalerss = StandardScaler()ss.fit(train_input)#ss.fit(test_input)을 하면 안됨!!-&gt; 훈련테스트 통계값으로 통일train_scaled = ss.transform(train_input)test_scaled = ss.transform(test_input) 모형 만들기 K-최근접 이웃 123456from sklearn.neighbors import KNeighborsClassifierkn = KNeighborsClassifier(n_neighbors = 3)kn.fit(train_scaled, train_target)print(kn.score(train_scaled, train_target))print(kn.score(test_scaled, test_target)) 0.8907563025210085 0.85 타깃값 확인 알파벳 순으로 정렬 1print(kn.classes_) ['Bream' 'Parkki' 'Perch' 'Pike' 'Roach' 'Smelt' 'Whitefish'] 다중분율 5개 샘플에 대한 예측은 어떤 확률이냐? 12345import numpy as npproba = kn.predict_proba(test_scaled[:5])print(kn.classes_)print(np.round(proba,decimals= 4)) ['Bream' 'Parkki' 'Perch' 'Pike' 'Roach' 'Smelt' 'Whitefish'] [[0. 0. 1. 0. 0. 0. 0. ] [0. 0. 0. 0. 0. 1. 0. ] [0. 0. 0. 1. 0. 0. 0. ] [0. 0. 0.6667 0. 0.3333 0. 0. ] [0. 0. 0.6667 0. 0.3333 0. 0. ]] 첫번째 클래스는 Perch 100% 확률로 Perch로 예측 네번째 클래스는 Perch 66.7%확률로 Perch로 예측 33.3%확률로 Roach로 예측 회귀식 y= ax + b 양변에 로그를 취함 원래 값으로 돌리기 위해 양변을 다시 지수로 변환-&gt;로지스틱 회귀 로지스틱 회귀로 이진분류 수행12char_arr = np.array(['A','B','C','D','E'])print(char_arr[[True,False,True,False,False]]) ['A' 'C'] 도미와 빙어의 행만 골라냄 (bream,smelt) 123456bream_smelt_indexes =(train_target == 'Bream') | (train_target == 'Smelt')print(bream_smelt_indexes)train_bream_smelt = train_scaled[bream_smelt_indexes]target_bream_smelt = train_target[bream_smelt_indexes]print(train_scaled.shape, train_bream_smelt.shape) [ True False True False False False False True False False False True False False False True True False False True False True False False False True False False True False False False False True False False True True False False False False False True False False False False False True False True False False True False False False True False False False False False False True False True False False False False False False False False False True False True False False True True False False False True False False False False False True False False False True False True False False True True False False False False False False False False True True False False True False False] (119, 5) (33, 5) 총 119마리에서 참인 값은 33마리만 추출 모델 만들기123from sklearn.linear_model import LogisticRegressionlr = LogisticRegression()lr.fit(train_bream_smelt,target_bream_smelt) LogisticRegression() 1print(lr.predict(train_bream_smelt[:5]))#훈련한 모델로 5개 샘플 예측 ['Bream' 'Smelt' 'Bream' 'Bream' 'Bream'] 1print(lr.predict_proba(train_bream_smelt[:5]))# 예측 확율을 출력 두번째만 도미가 아님 [[0.99759855 0.00240145] [0.02735183 0.97264817] [0.99486072 0.00513928] [0.98584202 0.01415798] [0.99767269 0.00232731]] 1print(lr.classes_) # 음성클라스 도미(0): 양성크라스 빙어(1) ['Bream' 'Smelt'] cf. 분류기준 : threshold 임계값 설정(경계선 설정) 도미 Vs 빙어 [0.51,0.49]-&gt; 이런값은 도미인가 빙어인가? [0.90,0.10] 계수와 절편 1print(lr.coef_, lr.intercept_)#로지스틱 회귀는 선형회귀와 비슷 [[-0.4037798 -0.57620209 -0.66280298 -1.01290277 -0.73168947]] [-2.16155132] 12decisions = lr.decision_function(train_bream_smelt[:5])#decision_function()메서드로 Z값 출력print(decisions) [[ 13.07724442 5.67940163 -3.35341274 -3.31343798 2.17367082 -20.94258142 6.67911528] [-11.87101288 2.30253045 5.38260123 -3.16152122 3.19003127 8.30344773 -4.14607657] [ 12.33862012 5.65079591 -4.66939988 -2.1462105 1.70362799 -17.38222731 4.50479367] [ 10.54150945 6.10969846 -4.81186721 -2.96238906 2.29032761 -14.96402558 3.79674632] [ 13.67852112 5.73152066 -4.25491239 -2.55085968 1.73528849 -20.24827704 5.90871883]] z값을 확율값으로 변환시켜야 함. 지수변환(p188)시켜야 함 expit() 12from scipy.special import expitprint(expit(decisions)) [0.00240145 0.97264817 0.00513928 0.01415798 0.00232731] 다중 분류 수행하기 2진분류의 확장판 1234567# 하이퍼 파라메터 세팅# 모형을 튜닝(잘모르면 건들지 않는게 좋음, defult값 사용)# 모형 결과의 과대적합 또는 과소적합을 방지하기 위한 것lr = LogisticRegression(C =20 , max_iter = 1000)lr.fit(train_scaled, train_target)print(lr.score(train_scaled, train_target))print(lr.score(test_scaled, test_target)) 0.9327731092436975 0.925 1print(lr.predict(test_scaled[:5])) ['Perch' 'Smelt' 'Pike' 'Roach' 'Perch'] 123proba = lr.predict_proba(test_scaled[:5])print(np.round(proba, decimals = 3))print(lr.classes_) [[0. 0.014 0.841 0. 0.136 0.007 0.003] [0. 0.003 0.044 0. 0.007 0.946 0. ] [0. 0. 0.034 0.935 0.015 0.016 0. ] [0.011 0.034 0.306 0.007 0.567 0. 0.076] [0. 0. 0.904 0.002 0.089 0.002 0.001]] ['Bream' 'Parkki' 'Perch' 'Pike' 'Roach' 'Smelt' 'Whitefish'] 다중 분류일 경우 선형 방정식은 어떤 모습일까? 분류 7개 컬럼 값 5개 123print(lr.coef_,lr.intercept_)print(&quot;&quot;)print(lr.coef_.shape, lr.intercept_.shape) [[-1.49002087 -1.02912886 2.59345551 7.70357682 -1.2007011 ] [ 0.19618235 -2.01068181 -3.77976834 6.50491489 -1.99482722] [ 3.56279745 6.34357182 -8.48971143 -5.75757348 3.79307308] [-0.10458098 3.60319431 3.93067812 -3.61736674 -1.75069691] [-1.40061442 -6.07503434 5.25969314 -0.87220069 1.86043659] [-1.38526214 1.49214574 1.39226167 -5.67734118 -4.40097523] [ 0.62149861 -2.32406685 -0.90660867 1.71599038 3.6936908 ]] [-0.09205179 -0.26290885 3.25101327 -0.14742956 2.65498283 -6.78782948 1.38422358] (7, 5) (7,) 평가지표 회귀 평가지표-&gt; 결정계수($R^2$)P.121 $1-[(타깃-예측)^2의 합/(타깃-평균)^2합]$ MAE, MSE, RMSE (실제 - 예측) =오차 MAE(mean absolute errer): 오차의 절댓값의 평균 MSE(m Squared e): 오차의 제곱의 평균 RMSE(Root MSE): MSE에 제곱근을 취한값 좋은 모델이란 결정계수 :1에 수렴하면 좋은 모델 MAE외 :0에 수렴하면 좋은 모델 123456789101112131415161718import numpy as npfrom sklearn.metrics import mean_absolute_error, mean_squared_error, r2_scoretrue = np.array([1,2,3,2,3,5,4,6,5,6,7,8,8]) #실제값preds = np.array([1,1,2,2,3,4,4,5,5,7,7,6,8])#예측값#절대값 오차의 평균mae = mean_absolute_error(true, preds)print(&quot;mae=&quot;,mae)#제곱 오차의 평균mse = mean_absolute_error(true, preds)print(&quot;mse=&quot;,mse)#mse제곱근rmse =np.sqrt(mse)print(&quot;rmse=&quot;,rmse)#결정계수r2 = r2_score(true, preds)print(&quot;r2=&quot;,r2) mae= 0.5384615384615384 mse= 0.5384615384615384 rmse= 0.7337993857053428 r2= 0.8617021276595744 분류 오차 행렬 오차 행렬 실제 값 [빙어, 도미, 도미, 빙어, 도미] 예측 값 [빙어, 빙어, 도미, 빙어, 빙어] TP(빙어를 빙어로 예측):2 TN(도미를 도미로 예측):1 FP(실제도미,예측 빙어):2 FN(실제빙어,예측 도미):0 모형의 정확도 3/5 =60% 사이킷런에 분류오차행렬 함수가 있다. TP,TN,FP,FN(5,4,3,7) 정확도(5+4/5+5+3+7) 정밀도(precision:5/5+3):양성이라 예측(TP+FP)중 실제 양성값(TP)의 비율(스팸메일)-&gt;실수를 옳다고 생각하면 안되는 값 재현율(5/5+7):실제 양성(TP+FN) 값 중 양성으로 예측한 값(TP)의 비율 (암진단)-&gt;사실을 거짓으로 판단하면 큰일나는 값 로그손실 ROC Curve(=AUC) 코로나 검사 양성(1) : 음성(99) 머신러닝 모형 :98%/ 정밀도 99 인간 음성진단 :99%/ 정밀도 95 검사자가 실제는 양성이나 진단은 음성으로 내릴 가능성이 높음(의료사고)-재현율로 파악하는 것이 옳다. 123456from sklearn.metrics import confusion_matrixtrue = [0,1,1,0,0]preds = [1,0,0,0,0]confusion_matrix(true, preds) array([[2, 1], [2, 0]])","link":"/2022/07/01/day0701_2/"},{"title":"","text":"title: ‘머신러닝4 확률적 경사 하강법’date: ‘2022-07-04 09:00’ 확률적 경사 하강법 점진적 학습 (step, 보폭) 학습률 XGBoost, Light GBM, 딥러닝(이미지 분류, 자연어처리, 옵티마이져) 샘플 신경망 이미지 데이터, 자연어 자율주행 하루 데이터 1TB –&gt;학습량이 너무 큼 한꺼번에 모델을 학습하기 어려움 샘플링, 배치, 에포크, 오차(=손실=loss)가 가장 작은 지점을 찾아야 함. 이러한 방법이 확률적 경사 하강법 빠른 시간내에 손실을 줄일 수 있는 방법을 찾는것= 손실함수를 이용. 10시간 걸려 정확도95% 1시간 걸려 정확도 80% 어느 것이 좋은가? 최적화. 손실함수 로지스틱 손실 함수 123import pandas as pdfish = pd.read_csv(&quot;http://bit.ly/fish_csv_data&quot;)fish.info() &lt;class 'pandas.core.frame.DataFrame'&gt; RangeIndex: 159 entries, 0 to 158 Data columns (total 6 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 Species 159 non-null object 1 Weight 159 non-null float64 2 Length 159 non-null float64 3 Diagonal 159 non-null float64 4 Height 159 non-null float64 5 Width 159 non-null float64 dtypes: float64(5), object(1) memory usage: 7.6+ KB 1234fish_input =fish[['Weight','Length','Diagonal','Height','Width']].to_numpy()fish_target =fish['Species'].to_numpy()fish_input.shape,fish_target.shape ((159, 5), (159,)) 훈련세트와 테스트 데이터 분리 1234from sklearn.model_selection import train_test_splittrain_input,test_input, train_target, test_target =train_test_split( fish_input, fish_target, random_state =42) 훈련세트와 테스트 세트의 특성 표준화 무게, 길이, 대각선 길이, 높이, 너비-표준화 처리진행 1234567from sklearn.preprocessing import StandardScalerss = StandardScaler()ss.fit(train_input)train_scaled = ss.transform(train_input)test_scaled = ss.transform(test_input)train_scaled[:5] array([[ 0.91965782, 0.60943175, 0.81041221, 1.85194896, 1.00075672], [ 0.30041219, 1.54653445, 1.45316551, -0.46981663, 0.27291745], [-1.0858536 , -1.68646987, -1.70848587, -1.70159849, -2.0044758 ], [-0.79734143, -0.60880176, -0.67486907, -0.82480589, -0.27631471], [-0.71289885, -0.73062511, -0.70092664, -0.0802298 , -0.7033869 ]]) 모델링 확률적 경사 하강법 123456from sklearn.linear_model import SGDClassifiersc = SGDClassifier(loss = 'log',max_iter =10, random_state =42)#에포크 10회는 좀 적으니 더 숫자를 넣어라~sc.fit(train_scaled, train_target)print(sc.score(train_scaled,train_target))print(sc.score(test_scaled, test_target)) 0.773109243697479 0.775 /usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_stochastic_gradient.py:700: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit. ConvergenceWarning, partial_fit()메서드 사용하면 추가학습 가능 123sc.partial_fit(train_scaled, train_target)print(sc.score(train_scaled, train_target))print(sc.score(test_scaled, test_target)) 0.8151260504201681 0.85 에포크와 과대/과소적합 에포크 숫자가 적으면 덜학습 됨 early_stopping 에포크 숫자를 1000회로 주어졌을 때 손실 10,9,8…..3,3,3 3에 도달한 시점에서 학습을 몇번하고 그만 둠 123456789101112import numpy as npsc = SGDClassifier(loss='log',random_state=42)train_score =[]test_score= []classes = np.unique(train_target)#300번 에포크 훈련을 반복#훈련할 때마다, train_score, test_score추가를 한다.for _ in range(0,300): sc.partial_fit(train_scaled,train_target, classes= classes) train_score.append(sc.score(train_scaled,train_target)) test_score.append(sc.score(test_scaled,test_target)) 123456import matplotlib.pyplot as plt fig,ax = plt.subplots()ax.plot(train_score)ax.plot(test_score)ax.legend([&quot;train&quot;, &quot;test&quot;])plt.show() XGBoost, LightGBM코드 train-loss, train-accuaray,test-loss,test-accurary","link":"/2022/07/04/day0704_1/"},{"title":"","text":"title: ‘머신러닝5 결정트리’date: ‘2022-07-04 11:00’ 결정트리(아주 중요)123import pandas as pdwine = pd.read_csv('https://bit.ly/wine_csv_data')print(wine.head()) alcohol sugar pH class 0 9.4 1.9 3.51 0.0 1 9.8 2.6 3.20 0.0 2 9.8 2.3 3.26 0.0 3 9.8 1.9 3.16 0.0 4 9.4 1.9 3.51 0.0 데이터 가공하기. 12data = wine[['alcohol','sugar','pH']].to_numpy()target = wine['class'].to_numpy() 훈련데이터 분리 123456from sklearn.model_selection import train_test_splittrain_input, test_input, train_target, test_target =train_test_split( data, target, test_size = 0.2, random_state=42)train_input.shape, test_input.shape, train_target.shape, test_target.shape ((5197, 3), (1300, 3), (5197,), (1300,)) 123456from sklearn.preprocessing import StandardScalerss = StandardScaler()ss.fit(train_input)train_scaled = ss.transform(train_input)test_scaled = ss.transform(test_input) 12345from sklearn.linear_model import LogisticRegressionlr = LogisticRegression()lr.fit(train_scaled, train_target)print(lr.score(train_scaled,train_target))print(lr.score(test_scaled,test_target)) 0.7808350971714451 0.7776923076923077 1print(lr.coef_,lr.intercept_) [[ 0.51270274 1.6733911 -0.68767781]] [1.81777902] 모델만들기 123456789101112from sklearn.tree import DecisionTreeClassifierimport matplotlib.pyplot as pltfrom sklearn.tree import plot_treedt = DecisionTreeClassifier(max_depth= 6,random_state=42)#깊이를 줄여보자dt.fit(train_scaled,train_target)print(dt.score(train_scaled,train_target)) #훈련 셋트print(dt.score(test_scaled,test_target)) #테스트 셋트plt.figure(figsize=(10,7))plot_tree(dt)plt.show() 0.8766596113142198 0.8523076923076923 훈련정확도는 99.6% 테스트 정확도는 85.9%+-&gt;과대적합이 일어남-max_depth= 7값을 조정하여 비슷하게 만듬 노드란 무엇인가? 0이면 레드 와인(1599) 1이면 화이트 와인(4898) 1wine['class'].value_counts() 1.0 4898 0.0 1599 Name: class, dtype: int64 123456plt.figure(figsize=(10,7))plot_tree(dt, max_depth=1, filled=True, feature_names=['alcohol','sugar','pH'])plt.show() 불순도(Gini impurity) 비율(0~0.5) 레드와인:화이트 와인 이 5:5 일때 불순도가 가장 높은 상태(0.5) 한범주안에서 서로 다른 데이터가 얼마나 섞여 있는지를 나타냄. 흰색과 검은색이 각각 반반이면 불순도 최대 0.5 흰색과 검은색이 완전 분리가 되면 흰색 노드 불순도 최소 0 검은색 노드 불순도 최소 0 엔트로피(Entropy) 불확실한 정도를 의미(0~1) 흰색과 검은색이 각각 반이면 엔트로피 최대 1 흰색과 검은색이 완전 분리가 되면 흰색 노드 엔트로피도 0 검은색 노드 엔트로피도 0 1234567891011121314from sklearn.tree import DecisionTreeClassifierimport matplotlib.pyplot as pltfrom sklearn.tree import plot_treedt = DecisionTreeClassifier(criterion = 'entropy', max_depth = 42, random_state=42)dt.fit(train_scaled,train_target)dt.score(train_scaled,train_target) #훈련 셋트dt.score(test_scaled,test_target) #테스트 셋트plt.figure(figsize=(10,7))plot_tree(dt, max_depth=1, filled=True, feature_names=['alcohol','sugar','pH'])plt.show() 특성 중요도 어떤 특성이 결정 트리 모델에 영향을 주었는가? 1print(dt.feature_importances_) [0.23739824 0.5051808 0.25742097] 현업에서의 적용 현업에서 DecisionTreeClassifier을 사용하기에는 오래되었다.(1970년대) 렘덤포르세트, XGBoost 하이퍼 파라미터가 매우 많음 검증 세트 훈련세트와 테스트세트 훈련 : 교과서로 공부하는 것 훈련세트 : 모의평가 검증 : 강남대성 모의고사 테스트 : 중간고사, 기말고사 실전 :수능 123456789import pandas as pdwine = pd.read_csv('https://bit.ly/wine_csv_data')data = wine[['alcohol','sugar','pH']].to_numpy()target = wine['class'].to_numpy()#훈련 80%, 테스트 20%train_input, test_input, train_target, test_target =train_test_split( data, target, test_size = 0.2, random_state=42)train_input.shape, test_input.shape, train_target.shape, test_target.shape ((5197, 3), (1300, 3), (5197,), (1300,)) 12345#훈련80%, 검증20%sub_input, val_input,sub_target,val_target = train_test_split( train_input, train_target, test_size =0.2, random_state=42)sub_input.shape,val_input.shape,sub_target.shape,val_target.shape ((4157, 3), (1040, 3), (4157,), (1040,)) 훈련데이터: sub_input, sub_target 검증데이터: val_input, val_target 테스트데이터: test_input, test_target 123456from sklearn.tree import DecisionTreeClassifierdt =DecisionTreeClassifier(random_state =42)dt.fit(sub_input,sub_target)print(&quot;훈련성과:&quot;,dt.score(sub_input,sub_target))print(&quot;검증성과:&quot;,dt.score(val_input,val_target))print(&quot;최종:&quot;,dt.score(test_input,test_target)) 훈련성과: 0.9971133028626413 검증성과: 0.864423076923077 최종: 0.8569230769230769 훈련 : 87% 검증 : 86%-&gt;과대적합 최종 : 85% 교차 검증 데이터 셋을 반복분할 For loop 샘플링의 편향성을 방지 교차검증을 한다고 해서 정확도가 무조건 올라가는 것은 아님. 모형을 안정적으로 만들어줌(과대적합 방지) 12345678from sklearn.model_selection import KFoldimport numpy as npdf = np.array([1,2,3,4,5,6,7,8,9,10])#데이터를 K폴드로 나눈다.folds = KFold(n_splits=5, shuffle = True)for train_idx,valid_idx in folds.split(df): print(f'훈련데이터:{df[train_idx]},검증데이터:{df[valid_idx]}') 훈련데이터:[1 2 3 4 5 6 8 9],검증데이터:[ 7 10] 훈련데이터:[ 1 2 3 4 6 7 8 10],검증데이터:[5 9] 훈련데이터:[ 1 2 4 5 7 8 9 10],검증데이터:[3 6] 훈련데이터:[ 1 3 4 5 6 7 9 10],검증데이터:[2 8] 훈련데이터:[ 2 3 5 6 7 8 9 10],검증데이터:[1 4] 교차 검증 함수 cross_validate() 1234from sklearn.model_selection import cross_validatescores =cross_validate(dt,train_input,train_target)print(scores)print(&quot;평균:&quot;,np.mean(scores['test_score'])) {'fit_time': array([0.01610422, 0.00758529, 0.00780439, 0.00830793, 0.00751185]), 'score_time': array([0.00120473, 0.00096321, 0.00094819, 0.00098753, 0.00134945]), 'test_score': array([0.86923077, 0.84615385, 0.87680462, 0.84889317, 0.83541867])} 평균: 0.855300214703487 StratifiedKFold 사용 타깃클래스를 골고루 나누기 위함 1234from sklearn.model_selection import StratifiedKFold scores = cross_validate(dt, train_input, train_target, cv = StratifiedKFold())print(scores)print(&quot;평균 : &quot;, np.mean(scores['test_score'])) {'fit_time': array([0.01418257, 0.00769162, 0.00846624, 0.00809741, 0.00809479]), 'score_time': array([0.00103283, 0.00094843, 0.00096679, 0.00104737, 0.00102282]), 'test_score': array([0.86923077, 0.84615385, 0.87680462, 0.84889317, 0.83541867])} 평균 : 0.855300214703487 10-폴드 교차 검증 12345from sklearn.model_selection import StratifiedKFoldsplitter =StratifiedKFold(n_splits = 10,shuffle =True, random_state = 42)#10번 더 교차 검증을 수행scores =cross_validate(dt,train_input,train_target,cv =splitter)print(scores)print(&quot;평균:&quot;,np.mean(scores['test_score'])) {'fit_time': array([0.01752782, 0.00863743, 0.00879025, 0.0088315 , 0.00837684, 0.00851107, 0.00831246, 0.00826931, 0.00829077, 0.00844049]), 'score_time': array([0.00099802, 0.00086832, 0.00085974, 0.00183129, 0.00091791, 0.0008316 , 0.00078368, 0.00092673, 0.00080705, 0.00082994]), 'test_score': array([0.83461538, 0.87884615, 0.85384615, 0.85384615, 0.84615385, 0.87307692, 0.85961538, 0.85549133, 0.85163776, 0.86705202])} 평균: 0.8574181117533719 하이퍼 파라미터 튜닝 그리드 서치(사람이 수동입력) max_depth: [1.3.5…] 랜덤 서치(사람이 범위만 지정) max_depth: 1~10/by random 베이지안 옵티마이제이션 사람의 개입없이 하이퍼파라미터 튜닝을 자동적으로 수행하는 기술을 AutoML이라고 함 예)PyCaret 각 모델마다 적게는 1-2개에서 많게는 5-6개의 매개 변수를 제공한다 하이퍼파라미터와 동시에 교차검증을 수행(불가능하다) 교차검증 5번 교차검증 1번 돌때, Max Depth3번 적용 총 결과값 3x5x2나옴 Max Death =1,3,7 criterion= gini.entropy 1234567from sklearn.model_selection import GridSearchCVparams = { 'min_impurity_decrease' : [0.0001,0.0002,0.0003,0.0004,0.0005] }gs = GridSearchCV(DecisionTreeClassifier(random_state =42),params, n_jobs= -1)gs.fit(train_input,train_target) GridSearchCV(estimator=DecisionTreeClassifier(random_state=42), n_jobs=-1, param_grid={'min_impurity_decrease': [0.0001, 0.0002, 0.0003, 0.0004, 0.0005]}) 123print(&quot;best:&quot;,gs.best_estimator_)dt = gs.best_estimator_print(dt.score(train_input, train_target)) best: DecisionTreeClassifier(min_impurity_decrease=0.0001, random_state=42) 0.9615162593804117 123456789101112from sklearn.model_selection import GridSearchCVparams = { 'criterion' : ['gini', 'entropy'], 'max_depth ': [1,3,7], 'min_impurity_decrease' : [0.0001,0.0002,0.0003,0.0004,0.0005] }gs = GridSearchCV(DecisionTreeClassifier(random_state =42),params, n_jobs= -1)gs.fit(train_input,train_target)print(&quot;best:&quot;,gs.best_estimator_)dt = gs.best_estimator_print(dt.score(train_input, train_target)) best: DecisionTreeClassifier(max_depth=7, min_impurity_decrease=0.0005, random_state=42) 0.8830094285164518","link":"/2022/07/04/day0704_2/"},{"title":"","text":"title: ‘머신러닝5 트리의 앙상블’date: ‘2022-07-05 09:00’ 랜덤 포레스트 Decision Tree(나무 1개)에서 출발 여러개 심음 샘플링 Feature Importances 예측해야 할 행의 갯수,100만개 컬럼의 갯수 200개 ==&gt;100개 축소 나무 100개를 심고 평균을 내자 나무 1개당 컬럼을 10개로 다양한 값 찾기 T1 mae :20, T2 mae :30, T3 mae 10…..-&gt;T1~T100 mae :20(평균값) Feature Importances 샘플링 : 부트스트랩 샘플(복원추출) 12345678910111213141516171819202122232425262728293031323334# 라이브러리 불러오기 import numpy as np import pandas as pd from sklearn.model_selection import train_test_split, cross_validatefrom sklearn.ensemble import RandomForestClassifier# 데이터 불러오기wine = pd.read_csv('https://bit.ly/wine_csv_data')# input, target 분리 data = wine[['alcohol', 'sugar', 'pH']].to_numpy()target = wine['class'].to_numpy()# 훈련데이터, 테스트 데이터 분리train_input, test_input, train_target, test_target = train_test_split( data, target, test_size = 0.2, random_state = 42)# 모델링rf = RandomForestClassifier(n_jobs=-1, random_state = 42)# 모형 평가scores = cross_validate(rf, train_input, train_target, return_train_score = True, n_jobs =-1)print(np.mean(scores['train_score']), np.mean(scores['test_score']),&quot;과대적합&quot;)# 특성 중요도rf.fit(train_input, train_target)print(rf.feature_importances_,&quot;역시 당도가 중요&quot;)# OOB rf = RandomForestClassifier(oob_score = True, n_jobs = -1, random_state = 42)rf.fit(train_input, train_target)print(rf.oob_score_,&quot;OOB검증세트와 비슷&quot;) 0.9973541965122431 0.8905151032797809 과대적합 [0.23167441 0.50039841 0.26792718] 역시 당도가 중요 0.8934000384837406 OOB검증세트와 비슷 그레이디언트 부스팅 기존알고리즘에 가중치(보정치)를 주어 학습을 시킴 경사하강법의 원리를 이용함 T1~Tn 증가하면서 오차를 보정해주며 정확성을 높임 랜덤포레스트와의 차이점 랜덤포레스트는 각 나무간의 상호 연관성이 없음(부트스트랩샘플) 그레이디언트 부스팅은 각 나무간 상호 연관성이 있음 -&gt;그러나 너무 느린 속도 XGBoost,LightGBM이 대안 1234from sklearn.ensemble import GradientBoostingClassifiergb = GradientBoostingClassifier(random_state=42)scores = cross_validate(gb,train_input,train_target, return_train_score=True,n_jobs=-1)print(np.mean(scores['train_score']),np.mean(scores['test_score'])) 0.8881086892152563 0.8720430147331015 1234#결정트리 갯수를 기본100-&gt;500개로 늘리고 학습율 기본0.1-&gt;0.2로 늘려 봄gb = GradientBoostingClassifier(n_estimators=500,learning_rate=0.2,random_state=42)scores = cross_validate(gb,train_input,train_target, return_train_score=True,n_jobs=-1)print(np.mean(scores['train_score']),np.mean(scores['test_score'])) 0.9464595437171814 0.8780082549788999 특성 중요도 12gb.fit(train_input, train_target)print(gb.feature_importances_) [0.15872278 0.68010884 0.16116839]","link":"/2022/07/05/day0705_1/"},{"title":"","text":"title: ‘머신러닝6 비지도학습’date: ‘2022-07-05 13:00’ 주성분 분석(PCA principal component analysis) 이론적으로 어려움 좌표계 공간개념(직교와 회전) 공분산(통계롼련 내용) Feature Engineerin기법 StandardScaler() 현 머신러닝의 문제점: 컬럼의 갯수가 매우 많음(요소의 다양성) 우리의 판단으로 컬럼갯수를 줄였으나 이제는 통계적으로 줄이자-&gt;차원축소 차원축소 특성이 많으면 훈련데이터가 쉽게 과대적합이 된다. 특성을 줄요서 학습모델의 성능을 향상시키다. 모델의 학습시간을 감소시켜줌 대표적인 방법론:PCA,EFA PCA vs EFA EFA(탐색적요인분석), Factor Analysis 예)국어 40, 수학 100, 과학 100, 영어 30 평가: 귀학생은 언어영역은 수준이 낮은 편이나 수리영역은 수준이 높습니다. 범주형&amp;수치데이터셋 PCA(주성분분석) 장비1, 장비2, 장비3….. PC1, PC2, PC3,….PCN(ex.장비1과 장비2의 무게, 장비3과 장비4의 길이….) 원래 가지고 있던 정보를 알 수 없음 (정보손실) 범주형 데이터셋에는 사용 안됨-&gt;무조건 수치형 데이터셋에서만 사용!! pca 실행전 ,반드시 표준화 처리(스케일링 실행) p320 1!wget https://bit.ly/fruits_300_data -O fruits_300.npy --2022-07-05 04:55:12-- https://bit.ly/fruits_300_data Resolving bit.ly (bit.ly)... 67.199.248.10, 67.199.248.11 Connecting to bit.ly (bit.ly)|67.199.248.10|:443... connected. HTTP request sent, awaiting response... 301 Moved Permanently Location: https://github.com/rickiepark/hg-mldl/raw/master/fruits_300.npy [following] --2022-07-05 04:55:12-- https://github.com/rickiepark/hg-mldl/raw/master/fruits_300.npy Resolving github.com (github.com)... 140.82.112.3 Connecting to github.com (github.com)|140.82.112.3|:443... connected. HTTP request sent, awaiting response... 302 Found Location: https://raw.githubusercontent.com/rickiepark/hg-mldl/master/fruits_300.npy [following] --2022-07-05 04:55:13-- https://raw.githubusercontent.com/rickiepark/hg-mldl/master/fruits_300.npy Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ... Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected. HTTP request sent, awaiting response... 200 OK Length: 3000128 (2.9M) [application/octet-stream] Saving to: ‘fruits_300.npy’ fruits_300.npy 100%[===================&gt;] 2.86M --.-KB/s in 0.02s 2022-07-05 04:55:13 (179 MB/s) - ‘fruits_300.npy’ saved [3000128/3000128] 12345import numpy as npfruits = np.load('/content/fruits_300.npy')fruits_2d = fruits.reshape(-1,100*100)#300개의 행 10000개의 열fruits_2d.shape (300, 10000) 123from sklearn.decomposition import PCApca = PCA(n_components=50)#행을 50개로 압축pca.fit(fruits_2d) PCA(n_components=50) 1print(pca.components_.shape)# 행을 300-&gt;50개로 줄임 (50, 10000) 12345678910111213141516import matplotlib.pyplot as pltdef draw_fruits(arr, ratio=1): n = len(arr) # n은 샘플 개수입니다 # 한 줄에 10개씩 이미지를 그립니다. 샘플 개수를 10으로 나누어 전체 행 개수를 계산합니다. rows = int(np.ceil(n/10)) # 행이 1개 이면 열 개수는 샘플 개수입니다. 그렇지 않으면 10개입니다. cols = n if rows &lt; 2 else 10 fig, axs = plt.subplots(rows, cols, figsize=(cols*ratio, rows*ratio), squeeze=False) for i in range(rows): for j in range(cols): if i*10 + j &lt; n: # n 개까지만 그립니다. axs[i, j].imshow(arr[i*10 + j], cmap='gray_r') axs[i, j].axis('off') plt.show() 1draw_fruits(pca.components_.reshape(-1,100,100)) 123# 머신러닝에서 컬럼의 갯수를 10000개에서 50개로 줄임fruits_pca = pca.transform(fruits_2d)print(fruits_pca.shape) (300, 50) 훈련데이터, 테스트 데이터로 분리 설명된 분산 주성분이 원본 데이터의 분산을 얼마나 잘 나타내는지 기록한 값 12# 원본 이미지 압축-&gt; 결과값은 92%print(np.sum(pca.explained_variance_ratio_)) 0.9215343906846957 12plt.plot(pca.explained_variance_ratio_)plt.show() 1print(np.sum(pca.explained_variance_ratio_[:20])) 0.8416602343085364","link":"/2022/07/05/day0705_2/"},{"title":"","text":"title: ‘REVIEW-그래프와 머신러닝’date: ‘2022-07-06 09:00’ 데이터 분석(머신러닝, 딥러닝) 프로세스 데이터 불러오기 CSV, 오라클, MySQL, PostgreSQL, 클라우드 DB연동 탐색적 자료 분석 데이터 전처리 및 가공 잠정적인 컬럼의 갯수를 지정 머신러닝 모델(=통계 모델링, t.test, 분산분석, 교차분석) 머신러닝 모델의 경우 배포(지금은 다루지 않음) JSP-스프링 웹개발 시 배우게 됨. 통계 모델링 경우 p-value값 기준으로 귀무가설 및 대립가설 검정 (공통) 결과보고서를 작성 필요. .PPT준비 그래프 복습 수치형 데이터 시각화 범주형 데이터 시각화 데이터 관계 시각화 matplotlib 라이브러리 방법(복잡) seaborn 라이브러리 방법(단순) 수치형 데이터 시각화123import seaborn as snstitanic = sns.load_dataset('titanic')print(titanic.head(10)) survived pclass sex age sibsp parch fare embarked class \\ 0 0 3 male 22.0 1 0 7.2500 S Third 1 1 1 female 38.0 1 0 71.2833 C First 2 1 3 female 26.0 0 0 7.9250 S Third 3 1 1 female 35.0 1 0 53.1000 S First 4 0 3 male 35.0 0 0 8.0500 S Third 5 0 3 male NaN 0 0 8.4583 Q Third 6 0 1 male 54.0 0 0 51.8625 S First 7 0 3 male 2.0 3 1 21.0750 S Third 8 1 3 female 27.0 0 2 11.1333 S Third 9 1 2 female 14.0 1 0 30.0708 C Second who adult_male deck embark_town alive alone 0 man True NaN Southampton no False 1 woman False C Cherbourg yes False 2 woman False NaN Southampton yes True 3 woman False C Southampton yes False 4 man True NaN Southampton no True 5 man True NaN Queenstown no True 6 man True E Southampton no True 7 child False NaN Southampton no False 8 woman False NaN Southampton yes False 9 child False NaN Cherbourg yes False 12# 히스토그램sns.histplot(data= titanic, x = 'age',bins=10, hue= 'alive',multiple='stack') &lt;matplotlib.axes._subplots.AxesSubplot at 0x7faf86744d10&gt; 123# 확률밀도추정(KDE) 함수 그래프- 히스토그램을 부드러운 곡선 형태로 표현한다.# 연속형 데이터 1개만 쓸 때 사용, y축은 수량의 비율sns.kdeplot(data =titanic, x ='age',hue= 'alive',multiple='stack') &lt;matplotlib.axes._subplots.AxesSubplot at 0x7faf856ddf50&gt; 분포도 수치형 데이터 한개 컬럼의 분포를 나타내는 그래프 정규분포인가? ditstplot()히스토그램에 kdeplot와,rugplot을 한번에 그림 1sns.displot(data =titanic, x ='age') &lt;seaborn.axisgrid.FacetGrid at 0x7faf85734850&gt; 1sns.displot(data =titanic, x ='age',kind='kde') &lt;seaborn.axisgrid.FacetGrid at 0x7faf85590050&gt; 1sns.displot(data =titanic, x ='age',kde =True) &lt;seaborn.axisgrid.FacetGrid at 0x7faf85621910&gt; 범주형 데이터 시각화 x축 범주형, y축 수치 데이터 123# 막대 그래프-matplotlib은 개수를 세는 작업을 해줘야하지만 seaborn은 알아서 해준다.sns.barplot(x='class',y='fare',data= titanic)# 클래스 별로 가격을 표시했지만 그 가격은 평균치를 나타내고 에러바(오차막대)를 만들어줌 &lt;matplotlib.axes._subplots.AxesSubplot at 0x7faf8540f810&gt; 12# 포인트 플롯sns.pointplot(x='class',y='fare',data=titanic) &lt;matplotlib.axes._subplots.AxesSubplot at 0x7faf853f1890&gt; 12# boxplot(박스플롯)sns.boxplot(x='class',y='age',data=titanic) &lt;matplotlib.axes._subplots.AxesSubplot at 0x7faf8535b1d0&gt; 12#바이올린 플롯sns.violinplot(x= 'class', y='age', hue='sex',data=titanic,split =True) &lt;matplotlib.axes._subplots.AxesSubplot at 0x7faf8527f590&gt; 123# 카운트 플롯# - 범주형데이터의 갯수 확인할 때 사용sns.countplot(x = 'alive', data= titanic) &lt;matplotlib.axes._subplots.AxesSubplot at 0x7faf852701d0&gt; 데이터 관계 시각화 여러 데이터 사이의 관계도 파악을 위한 그래 1234# 히트맵flights =sns.load_dataset('flights')print(flights.head(7)) year month passengers 0 1949 Jan 112 1 1949 Feb 118 2 1949 Mar 132 3 1949 Apr 129 4 1949 May 121 5 1949 Jun 135 6 1949 Jul 148 1234# 각 연도별 월별 승객수 구하기# flights['year'].value_count()flights_pivot = flights.pivot(index='month', columns='year', values='passengers')print(flights_pivot) year 1949 1950 1951 1952 1953 1954 1955 1956 1957 1958 1959 1960 month Jan 112 115 145 171 196 204 242 284 315 340 360 417 Feb 118 126 150 180 196 188 233 277 301 318 342 391 Mar 132 141 178 193 236 235 267 317 356 362 406 419 Apr 129 135 163 181 235 227 269 313 348 348 396 461 May 121 125 172 183 229 234 270 318 355 363 420 472 Jun 135 149 178 218 243 264 315 374 422 435 472 535 Jul 148 170 199 230 264 302 364 413 465 491 548 622 Aug 148 170 199 242 272 293 347 405 467 505 559 606 Sep 136 158 184 209 237 259 312 355 404 404 463 508 Oct 119 133 162 191 211 229 274 306 347 359 407 461 Nov 104 114 146 172 180 203 237 271 305 310 362 390 Dec 118 140 166 194 201 229 278 306 336 337 405 432 1sns.heatmap(data = flights_pivot)# 시각화를 통해 1960년대 8월에 가장 사람들 수가 많았다. &lt;matplotlib.axes._subplots.AxesSubplot at 0x7faf851e8610&gt; 12# 라인플롯sns.lineplot(x='year',y='passengers',data=flights) &lt;matplotlib.axes._subplots.AxesSubplot at 0x7faf85117f10&gt; 1234# 산점도tips =sns.load_dataset('tips')tips.head(8)# 영수증금액/ 팁/ 성별/ 담배/ 요일/ 시간/ 같이먹은 사람의 수(카운트데이터) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } total_bill tip sex smoker day time size 0 16.99 1.01 Female No Sun Dinner 2 1 10.34 1.66 Male No Sun Dinner 3 2 21.01 3.50 Male No Sun Dinner 3 3 23.68 3.31 Male No Sun Dinner 2 4 24.59 3.61 Female No Sun Dinner 4 5 25.29 4.71 Male No Sun Dinner 4 6 8.77 2.00 Male No Sun Dinner 2 7 26.88 3.12 Male No Sun Dinner 4 &lt;svg xmlns=”http://www.w3.org/2000/svg&quot; height=”24px”viewBox=”0 0 24 24” width=”24px”&gt; .colab-df-container { display:flex; flex-wrap:wrap; gap: 12px; } .colab-df-convert { background-color: #E8F0FE; border: none; border-radius: 50%; cursor: pointer; display: none; fill: #1967D2; height: 32px; padding: 0 0 0 0; width: 32px; } .colab-df-convert:hover { background-color: #E2EBFA; box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15); fill: #174EA6; } [theme=dark] .colab-df-convert { background-color: #3B4455; fill: #D2E3FC; } [theme=dark] .colab-df-convert:hover { background-color: #434B5C; box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15); filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3)); fill: #FFFFFF; } &lt;script&gt; const buttonEl = document.querySelector('#df-98401926-69e1-464f-9f4f-ce6a7afd0680 button.colab-df-convert'); buttonEl.style.display = google.colab.kernel.accessAllowed ? 'block' : 'none'; async function convertToInteractive(key) { const element = document.querySelector('#df-98401926-69e1-464f-9f4f-ce6a7afd0680'); const dataTable = await google.colab.kernel.invokeFunction('convertToInteractive', [key], {}); if (!dataTable) return; const docLinkHtml = 'Like what you see? Visit the ' + '&lt;a target=&quot;_blank&quot; href=https://colab.research.google.com/notebooks/data_table.ipynb&gt;data table notebook&lt;/a&gt;' + ' to learn more about interactive tables.'; element.innerHTML = ''; dataTable['output_type'] = 'display_data'; await google.colab.output.renderOutput(dataTable, element); const docLink = document.createElement('div'); docLink.innerHTML = docLinkHtml; element.appendChild(docLink); } &lt;/script&gt; &lt;/div&gt; 12# 두개의 연속형 데이터sns.scatterplot(x='total_bill',y='tip',data= tips) &lt;matplotlib.axes._subplots.AxesSubplot at 0x7faf8564f0d0&gt; 12sns.scatterplot(x='total_bill',y='tip',hue ='time',data= tips)# 저녁에 더 많은 팁을 준다. &lt;matplotlib.axes._subplots.AxesSubplot at 0x7faf854beed0&gt; 12sns.scatterplot(x='total_bill',y='tip',hue ='sex',data= tips)# 남자들의 경우 영수증 금액이 클수록 팁의 양(+)에 상관관계가 있다. &lt;matplotlib.axes._subplots.AxesSubplot at 0x7faf863f4750&gt; 1234# 회귀선#선형회귀 적합선(상관관계를 표현한 선)을 포함한 산점도를 그리자.sns.regplot(x= 'total_bill',y= 'tip', data =tips)# 30달라를 냈을 때 4달러의 팁이 예상됨. &lt;matplotlib.axes._subplots.AxesSubplot at 0x7faf866085d0&gt; 12sns.countplot(x='sex',data =tips)# 남자와 여자를 비교할 때 남자들이 자주 팁을 준다. &lt;matplotlib.axes._subplots.AxesSubplot at 0x7faf867a81d0&gt; 12sns.countplot(x='sex',hue='time',data=tips)# 남자가 여자보다 저녁시간에 팁주는 횟수가 많다. &lt;matplotlib.axes._subplots.AxesSubplot at 0x7faf86796b10&gt; 1sns.relplot(x=&quot;total_bill&quot;,y=&quot;tip&quot;,hue='size',data=tips) &lt;seaborn.axisgrid.FacetGrid at 0x7faf8692f450&gt; 머신러닝 리뷰 가장 인기 있는 모델 lightGBM, XGBoost 선형회귀 선형회귀식을 찾는 것이 중요 $y =3 x +4$ 에 근사한 데이터 50개 생성 12345678910111213141516import numpy as np import pandas as pd# 시드값 고정- 랜덤한 내용이 다른 사람들과 같은 결과를 얻기 위해np.random.seed(0)intercept = 4 # 절편, 상수slope = 3 # 기울기# 변동성 주기 위해 노이즈 생성noise = np.random.randn(50, 1)x = 5 * np.random.rand(50, 1) # 0과 5사이의 실숫값 50개 생성y = slope * x + intercept + noise# 데이터 프레임 생성data = pd.DataFrame({'X' : x[:, 0], 'Y' : y[:, 0]})print(data) X Y 0 0.794848 8.148596 1 0.551876 6.055784 2 3.281648 14.823682 3 0.690915 8.313637 4 0.982912 8.816293 5 1.843626 8.553600 6 4.104966 17.264987 7 0.485506 5.305162 8 4.189725 16.465955 9 0.480492 5.852075 10 4.882297 18.790936 11 2.343256 12.484042 12 4.883805 19.412454 13 3.024228 13.194358 14 3.696318 15.532817 15 0.195939 4.921491 16 1.414035 9.736184 17 0.600983 5.597790 18 1.480701 8.755171 19 0.593639 4.926820 20 1.589916 6.216758 21 2.071315 10.867564 22 0.320737 5.826649 23 3.462361 13.644917 24 2.833007 14.768776 25 1.326947 6.526477 26 2.616240 11.894479 27 0.469703 5.221924 28 2.879732 14.171977 29 4.646481 19.408802 30 1.592845 8.933482 31 3.337052 14.389318 32 0.658989 5.089182 33 3.581636 12.764112 34 1.447030 7.993179 35 0.915957 6.904219 36 2.932565 14.027985 37 0.100538 5.503993 38 4.144700 16.046774 39 0.023477 3.768129 40 3.389083 13.118695 41 1.350040 6.630102 42 3.675970 13.321640 43 4.810943 20.383604 44 1.243766 7.221645 45 2.880787 12.204286 46 2.960210 11.627834 47 2.861260 13.361269 48 1.115408 5.732327 49 4.763745 18.078495 1234import matplotlib.pyplot as pltfig, ax = plt.subplots()ax.scatter(data['X'], data['Y'])plt.show() 12import seaborn as sns sns.scatterplot(x = 'X', y = 'Y', data = data) &lt;matplotlib.axes._subplots.AxesSubplot at 0x7faf85003c10&gt; 선형회귀 모형 훈련 모형 생성 후, 회귀계수 3과 y절편 4에 근사한 값이 나와야 함 1234567from sklearn import linear_modelfrom sklearn.linear_model import LinearRegressionlr_model = LinearRegression()#,선형회귀 모델lr_model.fit(x,y)##모델훈련print('y절편:', lr_model.intercept_)print('회귀계수:', lr_model.coef_) y절편: [4.05757639] 회귀계수: [[3.03754061]] 1234567891011# 예측값y_pred = lr_model.predict(x)fig, ax = plt.subplots()ax.scatter(x, y)ax.plot(x, y_pred, color='green')# slope, intercept label = 'slope: {}\\nintercept: {}'.format(round(lr_model.coef_[0][0], 2), round(lr_model.intercept_[0], 2))ax.text(3.5, 4, label, style ='italic', fontsize = 10, color =&quot;green&quot;)plt.show() 로지스틱 회귀- 123456789101112131415161718192021import numpy as npimport matplotlib.pyplot as pltdef sigmoid(arr, scale=1): arr = np.asarray(arr) result = 1/(1 + np.exp(-arr*scale)) return resultx = np.linspace(-6, 6)y = sigmoid(x)fig, ax = plt.subplots()ax.plot(x, y)ax.grid(which='major', axis='y', linestyle='--')ax.axvline(x=0, color='r', linestyle='--', linewidth=1)ax.set_ylim(0,1)ax.set_yticks([0, 1, 0.5])ax.text(0-0.1, 0.5, '0.5', ha='right')ax.set_title('Sigmoid Graph')plt.show() 12345678910111213# 라이브러리 불러오기import matplotlib.pyplot as pltimport numpy as npfrom sklearn.linear_model import LogisticRegressionfrom sklearn.metrics import classification_report, confusion_matrix# 데이터 가져오기x = np.arange(10).reshape(-1, 1)y = np.array([0, 0, 0, 0, 0, 1, 1, 1, 1, 1])# 모델 생성 및 학습model = LogisticRegression(solver='liblinear', C=10.0, random_state=0)model.fit(x, y) LogisticRegression(C=10.0, random_state=0, solver='liblinear') 123# 모형 평가p_pred =model.predict_proba(x)print('p_pred',p_pred,sep ='\\n') p_pred [[0.97979027 0.02020973] [0.94958202 0.05041798] [0.87976149 0.12023851] [0.73975066 0.26024934] [0.52477284 0.47522716] [0.30020373 0.69979627] [0.1428487 0.8571513 ] [0.06080627 0.93919373] [0.02453462 0.97546538] [0.00967652 0.99032348]] 12y_pred = model.predict(x)print('y_pred',y_pred) y_pred [0 0 0 0 0 1 1 1 1 1] 12345678910fig, ax = plt.subplots()ax.scatter(x, y)ax.plot(x, p_pred[:, 1], color = 'black', marker='o', markersize=6)ax.plot()ax.set_xticks(x)ax.set_yticks(np.arange(0, 1.1, 0.1))ax.grid(which='major', alpha=0.5)plt.show() 12conf_m = confusion_matrix(y,y_pred)print(conf_m) [[5 0] [0 5]] 123456789101112cm = confusion_matrix(y, y_pred)fig, ax = plt.subplots(figsize=(8, 8))ax.imshow(cm, cmap = 'Pastel1')# pastel2는 색깔임 'GnBu'ax.grid(False)ax.xaxis.set(ticks=(0, 1), ticklabels=('Predicted 0', 'Predicted 1'))ax.yaxis.set(ticks=(0, 1), ticklabels=('Actual 0', 'Actual 1'))ax.set_ylim(1.5, -0.5)for i in range(2): for j in range(2): ax.text(j, i, cm[i, j], ha='center', va='center', color='black', fontsize=20)plt.show() 결정 트리 분류와 회귀 문제에 모두 사용가능 주요 개념 작동 원리 데이터를 가장 잘 구분하는 조건을 정함. 조건을 기준으로 데이터를 두 범주로 나눔 나뉜 각 범주의 데이터를 구분하는 조건을 정함 각 조건을 기준으로 데이터를 두 범주로 나눔 언제까지 계속 분할할지 정한 후, 최종 결정 값을 구함. 불순도(Impurity) 한 범주 안에 서로 다른 데이터가 얼마나 섞여 있는지 나타냄 흰색과 검은색이 50:50으로 섞여 있다. (불순도 최대) 흰색과 검은색으로 완전 분리 되었다. (불순도 최소) 엔트로피(Entropy) 불확실한 정도를 의미함. 0 ~ 1로 정함. 흰색과 검은색이 50:50으로 섞여 있다. 엔트로피 1 흰색과 검은색으로 완전 분리 되었다. 엔트로피 0 정보이득(Information Gain) 1에서 엔트로피를 뺀 수치 정보 이득을 최대화하는 방향(엔트로피를 최소화 하는 방향)으로 노드를 분할함 지니 불순도(Gini Impurity) 지니 불순도 값이 클수록 불순도도 높고, 작을수록 불순도도 낮음. 엔트로피와 마찬가지로 지니 불순도가 낮아지는 방향으로 노드 분할함. 1234567from sklearn.tree import DecisionTreeClassifierfrom sklearn.model_selection import train_test_split import seaborn as sns # tips 데이터셋 titanic = sns.load_dataset('titanic')titanic.info() &lt;class 'pandas.core.frame.DataFrame'&gt; RangeIndex: 891 entries, 0 to 890 Data columns (total 15 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 survived 891 non-null int64 1 pclass 891 non-null int64 2 sex 891 non-null object 3 age 714 non-null float64 4 sibsp 891 non-null int64 5 parch 891 non-null int64 6 fare 891 non-null float64 7 embarked 889 non-null object 8 class 891 non-null category 9 who 891 non-null object 10 adult_male 891 non-null bool 11 deck 203 non-null category 12 embark_town 889 non-null object 13 alive 891 non-null object 14 alone 891 non-null bool dtypes: bool(2), category(2), float64(2), int64(4), object(5) memory usage: 80.7+ KB suvived의 비율을 구한다 0: 사망자 1: 생존 1titanic['survived'].value_counts() 0 549 1 342 Name: survived, dtype: int64 123456X = titanic[['pclass', 'parch', 'fare']]y = titanic['survived']# 훈련데이터, 테스트 데이터 분리X_train, X_test, y_train, y_test = train_test_split(X, y, stratify = y, test_size = 0.3, random_state=42)X_train.shape, X_test.shape, y_train.shape, y_test.shape ((623, 3), (268, 3), (623,), (268,)) 12345tree_model = DecisionTreeClassifier()tree_model.fit(X_train, y_train)acc = tree_model.score(X_test, y_test)print(f'모형 정확도 : {acc:.3f}') # 정확도 측정 모형 정확도 : 0.675 랜덤 포레스12345678910111213141516171819from sklearn.ensemble import RandomForestClassifierfrom sklearn.model_selection import train_test_split import seaborn as sns # tips 데이터셋 titanic = sns.load_dataset('titanic')X = titanic[['pclass', 'parch', 'fare']]y = titanic['survived']# 훈련데이터, 테스트 데이터 분리X_train, X_test, y_train, y_test = train_test_split(X, y, stratify = y, test_size = 0.3, random_state=42)# 모델 훈련rf_model = RandomForestClassifier(random_state=42) # 랜덤 포레스트 정의rf_model.fit(X_train, y_train)acc = rf_model.score(X_test, y_test)print(f'모형 정확도 : {acc:.3f}') # 정확도 측정 모형 정확도 : 0.675 XGBoost(2016) &amp; LightGBM 전통적인 머신러닝 알고리즘의 융합 선형회귀 릿지 라쏘, 과적합 방지를 위한 규제 결정트리의 핵심적인 알고리즘 경사 하강법 부스팅 기법 문제점 : 파라미터의 개수가 매우 많음 왜 많이 사용할까? 모델 학습 속도 성능 가장 좋은 모델이란 학습속도는 빠르고 성능이 좋음.(기준: 지금까지 나온 알고리즘과 비교해서) 언어를 Python,Java에서 시작했어도 C,C++로 가야만 됨. 개발 초기: 자체 사용 용도로 개발 –&gt; Python Wrapper R, 머신러닝 프레임워크 종류가 다양. 파이썬 머신러닝 중 Scikit-Learn이 대세로 떠오름 개발 중기: 파이썬 머신러닝 Scikit-Learn에서 API를 사용해 XGBoost을 사용 XGBoost-Python Wrapper 방식 X_train, Y_train 각 모듈에 맞도록 행렬을 재변환해야 함. 123import xgboost as xgb # 엑스지부스터를 사용-&gt;파이썬래퍼 방식from sklearn.model_selection import train_test_splitimport seaborn as sns 12345678910111213141516# 데이터 분리titanic = sns.load_dataset('titanic')titanic.info()#x,독립변수 y종속변수X = titanic[['pclass', 'parch', 'fare']]y = titanic['survived']# 훈련데이터, 테스트 데이터 분리X_train, X_test, y_train, y_test = train_test_split(X, y, stratify = y, test_size = 0.3, random_state=42)X_train.shape, X_test.shape, y_train.shape, y_test.shape &lt;class 'pandas.core.frame.DataFrame'&gt; RangeIndex: 891 entries, 0 to 890 Data columns (total 15 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 survived 891 non-null int64 1 pclass 891 non-null int64 2 sex 891 non-null object 3 age 714 non-null float64 4 sibsp 891 non-null int64 5 parch 891 non-null int64 6 fare 891 non-null float64 7 embarked 889 non-null object 8 class 891 non-null category 9 who 891 non-null object 10 adult_male 891 non-null bool 11 deck 203 non-null category 12 embark_town 889 non-null object 13 alive 891 non-null object 14 alone 891 non-null bool dtypes: bool(2), category(2), float64(2), int64(4), object(5) memory usage: 80.7+ KB ((623, 3), (268, 3), (623,), (268,)) 여기가 핵심 다른 데이터로 변신 1234dtrain = xgb.DMatrix(data = X_train, label = y_train)dtest = xgb.DMatrix(data= X_test, label= y_test)print(dtrain) &lt;xgboost.core.DMatrix object at 0x7faf7bee2490&gt; 1234567891011121314params = { 'max_depth':3,#트리 깊이는 3 'n_estimator':100,#100번 심기-&gt;결정트리갯수 'eta':0.1, 'objectice' : 'binary:logistic'}num_rounds = 400w_list = [(dtrain, 'train'), (dtest, 'test')]xgb_ml = xgb.train(params = params, dtrain = dtrain, num_boost_round = 400, #경사하강법의 에포크(가중치를 계속 줌) early_stopping_rounds = 100, # 100번하고 효과 없으면 멈춰라 evals = w_list) [0] train-rmse:0.487916 test-rmse:0.490147 Multiple eval metrics have been passed: 'test-rmse' will be used for early stopping. Will train until test-rmse hasn't improved in 100 rounds. [1] train-rmse:0.477856 test-rmse:0.482183 [2] train-rmse:0.470061 test-rmse:0.47532 [3] train-rmse:0.462181 test-rmse:0.470924 [4] train-rmse:0.455721 test-rmse:0.466351 [5] train-rmse:0.450355 test-rmse:0.462927 [6] train-rmse:0.445684 test-rmse:0.459554 [7] train-rmse:0.441298 test-rmse:0.458577 [8] train-rmse:0.437792 test-rmse:0.456831 [9] train-rmse:0.434393 test-rmse:0.456526 [10] train-rmse:0.431725 test-rmse:0.455362 [11] train-rmse:0.428592 test-rmse:0.454746 [12] train-rmse:0.426484 test-rmse:0.454121 [13] train-rmse:0.423993 test-rmse:0.453891 [14] train-rmse:0.421947 test-rmse:0.453115 [15] train-rmse:0.42036 test-rmse:0.452995 [16] train-rmse:0.418511 test-rmse:0.452991 [17] train-rmse:0.417025 test-rmse:0.452558 [18] train-rmse:0.415955 test-rmse:0.45293 [19] train-rmse:0.41396 test-rmse:0.453367 [20] train-rmse:0.413199 test-rmse:0.453852 [21] train-rmse:0.412091 test-rmse:0.453547 [22] train-rmse:0.410501 test-rmse:0.454099 [23] train-rmse:0.409461 test-rmse:0.453524 [24] train-rmse:0.408469 test-rmse:0.453724 [25] train-rmse:0.407781 test-rmse:0.454117 [26] train-rmse:0.406954 test-rmse:0.454325 [27] train-rmse:0.405709 test-rmse:0.454997 [28] train-rmse:0.405121 test-rmse:0.455544 [29] train-rmse:0.40445 test-rmse:0.455746 [30] train-rmse:0.403643 test-rmse:0.45576 [31] train-rmse:0.403092 test-rmse:0.45603 [32] train-rmse:0.40252 test-rmse:0.456502 [33] train-rmse:0.401617 test-rmse:0.456903 [34] train-rmse:0.401175 test-rmse:0.457341 [35] train-rmse:0.400151 test-rmse:0.458455 [36] train-rmse:0.399748 test-rmse:0.458725 [37] train-rmse:0.398984 test-rmse:0.45933 [38] train-rmse:0.3982 test-rmse:0.459086 [39] train-rmse:0.397529 test-rmse:0.459736 [40] train-rmse:0.39734 test-rmse:0.460037 [41] train-rmse:0.396627 test-rmse:0.460473 [42] train-rmse:0.396461 test-rmse:0.460603 [43] train-rmse:0.395536 test-rmse:0.460408 [44] train-rmse:0.395255 test-rmse:0.460791 [45] train-rmse:0.394568 test-rmse:0.461165 [46] train-rmse:0.39406 test-rmse:0.461553 [47] train-rmse:0.39335 test-rmse:0.461595 [48] train-rmse:0.393206 test-rmse:0.461718 [49] train-rmse:0.392991 test-rmse:0.462002 [50] train-rmse:0.392352 test-rmse:0.461921 [51] train-rmse:0.391973 test-rmse:0.462235 [52] train-rmse:0.391844 test-rmse:0.462413 [53] train-rmse:0.391345 test-rmse:0.462504 [54] train-rmse:0.391184 test-rmse:0.462824 [55] train-rmse:0.391068 test-rmse:0.462939 [56] train-rmse:0.390596 test-rmse:0.462162 [57] train-rmse:0.390164 test-rmse:0.462743 [58] train-rmse:0.389861 test-rmse:0.463045 [59] train-rmse:0.389441 test-rmse:0.462628 [60] train-rmse:0.389338 test-rmse:0.462737 [61] train-rmse:0.388745 test-rmse:0.462943 [62] train-rmse:0.388405 test-rmse:0.462691 [63] train-rmse:0.388277 test-rmse:0.463041 [64] train-rmse:0.387828 test-rmse:0.463243 [65] train-rmse:0.387614 test-rmse:0.463214 [66] train-rmse:0.387088 test-rmse:0.463584 [67] train-rmse:0.386906 test-rmse:0.463627 [68] train-rmse:0.386444 test-rmse:0.463517 [69] train-rmse:0.385685 test-rmse:0.463735 [70] train-rmse:0.385353 test-rmse:0.463113 [71] train-rmse:0.384828 test-rmse:0.463005 [72] train-rmse:0.38444 test-rmse:0.462966 [73] train-rmse:0.383198 test-rmse:0.462825 [74] train-rmse:0.382841 test-rmse:0.462947 [75] train-rmse:0.382416 test-rmse:0.463253 [76] train-rmse:0.381966 test-rmse:0.462963 [77] train-rmse:0.381655 test-rmse:0.463399 [78] train-rmse:0.381399 test-rmse:0.463326 [79] train-rmse:0.380593 test-rmse:0.463278 [80] train-rmse:0.380329 test-rmse:0.463051 [81] train-rmse:0.380233 test-rmse:0.46316 [82] train-rmse:0.379942 test-rmse:0.463234 [83] train-rmse:0.379686 test-rmse:0.463517 [84] train-rmse:0.37893 test-rmse:0.463051 [85] train-rmse:0.37884 test-rmse:0.463287 [86] train-rmse:0.378756 test-rmse:0.463389 [87] train-rmse:0.378501 test-rmse:0.463446 [88] train-rmse:0.378011 test-rmse:0.463085 [89] train-rmse:0.377178 test-rmse:0.46281 [90] train-rmse:0.376872 test-rmse:0.462851 [91] train-rmse:0.376563 test-rmse:0.46331 [92] train-rmse:0.376317 test-rmse:0.463088 [93] train-rmse:0.376049 test-rmse:0.463515 [94] train-rmse:0.375914 test-rmse:0.463486 [95] train-rmse:0.375643 test-rmse:0.463194 [96] train-rmse:0.375378 test-rmse:0.463459 [97] train-rmse:0.375145 test-rmse:0.46375 [98] train-rmse:0.37453 test-rmse:0.463309 [99] train-rmse:0.374021 test-rmse:0.463431 [100] train-rmse:0.373289 test-rmse:0.463593 [101] train-rmse:0.373032 test-rmse:0.463798 [102] train-rmse:0.372814 test-rmse:0.464226 [103] train-rmse:0.372443 test-rmse:0.464454 [104] train-rmse:0.372217 test-rmse:0.464417 [105] train-rmse:0.372146 test-rmse:0.464435 [106] train-rmse:0.371726 test-rmse:0.464241 [107] train-rmse:0.37158 test-rmse:0.464172 [108] train-rmse:0.371396 test-rmse:0.464582 [109] train-rmse:0.371335 test-rmse:0.4646 [110] train-rmse:0.371127 test-rmse:0.464403 [111] train-rmse:0.371001 test-rmse:0.464328 [112] train-rmse:0.370928 test-rmse:0.464546 [113] train-rmse:0.370688 test-rmse:0.464457 [114] train-rmse:0.370634 test-rmse:0.464475 [115] train-rmse:0.370421 test-rmse:0.464819 [116] train-rmse:0.37007 test-rmse:0.46479 [117] train-rmse:0.369922 test-rmse:0.464798 Stopping. Best iteration: [17] train-rmse:0.417025 test-rmse:0.452558 1234567#평가from sklearn.metrics import accuracy_scorepred_probs = xgb_ml.predict(dtest)y_pred = [1 if x &gt; 0.5 else 0 for x in pred_probs]# 예측 라벨과 실제 라벨 사이의 정확도 측정accuracy_score(y_pred,y_test) 0.6977611940298507 XGBoost Scikit-Learn API방식 application programming interface 123456789101112131415161718192021222324252627282930#xgb를 사용하지 않음(파이썬라이브러리를 가져오지 않고 사이킥런인공지능 사용)#from sklearn.tree import DecisionTreeClassifierfrom xgboost import XGBClassifier # 사이킥런 API#dt = DecisionTreeClassifier()params = { 'max_depth':3,#트리 깊이는 3 'n_estimator':100,#100번 심기-&gt;결정트리갯수 'eta':0.1, 'objectice' : 'binary:logistic'}num_rounds = 400xgb_model = XGBClassifier(objective= 'binary:logistic', n_estimators=100, max_depth=3, learning_rate =0.1, num_rounds = 400,#이건 엑스지부스터랑 같은내용이지만 양식이 다름 random_state = 42)w_list = [(X_train, y_train), (X_test, y_test)]xgb_model.fit(X_train, y_train, eval_set = w_list, eval_metric='error', verbose=True)y_probas = xgb_model.predict_proba(X_test)y_pred = [1 if x &gt; 0.5 else 0 for x in pred_probs]# 예측 라벨과 실제 라벨 사이의 정확도 측정accuracy_score(y_pred, y_test) [0] validation_0-error:0.260032 validation_1-error:0.302239 [1] validation_0-error:0.260032 validation_1-error:0.302239 [2] validation_0-error:0.260032 validation_1-error:0.302239 [3] validation_0-error:0.260032 validation_1-error:0.302239 [4] validation_0-error:0.260032 validation_1-error:0.302239 [5] validation_0-error:0.260032 validation_1-error:0.302239 [6] validation_0-error:0.260032 validation_1-error:0.302239 [7] validation_0-error:0.260032 validation_1-error:0.302239 [8] validation_0-error:0.260032 validation_1-error:0.302239 [9] validation_0-error:0.260032 validation_1-error:0.302239 [10] validation_0-error:0.260032 validation_1-error:0.302239 [11] validation_0-error:0.260032 validation_1-error:0.302239 [12] validation_0-error:0.260032 validation_1-error:0.302239 [13] validation_0-error:0.247191 validation_1-error:0.298507 [14] validation_0-error:0.247191 validation_1-error:0.298507 [15] validation_0-error:0.248796 validation_1-error:0.302239 [16] validation_0-error:0.248796 validation_1-error:0.302239 [17] validation_0-error:0.248796 validation_1-error:0.302239 [18] validation_0-error:0.248796 validation_1-error:0.302239 [19] validation_0-error:0.248796 validation_1-error:0.302239 [20] validation_0-error:0.248796 validation_1-error:0.302239 [21] validation_0-error:0.248796 validation_1-error:0.302239 [22] validation_0-error:0.248796 validation_1-error:0.302239 [23] validation_0-error:0.248796 validation_1-error:0.302239 [24] validation_0-error:0.248796 validation_1-error:0.302239 [25] validation_0-error:0.248796 validation_1-error:0.302239 [26] validation_0-error:0.248796 validation_1-error:0.302239 [27] validation_0-error:0.248796 validation_1-error:0.302239 [28] validation_0-error:0.247191 validation_1-error:0.302239 [29] validation_0-error:0.247191 validation_1-error:0.302239 [30] validation_0-error:0.247191 validation_1-error:0.302239 [31] validation_0-error:0.243981 validation_1-error:0.298507 [32] validation_0-error:0.247191 validation_1-error:0.302239 [33] validation_0-error:0.243981 validation_1-error:0.298507 [34] validation_0-error:0.243981 validation_1-error:0.298507 [35] validation_0-error:0.242376 validation_1-error:0.294776 [36] validation_0-error:0.24077 validation_1-error:0.294776 [37] validation_0-error:0.24077 validation_1-error:0.294776 [38] validation_0-error:0.24077 validation_1-error:0.294776 [39] validation_0-error:0.24077 validation_1-error:0.294776 [40] validation_0-error:0.24077 validation_1-error:0.294776 [41] validation_0-error:0.24077 validation_1-error:0.294776 [42] validation_0-error:0.24077 validation_1-error:0.294776 [43] validation_0-error:0.24077 validation_1-error:0.294776 [44] validation_0-error:0.24077 validation_1-error:0.302239 [45] validation_0-error:0.24077 validation_1-error:0.302239 [46] validation_0-error:0.24077 validation_1-error:0.302239 [47] validation_0-error:0.24077 validation_1-error:0.302239 [48] validation_0-error:0.24077 validation_1-error:0.302239 [49] validation_0-error:0.24077 validation_1-error:0.302239 [50] validation_0-error:0.24077 validation_1-error:0.302239 [51] validation_0-error:0.24077 validation_1-error:0.302239 [52] validation_0-error:0.23435 validation_1-error:0.302239 [53] validation_0-error:0.23435 validation_1-error:0.302239 [54] validation_0-error:0.232745 validation_1-error:0.298507 [55] validation_0-error:0.229535 validation_1-error:0.298507 [56] validation_0-error:0.229535 validation_1-error:0.298507 [57] validation_0-error:0.229535 validation_1-error:0.298507 [58] validation_0-error:0.229535 validation_1-error:0.298507 [59] validation_0-error:0.227929 validation_1-error:0.294776 [60] validation_0-error:0.227929 validation_1-error:0.298507 [61] validation_0-error:0.227929 validation_1-error:0.298507 [62] validation_0-error:0.227929 validation_1-error:0.298507 [63] validation_0-error:0.227929 validation_1-error:0.298507 [64] validation_0-error:0.227929 validation_1-error:0.298507 [65] validation_0-error:0.227929 validation_1-error:0.298507 [66] validation_0-error:0.227929 validation_1-error:0.298507 [67] validation_0-error:0.227929 validation_1-error:0.298507 [68] validation_0-error:0.227929 validation_1-error:0.298507 [69] validation_0-error:0.227929 validation_1-error:0.298507 [70] validation_0-error:0.227929 validation_1-error:0.298507 [71] validation_0-error:0.227929 validation_1-error:0.298507 [72] validation_0-error:0.227929 validation_1-error:0.302239 [73] validation_0-error:0.227929 validation_1-error:0.302239 [74] validation_0-error:0.229535 validation_1-error:0.30597 [75] validation_0-error:0.229535 validation_1-error:0.30597 [76] validation_0-error:0.229535 validation_1-error:0.30597 [77] validation_0-error:0.229535 validation_1-error:0.30597 [78] validation_0-error:0.229535 validation_1-error:0.30597 [79] validation_0-error:0.229535 validation_1-error:0.30597 [80] validation_0-error:0.229535 validation_1-error:0.30597 [81] validation_0-error:0.229535 validation_1-error:0.30597 [82] validation_0-error:0.229535 validation_1-error:0.30597 [83] validation_0-error:0.229535 validation_1-error:0.30597 [84] validation_0-error:0.229535 validation_1-error:0.30597 [85] validation_0-error:0.229535 validation_1-error:0.30597 [86] validation_0-error:0.229535 validation_1-error:0.30597 [87] validation_0-error:0.229535 validation_1-error:0.30597 [88] validation_0-error:0.229535 validation_1-error:0.30597 [89] validation_0-error:0.229535 validation_1-error:0.30597 [90] validation_0-error:0.229535 validation_1-error:0.30597 [91] validation_0-error:0.229535 validation_1-error:0.30597 [92] validation_0-error:0.229535 validation_1-error:0.30597 [93] validation_0-error:0.229535 validation_1-error:0.30597 [94] validation_0-error:0.227929 validation_1-error:0.313433 [95] validation_0-error:0.226324 validation_1-error:0.313433 [96] validation_0-error:0.223114 validation_1-error:0.317164 [97] validation_0-error:0.223114 validation_1-error:0.317164 [98] validation_0-error:0.223114 validation_1-error:0.317164 [99] validation_0-error:0.223114 validation_1-error:0.317164 0.6977611940298507 LightGBM Rython Wrapper방식1234567891011121314151617181920212223242526272829303132333435import lightgbm as lgb from sklearn.model_selection import train_test_split from sklearn.metrics import accuracy_scoreimport seaborn as sns # tips 데이터셋 titanic = sns.load_dataset('titanic')X = titanic[['pclass', 'parch', 'fare']]y = titanic['survived']# 훈련데이터, 테스트 데이터 분리X_train, X_test, y_train, y_test = train_test_split(X, y, stratify = y, test_size = 0.3, random_state=42)# XGBoost 코드와 유사하다. dtrain = lgb.Dataset(data = X_train, label = y_train)dtest = lgb.Dataset(data = X_test, label = y_test)params = {'max_depth':3, 'n_estimators':100, 'learning_rate': 0.1, #xgbooost eta 'objective':'binary',# xgboost objectice' : 'binary:logistic' 'metric' : 'binary_error', 'num_boost_round' : 400, 'verbose' : 1} w_list = [dtrain, dtest]lgb_ml = lgb.train(params=params, train_set = dtrain,\\ early_stopping_rounds=100, valid_sets= w_list)pred_probs = lgb_ml.predict(X_test)y_pred=[1 if x &gt; 0.5 else 0 for x in pred_probs]# 예측 라벨과 실제 라벨 사이의 정확도 측정accuracy_score(y_pred, y_test) /usr/local/lib/python3.7/dist-packages/lightgbm/engine.py:118: UserWarning: Found `num_boost_round` in params. Will use it instead of argument warnings.warn(&quot;Found `{}` in params. Will use it instead of argument&quot;.format(alias)) [1] training's binary_error: 0.383628 valid_1's binary_error: 0.384328 Training until validation scores don't improve for 100 rounds. [2] training's binary_error: 0.383628 valid_1's binary_error: 0.384328 [3] training's binary_error: 0.354735 valid_1's binary_error: 0.369403 [4] training's binary_error: 0.29695 valid_1's binary_error: 0.354478 [5] training's binary_error: 0.272873 valid_1's binary_error: 0.33209 [6] training's binary_error: 0.272873 valid_1's binary_error: 0.33209 [7] training's binary_error: 0.269663 valid_1's binary_error: 0.317164 [8] training's binary_error: 0.269663 valid_1's binary_error: 0.317164 [9] training's binary_error: 0.264848 valid_1's binary_error: 0.309701 [10] training's binary_error: 0.269663 valid_1's binary_error: 0.309701 [11] training's binary_error: 0.264848 valid_1's binary_error: 0.309701 [12] training's binary_error: 0.264848 valid_1's binary_error: 0.309701 [13] training's binary_error: 0.264848 valid_1's binary_error: 0.309701 [14] training's binary_error: 0.264848 valid_1's binary_error: 0.309701 [15] training's binary_error: 0.264848 valid_1's binary_error: 0.309701 [16] training's binary_error: 0.266453 valid_1's binary_error: 0.313433 [17] training's binary_error: 0.266453 valid_1's binary_error: 0.313433 [18] training's binary_error: 0.266453 valid_1's binary_error: 0.313433 [19] training's binary_error: 0.266453 valid_1's binary_error: 0.313433 [20] training's binary_error: 0.266453 valid_1's binary_error: 0.313433 [21] training's binary_error: 0.266453 valid_1's binary_error: 0.313433 [22] training's binary_error: 0.266453 valid_1's binary_error: 0.313433 [23] training's binary_error: 0.271268 valid_1's binary_error: 0.313433 [24] training's binary_error: 0.258427 valid_1's binary_error: 0.309701 [25] training's binary_error: 0.258427 valid_1's binary_error: 0.309701 [26] training's binary_error: 0.258427 valid_1's binary_error: 0.309701 [27] training's binary_error: 0.258427 valid_1's binary_error: 0.309701 [28] training's binary_error: 0.258427 valid_1's binary_error: 0.309701 [29] training's binary_error: 0.255217 valid_1's binary_error: 0.309701 [30] training's binary_error: 0.255217 valid_1's binary_error: 0.309701 [31] training's binary_error: 0.255217 valid_1's binary_error: 0.309701 [32] training's binary_error: 0.255217 valid_1's binary_error: 0.309701 [33] training's binary_error: 0.255217 valid_1's binary_error: 0.317164 [34] training's binary_error: 0.255217 valid_1's binary_error: 0.317164 [35] training's binary_error: 0.255217 valid_1's binary_error: 0.317164 [36] training's binary_error: 0.255217 valid_1's binary_error: 0.309701 [37] training's binary_error: 0.255217 valid_1's binary_error: 0.317164 [38] training's binary_error: 0.255217 valid_1's binary_error: 0.317164 [39] training's binary_error: 0.248796 valid_1's binary_error: 0.309701 [40] training's binary_error: 0.248796 valid_1's binary_error: 0.313433 [41] training's binary_error: 0.248796 valid_1's binary_error: 0.313433 [42] training's binary_error: 0.248796 valid_1's binary_error: 0.313433 [43] training's binary_error: 0.248796 valid_1's binary_error: 0.313433 [44] training's binary_error: 0.248796 valid_1's binary_error: 0.313433 [45] training's binary_error: 0.247191 valid_1's binary_error: 0.313433 [46] training's binary_error: 0.247191 valid_1's binary_error: 0.313433 [47] training's binary_error: 0.247191 valid_1's binary_error: 0.313433 [48] training's binary_error: 0.247191 valid_1's binary_error: 0.313433 [49] training's binary_error: 0.247191 valid_1's binary_error: 0.313433 [50] training's binary_error: 0.247191 valid_1's binary_error: 0.313433 [51] training's binary_error: 0.247191 valid_1's binary_error: 0.313433 [52] training's binary_error: 0.247191 valid_1's binary_error: 0.313433 [53] training's binary_error: 0.247191 valid_1's binary_error: 0.313433 [54] training's binary_error: 0.247191 valid_1's binary_error: 0.313433 [55] training's binary_error: 0.247191 valid_1's binary_error: 0.313433 [56] training's binary_error: 0.247191 valid_1's binary_error: 0.313433 [57] training's binary_error: 0.247191 valid_1's binary_error: 0.313433 [58] training's binary_error: 0.247191 valid_1's binary_error: 0.313433 [59] training's binary_error: 0.247191 valid_1's binary_error: 0.313433 [60] training's binary_error: 0.247191 valid_1's binary_error: 0.313433 [61] training's binary_error: 0.247191 valid_1's binary_error: 0.313433 [62] training's binary_error: 0.247191 valid_1's binary_error: 0.313433 [63] training's binary_error: 0.247191 valid_1's binary_error: 0.313433 [64] training's binary_error: 0.247191 valid_1's binary_error: 0.313433 [65] training's binary_error: 0.247191 valid_1's binary_error: 0.313433 [66] training's binary_error: 0.243981 valid_1's binary_error: 0.309701 [67] training's binary_error: 0.23435 valid_1's binary_error: 0.309701 [68] training's binary_error: 0.23435 valid_1's binary_error: 0.309701 [69] training's binary_error: 0.23435 valid_1's binary_error: 0.309701 [70] training's binary_error: 0.229535 valid_1's binary_error: 0.309701 [71] training's binary_error: 0.229535 valid_1's binary_error: 0.309701 [72] training's binary_error: 0.229535 valid_1's binary_error: 0.309701 [73] training's binary_error: 0.229535 valid_1's binary_error: 0.309701 [74] training's binary_error: 0.229535 valid_1's binary_error: 0.309701 [75] training's binary_error: 0.229535 valid_1's binary_error: 0.309701 [76] training's binary_error: 0.229535 valid_1's binary_error: 0.309701 [77] training's binary_error: 0.229535 valid_1's binary_error: 0.309701 [78] training's binary_error: 0.232745 valid_1's binary_error: 0.313433 [79] training's binary_error: 0.232745 valid_1's binary_error: 0.313433 [80] training's binary_error: 0.232745 valid_1's binary_error: 0.313433 [81] training's binary_error: 0.229535 valid_1's binary_error: 0.309701 [82] training's binary_error: 0.229535 valid_1's binary_error: 0.309701 [83] training's binary_error: 0.229535 valid_1's binary_error: 0.309701 [84] training's binary_error: 0.229535 valid_1's binary_error: 0.309701 [85] training's binary_error: 0.229535 valid_1's binary_error: 0.309701 [86] training's binary_error: 0.229535 valid_1's binary_error: 0.309701 [87] training's binary_error: 0.229535 valid_1's binary_error: 0.309701 [88] training's binary_error: 0.229535 valid_1's binary_error: 0.309701 [89] training's binary_error: 0.229535 valid_1's binary_error: 0.309701 [90] training's binary_error: 0.229535 valid_1's binary_error: 0.309701 [91] training's binary_error: 0.229535 valid_1's binary_error: 0.309701 [92] training's binary_error: 0.229535 valid_1's binary_error: 0.309701 [93] training's binary_error: 0.227929 valid_1's binary_error: 0.309701 [94] training's binary_error: 0.227929 valid_1's binary_error: 0.309701 [95] training's binary_error: 0.227929 valid_1's binary_error: 0.309701 [96] training's binary_error: 0.227929 valid_1's binary_error: 0.309701 [97] training's binary_error: 0.227929 valid_1's binary_error: 0.309701 [98] training's binary_error: 0.227929 valid_1's binary_error: 0.309701 [99] training's binary_error: 0.221509 valid_1's binary_error: 0.317164 [100] training's binary_error: 0.227929 valid_1's binary_error: 0.309701 [101] training's binary_error: 0.23114 valid_1's binary_error: 0.30597 [102] training's binary_error: 0.23114 valid_1's binary_error: 0.30597 [103] training's binary_error: 0.227929 valid_1's binary_error: 0.309701 [104] training's binary_error: 0.221509 valid_1's binary_error: 0.317164 [105] training's binary_error: 0.221509 valid_1's binary_error: 0.317164 [106] training's binary_error: 0.224719 valid_1's binary_error: 0.313433 [107] training's binary_error: 0.224719 valid_1's binary_error: 0.317164 [108] training's binary_error: 0.224719 valid_1's binary_error: 0.317164 [109] training's binary_error: 0.224719 valid_1's binary_error: 0.317164 [110] training's binary_error: 0.224719 valid_1's binary_error: 0.317164 [111] training's binary_error: 0.223114 valid_1's binary_error: 0.313433 [112] training's binary_error: 0.223114 valid_1's binary_error: 0.313433 [113] training's binary_error: 0.223114 valid_1's binary_error: 0.313433 [114] training's binary_error: 0.223114 valid_1's binary_error: 0.313433 [115] training's binary_error: 0.223114 valid_1's binary_error: 0.313433 [116] training's binary_error: 0.223114 valid_1's binary_error: 0.313433 [117] training's binary_error: 0.223114 valid_1's binary_error: 0.313433 [118] training's binary_error: 0.223114 valid_1's binary_error: 0.313433 [119] training's binary_error: 0.223114 valid_1's binary_error: 0.313433 [120] training's binary_error: 0.223114 valid_1's binary_error: 0.309701 [121] training's binary_error: 0.223114 valid_1's binary_error: 0.309701 [122] training's binary_error: 0.223114 valid_1's binary_error: 0.309701 [123] training's binary_error: 0.223114 valid_1's binary_error: 0.309701 [124] training's binary_error: 0.223114 valid_1's binary_error: 0.309701 [125] training's binary_error: 0.223114 valid_1's binary_error: 0.309701 [126] training's binary_error: 0.223114 valid_1's binary_error: 0.309701 [127] training's binary_error: 0.223114 valid_1's binary_error: 0.309701 [128] training's binary_error: 0.223114 valid_1's binary_error: 0.309701 [129] training's binary_error: 0.223114 valid_1's binary_error: 0.309701 [130] training's binary_error: 0.223114 valid_1's binary_error: 0.309701 [131] training's binary_error: 0.223114 valid_1's binary_error: 0.309701 [132] training's binary_error: 0.223114 valid_1's binary_error: 0.309701 [133] training's binary_error: 0.223114 valid_1's binary_error: 0.309701 [134] training's binary_error: 0.223114 valid_1's binary_error: 0.309701 [135] training's binary_error: 0.223114 valid_1's binary_error: 0.309701 [136] training's binary_error: 0.223114 valid_1's binary_error: 0.309701 [137] training's binary_error: 0.219904 valid_1's binary_error: 0.309701 [138] training's binary_error: 0.219904 valid_1's binary_error: 0.309701 [139] training's binary_error: 0.223114 valid_1's binary_error: 0.309701 [140] training's binary_error: 0.223114 valid_1's binary_error: 0.309701 [141] training's binary_error: 0.223114 valid_1's binary_error: 0.309701 [142] training's binary_error: 0.223114 valid_1's binary_error: 0.309701 [143] training's binary_error: 0.223114 valid_1's binary_error: 0.309701 [144] training's binary_error: 0.221509 valid_1's binary_error: 0.320896 [145] training's binary_error: 0.223114 valid_1's binary_error: 0.313433 [146] training's binary_error: 0.221509 valid_1's binary_error: 0.313433 [147] training's binary_error: 0.221509 valid_1's binary_error: 0.313433 [148] training's binary_error: 0.221509 valid_1's binary_error: 0.313433 [149] training's binary_error: 0.221509 valid_1's binary_error: 0.313433 [150] training's binary_error: 0.219904 valid_1's binary_error: 0.324627 [151] training's binary_error: 0.219904 valid_1's binary_error: 0.324627 [152] training's binary_error: 0.221509 valid_1's binary_error: 0.313433 [153] training's binary_error: 0.219904 valid_1's binary_error: 0.324627 [154] training's binary_error: 0.219904 valid_1's binary_error: 0.324627 [155] training's binary_error: 0.219904 valid_1's binary_error: 0.324627 [156] training's binary_error: 0.219904 valid_1's binary_error: 0.324627 [157] training's binary_error: 0.219904 valid_1's binary_error: 0.324627 [158] training's binary_error: 0.219904 valid_1's binary_error: 0.324627 [159] training's binary_error: 0.219904 valid_1's binary_error: 0.324627 [160] training's binary_error: 0.219904 valid_1's binary_error: 0.324627 [161] training's binary_error: 0.219904 valid_1's binary_error: 0.324627 [162] training's binary_error: 0.219904 valid_1's binary_error: 0.324627 [163] training's binary_error: 0.219904 valid_1's binary_error: 0.324627 [164] training's binary_error: 0.219904 valid_1's binary_error: 0.324627 [165] training's binary_error: 0.219904 valid_1's binary_error: 0.324627 [166] training's binary_error: 0.219904 valid_1's binary_error: 0.324627 [167] training's binary_error: 0.219904 valid_1's binary_error: 0.324627 [168] training's binary_error: 0.219904 valid_1's binary_error: 0.324627 [169] training's binary_error: 0.219904 valid_1's binary_error: 0.324627 [170] training's binary_error: 0.215088 valid_1's binary_error: 0.317164 [171] training's binary_error: 0.215088 valid_1's binary_error: 0.317164 [172] training's binary_error: 0.215088 valid_1's binary_error: 0.317164 [173] training's binary_error: 0.215088 valid_1's binary_error: 0.317164 [174] training's binary_error: 0.215088 valid_1's binary_error: 0.317164 [175] training's binary_error: 0.216693 valid_1's binary_error: 0.320896 [176] training's binary_error: 0.216693 valid_1's binary_error: 0.320896 [177] training's binary_error: 0.221509 valid_1's binary_error: 0.328358 [178] training's binary_error: 0.221509 valid_1's binary_error: 0.328358 [179] training's binary_error: 0.221509 valid_1's binary_error: 0.328358 [180] training's binary_error: 0.221509 valid_1's binary_error: 0.328358 [181] training's binary_error: 0.221509 valid_1's binary_error: 0.328358 [182] training's binary_error: 0.216693 valid_1's binary_error: 0.320896 [183] training's binary_error: 0.216693 valid_1's binary_error: 0.320896 [184] training's binary_error: 0.216693 valid_1's binary_error: 0.320896 [185] training's binary_error: 0.216693 valid_1's binary_error: 0.320896 [186] training's binary_error: 0.216693 valid_1's binary_error: 0.320896 [187] training's binary_error: 0.216693 valid_1's binary_error: 0.320896 [188] training's binary_error: 0.216693 valid_1's binary_error: 0.320896 [189] training's binary_error: 0.216693 valid_1's binary_error: 0.320896 [190] training's binary_error: 0.216693 valid_1's binary_error: 0.320896 [191] training's binary_error: 0.216693 valid_1's binary_error: 0.320896 [192] training's binary_error: 0.216693 valid_1's binary_error: 0.320896 [193] training's binary_error: 0.216693 valid_1's binary_error: 0.320896 [194] training's binary_error: 0.216693 valid_1's binary_error: 0.320896 [195] training's binary_error: 0.216693 valid_1's binary_error: 0.320896 [196] training's binary_error: 0.216693 valid_1's binary_error: 0.320896 [197] training's binary_error: 0.215088 valid_1's binary_error: 0.317164 [198] training's binary_error: 0.215088 valid_1's binary_error: 0.317164 [199] training's binary_error: 0.215088 valid_1's binary_error: 0.317164 [200] training's binary_error: 0.215088 valid_1's binary_error: 0.317164 [201] training's binary_error: 0.215088 valid_1's binary_error: 0.317164 Early stopping, best iteration is: [101] training's binary_error: 0.23114 valid_1's binary_error: 0.30597 0.6940298507462687 https://lightgbm.readthedocs.io/en/latest/Parameters.html 파라메터를 보고 할 것 LightGBM Scikit-Learn API방식 application programming interface 12345678910111213141516171819202122from lightgbm import LGBMClassifierfrom sklearn.metrics import accuracy_score# model w_list = [dtrain, dtest]model = LGBMClassifier(objective = 'binary', metric = 'binary_error', n_estimators=100, learning_rate=0.1, max_depth=3, num_boost_round = 400, random_state = 32)model.fit(X_train, y_train, eval_set = [(X_train, y_train), (X_test, y_test)], verbose=1, early_stopping_rounds = 100)y_probas = model.predict_proba(X_test) y_pred=[1 if x &gt; 0.5 else 0 for x in y_probas[:, 1]] # 예측 라벨(0과 1로 예측)# 예측 라벨과 실제 라벨 사이의 정확도 측정accuracy_score(y_pred, y_test) [1] training's binary_error: 0.383628 valid_1's binary_error: 0.384328 Training until validation scores don't improve for 100 rounds. [2] training's binary_error: 0.383628 valid_1's binary_error: 0.384328 [3] training's binary_error: 0.354735 valid_1's binary_error: 0.369403 [4] training's binary_error: 0.29695 valid_1's binary_error: 0.354478 [5] training's binary_error: 0.272873 valid_1's binary_error: 0.33209 [6] training's binary_error: 0.272873 valid_1's binary_error: 0.33209 [7] training's binary_error: 0.269663 valid_1's binary_error: 0.317164 [8] training's binary_error: 0.269663 valid_1's binary_error: 0.317164 [9] training's binary_error: 0.264848 valid_1's binary_error: 0.309701 [10] training's binary_error: 0.269663 valid_1's binary_error: 0.309701 [11] training's binary_error: 0.264848 valid_1's binary_error: 0.309701 [12] training's binary_error: 0.264848 valid_1's binary_error: 0.309701 [13] training's binary_error: 0.264848 valid_1's binary_error: 0.309701 [14] training's binary_error: 0.264848 valid_1's binary_error: 0.309701 [15] training's binary_error: 0.264848 valid_1's binary_error: 0.309701 [16] training's binary_error: 0.266453 valid_1's binary_error: 0.313433 [17] training's binary_error: 0.266453 valid_1's binary_error: 0.313433 [18] training's binary_error: 0.266453 valid_1's binary_error: 0.313433 [19] training's binary_error: 0.266453 valid_1's binary_error: 0.313433 [20] training's binary_error: 0.266453 valid_1's binary_error: 0.313433 [21] training's binary_error: 0.266453 valid_1's binary_error: 0.313433 [22] training's binary_error: 0.266453 valid_1's binary_error: 0.313433 [23] training's binary_error: 0.271268 valid_1's binary_error: 0.313433 [24] training's binary_error: 0.258427 valid_1's binary_error: 0.309701 [25] training's binary_error: 0.258427 valid_1's binary_error: 0.309701 [26] training's binary_error: 0.258427 valid_1's binary_error: 0.309701 [27] training's binary_error: 0.258427 valid_1's binary_error: 0.309701 [28] training's binary_error: 0.258427 valid_1's binary_error: 0.309701 [29] training's binary_error: 0.255217 valid_1's binary_error: 0.309701 [30] training's binary_error: 0.255217 valid_1's binary_error: 0.309701 [31] training's binary_error: 0.255217 valid_1's binary_error: 0.309701 [32] training's binary_error: 0.255217 valid_1's binary_error: 0.309701 [33] training's binary_error: 0.255217 valid_1's binary_error: 0.317164 [34] training's binary_error: 0.255217 valid_1's binary_error: 0.317164 [35] training's binary_error: 0.255217 valid_1's binary_error: 0.317164 [36] training's binary_error: 0.255217 valid_1's binary_error: 0.309701 [37] training's binary_error: 0.255217 valid_1's binary_error: 0.317164 [38] training's binary_error: 0.255217 valid_1's binary_error: 0.317164 [39] training's binary_error: 0.248796 valid_1's binary_error: 0.309701 [40] training's binary_error: 0.248796 valid_1's binary_error: 0.313433 [41] training's binary_error: 0.248796 valid_1's binary_error: 0.313433 [42] training's binary_error: 0.248796 valid_1's binary_error: 0.313433 [43] training's binary_error: 0.248796 valid_1's binary_error: 0.313433 [44] training's binary_error: 0.248796 valid_1's binary_error: 0.313433 [45] training's binary_error: 0.247191 valid_1's binary_error: 0.313433 [46] training's binary_error: 0.247191 valid_1's binary_error: 0.313433 [47] training's binary_error: 0.247191 valid_1's binary_error: 0.313433 [48] training's binary_error: 0.247191 valid_1's binary_error: 0.313433 [49] training's binary_error: 0.247191 valid_1's binary_error: 0.313433 [50] training's binary_error: 0.247191 valid_1's binary_error: 0.313433 [51] training's binary_error: 0.247191 valid_1's binary_error: 0.313433 [52] training's binary_error: 0.247191 valid_1's binary_error: 0.313433 [53] training's binary_error: 0.247191 valid_1's binary_error: 0.313433 [54] training's binary_error: 0.247191 valid_1's binary_error: 0.313433 [55] training's binary_error: 0.247191 valid_1's binary_error: 0.313433 [56] training's binary_error: 0.247191 valid_1's binary_error: 0.313433 [57] training's binary_error: 0.247191 valid_1's binary_error: 0.313433 [58] training's binary_error: 0.247191 valid_1's binary_error: 0.313433 [59] training's binary_error: 0.247191 valid_1's binary_error: 0.313433 [60] training's binary_error: 0.247191 valid_1's binary_error: 0.313433 [61] training's binary_error: 0.247191 valid_1's binary_error: 0.313433 [62] training's binary_error: 0.247191 valid_1's binary_error: 0.313433 [63] training's binary_error: 0.247191 valid_1's binary_error: 0.313433 [64] training's binary_error: 0.247191 valid_1's binary_error: 0.313433 [65] training's binary_error: 0.247191 valid_1's binary_error: 0.313433 [66] training's binary_error: 0.243981 valid_1's binary_error: 0.309701 [67] training's binary_error: 0.23435 valid_1's binary_error: 0.309701 [68] training's binary_error: 0.23435 valid_1's binary_error: 0.309701 [69] training's binary_error: 0.23435 valid_1's binary_error: 0.309701 [70] training's binary_error: 0.229535 valid_1's binary_error: 0.309701 [71] training's binary_error: 0.229535 valid_1's binary_error: 0.309701 [72] training's binary_error: 0.229535 valid_1's binary_error: 0.309701 [73] training's binary_error: 0.229535 valid_1's binary_error: 0.309701 [74] training's binary_error: 0.229535 valid_1's binary_error: 0.309701 [75] training's binary_error: 0.229535 valid_1's binary_error: 0.309701 [76] training's binary_error: 0.229535 valid_1's binary_error: 0.309701 [77] training's binary_error: 0.229535 valid_1's binary_error: 0.309701 [78] training's binary_error: 0.232745 valid_1's binary_error: 0.313433 [79] training's binary_error: 0.232745 valid_1's binary_error: 0.313433 [80] training's binary_error: 0.232745 valid_1's binary_error: 0.313433 [81] training's binary_error: 0.229535 valid_1's binary_error: 0.309701 [82] training's binary_error: 0.229535 valid_1's binary_error: 0.309701 [83] training's binary_error: 0.229535 valid_1's binary_error: 0.309701 [84] training's binary_error: 0.229535 valid_1's binary_error: 0.309701 [85] training's binary_error: 0.229535 valid_1's binary_error: 0.309701 [86] training's binary_error: 0.229535 valid_1's binary_error: 0.309701 [87] training's binary_error: 0.229535 valid_1's binary_error: 0.309701 [88] training's binary_error: 0.229535 valid_1's binary_error: 0.309701 [89] training's binary_error: 0.229535 valid_1's binary_error: 0.309701 [90] training's binary_error: 0.229535 valid_1's binary_error: 0.309701 [91] training's binary_error: 0.229535 valid_1's binary_error: 0.309701 [92] training's binary_error: 0.229535 valid_1's binary_error: 0.309701 [93] training's binary_error: 0.227929 valid_1's binary_error: 0.309701 [94] training's binary_error: 0.227929 valid_1's binary_error: 0.309701 [95] training's binary_error: 0.227929 valid_1's binary_error: 0.309701 [96] training's binary_error: 0.227929 valid_1's binary_error: 0.309701 [97] training's binary_error: 0.227929 valid_1's binary_error: 0.309701 [98] training's binary_error: 0.227929 valid_1's binary_error: 0.309701 [99] training's binary_error: 0.221509 valid_1's binary_error: 0.317164 [100] training's binary_error: 0.227929 valid_1's binary_error: 0.309701 [101] training's binary_error: 0.23114 valid_1's binary_error: 0.30597 [102] training's binary_error: 0.23114 valid_1's binary_error: 0.30597 [103] training's binary_error: 0.227929 valid_1's binary_error: 0.309701 [104] training's binary_error: 0.221509 valid_1's binary_error: 0.317164 [105] training's binary_error: 0.221509 valid_1's binary_error: 0.317164 [106] training's binary_error: 0.224719 valid_1's binary_error: 0.313433 [107] training's binary_error: 0.224719 valid_1's binary_error: 0.317164 [108] training's binary_error: 0.224719 valid_1's binary_error: 0.317164 [109] training's binary_error: 0.224719 valid_1's binary_error: 0.317164 [110] training's binary_error: 0.224719 valid_1's binary_error: 0.317164 [111] training's binary_error: 0.223114 valid_1's binary_error: 0.313433 [112] training's binary_error: 0.223114 valid_1's binary_error: 0.313433 [113] training's binary_error: 0.223114 valid_1's binary_error: 0.313433 [114] training's binary_error: 0.223114 valid_1's binary_error: 0.313433 [115] training's binary_error: 0.223114 valid_1's binary_error: 0.313433 [116] training's binary_error: 0.223114 valid_1's binary_error: 0.313433 [117] training's binary_error: 0.223114 valid_1's binary_error: 0.313433 [118] training's binary_error: 0.223114 valid_1's binary_error: 0.313433 [119] training's binary_error: 0.223114 valid_1's binary_error: 0.313433 [120] training's binary_error: 0.223114 valid_1's binary_error: 0.309701 [121] training's binary_error: 0.223114 valid_1's binary_error: 0.309701 [122] training's binary_error: 0.223114 valid_1's binary_error: 0.309701 [123] training's binary_error: 0.223114 valid_1's binary_error: 0.309701 [124] training's binary_error: 0.223114 valid_1's binary_error: 0.309701 [125] training's binary_error: 0.223114 valid_1's binary_error: 0.309701 [126] training's binary_error: 0.223114 valid_1's binary_error: 0.309701 [127] training's binary_error: 0.223114 valid_1's binary_error: 0.309701 [128] training's binary_error: 0.223114 valid_1's binary_error: 0.309701 [129] training's binary_error: 0.223114 valid_1's binary_error: 0.309701 [130] training's binary_error: 0.223114 valid_1's binary_error: 0.309701 [131] training's binary_error: 0.223114 valid_1's binary_error: 0.309701 [132] training's binary_error: 0.223114 valid_1's binary_error: 0.309701 [133] training's binary_error: 0.223114 valid_1's binary_error: 0.309701 [134] training's binary_error: 0.223114 valid_1's binary_error: 0.309701 [135] training's binary_error: 0.223114 valid_1's binary_error: 0.309701 [136] training's binary_error: 0.223114 valid_1's binary_error: 0.309701 [137] training's binary_error: 0.219904 valid_1's binary_error: 0.309701 [138] training's binary_error: 0.219904 valid_1's binary_error: 0.309701 [139] training's binary_error: 0.223114 valid_1's binary_error: 0.309701 [140] training's binary_error: 0.223114 valid_1's binary_error: 0.309701 [141] training's binary_error: 0.223114 valid_1's binary_error: 0.309701 [142] training's binary_error: 0.223114 valid_1's binary_error: 0.309701 [143] training's binary_error: 0.223114 valid_1's binary_error: 0.309701 [144] training's binary_error: 0.221509 valid_1's binary_error: 0.320896 [145] training's binary_error: 0.223114 valid_1's binary_error: 0.313433 [146] training's binary_error: 0.221509 valid_1's binary_error: 0.313433 [147] training's binary_error: 0.221509 valid_1's binary_error: 0.313433 [148] training's binary_error: 0.221509 valid_1's binary_error: 0.313433 [149] training's binary_error: 0.221509 valid_1's binary_error: 0.313433 [150] training's binary_error: 0.219904 valid_1's binary_error: 0.324627 [151] training's binary_error: 0.219904 valid_1's binary_error: 0.324627 [152] training's binary_error: 0.221509 valid_1's binary_error: 0.313433 [153] training's binary_error: 0.219904 valid_1's binary_error: 0.324627 [154] training's binary_error: 0.219904 valid_1's binary_error: 0.324627 [155] training's binary_error: 0.219904 valid_1's binary_error: 0.324627 [156] training's binary_error: 0.219904 valid_1's binary_error: 0.324627 [157] training's binary_error: 0.219904 valid_1's binary_error: 0.324627 [158] training's binary_error: 0.219904 valid_1's binary_error: 0.324627 [159] training's binary_error: 0.219904 valid_1's binary_error: 0.324627 [160] training's binary_error: 0.219904 valid_1's binary_error: 0.324627 [161] training's binary_error: 0.219904 valid_1's binary_error: 0.324627 [162] training's binary_error: 0.219904 valid_1's binary_error: 0.324627 [163] training's binary_error: 0.219904 valid_1's binary_error: 0.324627 [164] training's binary_error: 0.219904 valid_1's binary_error: 0.324627 [165] training's binary_error: 0.219904 valid_1's binary_error: 0.324627 [166] training's binary_error: 0.219904 valid_1's binary_error: 0.324627 [167] training's binary_error: 0.219904 valid_1's binary_error: 0.324627 [168] training's binary_error: 0.219904 valid_1's binary_error: 0.324627 [169] training's binary_error: 0.219904 valid_1's binary_error: 0.324627 [170] training's binary_error: 0.215088 valid_1's binary_error: 0.317164 [171] training's binary_error: 0.215088 valid_1's binary_error: 0.317164 [172] training's binary_error: 0.215088 valid_1's binary_error: 0.317164 [173] training's binary_error: 0.215088 valid_1's binary_error: 0.317164 [174] training's binary_error: 0.215088 valid_1's binary_error: 0.317164 [175] training's binary_error: 0.216693 valid_1's binary_error: 0.320896 [176] training's binary_error: 0.216693 valid_1's binary_error: 0.320896 [177] training's binary_error: 0.221509 valid_1's binary_error: 0.328358 [178] training's binary_error: 0.221509 valid_1's binary_error: 0.328358 [179] training's binary_error: 0.221509 valid_1's binary_error: 0.328358 [180] training's binary_error: 0.221509 valid_1's binary_error: 0.328358 [181] training's binary_error: 0.221509 valid_1's binary_error: 0.328358 [182] training's binary_error: 0.216693 valid_1's binary_error: 0.320896 [183] training's binary_error: 0.216693 valid_1's binary_error: 0.320896 [184] training's binary_error: 0.216693 valid_1's binary_error: 0.320896 [185] training's binary_error: 0.216693 valid_1's binary_error: 0.320896 [186] training's binary_error: 0.216693 valid_1's binary_error: 0.320896 [187] training's binary_error: 0.216693 valid_1's binary_error: 0.320896 [188] training's binary_error: 0.216693 valid_1's binary_error: 0.320896 [189] training's binary_error: 0.216693 valid_1's binary_error: 0.320896 [190] training's binary_error: 0.216693 valid_1's binary_error: 0.320896 [191] training's binary_error: 0.216693 valid_1's binary_error: 0.320896 [192] training's binary_error: 0.216693 valid_1's binary_error: 0.320896 [193] training's binary_error: 0.216693 valid_1's binary_error: 0.320896 [194] training's binary_error: 0.216693 valid_1's binary_error: 0.320896 [195] training's binary_error: 0.216693 valid_1's binary_error: 0.320896 [196] training's binary_error: 0.216693 valid_1's binary_error: 0.320896 [197] training's binary_error: 0.215088 valid_1's binary_error: 0.317164 [198] training's binary_error: 0.215088 valid_1's binary_error: 0.317164 [199] training's binary_error: 0.215088 valid_1's binary_error: 0.317164 [200] training's binary_error: 0.215088 valid_1's binary_error: 0.317164 [201] training's binary_error: 0.215088 valid_1's binary_error: 0.317164 Early stopping, best iteration is: [101] training's binary_error: 0.23114 valid_1's binary_error: 0.30597 /usr/local/lib/python3.7/dist-packages/lightgbm/engine.py:118: UserWarning: Found `num_boost_round` in params. Will use it instead of argument warnings.warn(&quot;Found `{}` in params. Will use it instead of argument&quot;.format(alias)) 0.6940298507462687","link":"/2022/07/06/day0706/"},{"title":"","text":"title: ‘데이터분석-하우스’date: ‘2022-07-07 09:00’ 1## 구글 드라이브 연동 12from google.colab import drive drive.mount(&quot;/content/drive&quot;) Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(&quot;/content/drive&quot;, force_remount=True). 1234567891011121314151617# This Python 3 environment comes with many helpful analytics libraries installed# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python# For example, here's several helpful packages to loadimport numpy as np # linear algebraimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)# Input data files are available in the read-only &quot;../input/&quot; directory# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directoryimport osfor dirname, _, filenames in os.walk('/kaggle/input'): for filename in filenames: print(os.path.join(dirname, filename))# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using &quot;Save &amp; Run All&quot; # You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session 라이브러리 불러오기 주요 라이브러리 버전을 확인한다. 123456789101112131415import pandas as pd import numpy as np import matplotlib as mpl import seaborn as sns import sklearnimport xgboost as xgb import lightgbm as lgbprint(&quot;pandas version :&quot;, pd.__version__)print(&quot;numpy version :&quot;, np.__version__)print(&quot;matplotlib version :&quot;, mpl.__version__)print(&quot;seaborn version :&quot;, sns.__version__)print(&quot;scikit-learn version :&quot;, sklearn.__version__)print(&quot;xgboost version :&quot;, xgb.__version__)print(&quot;lightgbm version :&quot;, lgb.__version__) pandas version : 1.3.5 numpy version : 1.21.6 matplotlib version : 3.2.2 seaborn version : 0.11.2 scikit-learn version : 1.0.2 xgboost version : 0.90 lightgbm version : 2.2.3 데이터 불러오기 pandas 활용 12345DATA_PATH = '/content/drive/MyDrive/Colab Notebooks/Human_ai/lecture/house/'train = pd.read_csv(DATA_PATH + &quot;train.csv&quot;)test = pd.read_csv(DATA_PATH + &quot;test.csv&quot;)print(&quot;데이터 불러오기 완료!&quot;) 데이터 불러오기 완료! 데이터 둘러보기 데이터를 둘러봅니다. train : 행 갯수 1460 열 갯수 81 (SalePrice 존재) test : 행 갯수 1459, 열 갯수 80 (SalePrice 컬럼 미 존재) 1train.shape, test.shape ((1460, 81), (1459, 80)) 1train.info() &lt;class 'pandas.core.frame.DataFrame'&gt; RangeIndex: 1460 entries, 0 to 1459 Data columns (total 81 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 Id 1460 non-null int64 1 MSSubClass 1460 non-null int64 2 MSZoning 1460 non-null object 3 LotFrontage 1201 non-null float64 4 LotArea 1460 non-null int64 5 Street 1460 non-null object 6 Alley 91 non-null object 7 LotShape 1460 non-null object 8 LandContour 1460 non-null object 9 Utilities 1460 non-null object 10 LotConfig 1460 non-null object 11 LandSlope 1460 non-null object 12 Neighborhood 1460 non-null object 13 Condition1 1460 non-null object 14 Condition2 1460 non-null object 15 BldgType 1460 non-null object 16 HouseStyle 1460 non-null object 17 OverallQual 1460 non-null int64 18 OverallCond 1460 non-null int64 19 YearBuilt 1460 non-null int64 20 YearRemodAdd 1460 non-null int64 21 RoofStyle 1460 non-null object 22 RoofMatl 1460 non-null object 23 Exterior1st 1460 non-null object 24 Exterior2nd 1460 non-null object 25 MasVnrType 1452 non-null object 26 MasVnrArea 1452 non-null float64 27 ExterQual 1460 non-null object 28 ExterCond 1460 non-null object 29 Foundation 1460 non-null object 30 BsmtQual 1423 non-null object 31 BsmtCond 1423 non-null object 32 BsmtExposure 1422 non-null object 33 BsmtFinType1 1423 non-null object 34 BsmtFinSF1 1460 non-null int64 35 BsmtFinType2 1422 non-null object 36 BsmtFinSF2 1460 non-null int64 37 BsmtUnfSF 1460 non-null int64 38 TotalBsmtSF 1460 non-null int64 39 Heating 1460 non-null object 40 HeatingQC 1460 non-null object 41 CentralAir 1460 non-null object 42 Electrical 1459 non-null object 43 1stFlrSF 1460 non-null int64 44 2ndFlrSF 1460 non-null int64 45 LowQualFinSF 1460 non-null int64 46 GrLivArea 1460 non-null int64 47 BsmtFullBath 1460 non-null int64 48 BsmtHalfBath 1460 non-null int64 49 FullBath 1460 non-null int64 50 HalfBath 1460 non-null int64 51 BedroomAbvGr 1460 non-null int64 52 KitchenAbvGr 1460 non-null int64 53 KitchenQual 1460 non-null object 54 TotRmsAbvGrd 1460 non-null int64 55 Functional 1460 non-null object 56 Fireplaces 1460 non-null int64 57 FireplaceQu 770 non-null object 58 GarageType 1379 non-null object 59 GarageYrBlt 1379 non-null float64 60 GarageFinish 1379 non-null object 61 GarageCars 1460 non-null int64 62 GarageArea 1460 non-null int64 63 GarageQual 1379 non-null object 64 GarageCond 1379 non-null object 65 PavedDrive 1460 non-null object 66 WoodDeckSF 1460 non-null int64 67 OpenPorchSF 1460 non-null int64 68 EnclosedPorch 1460 non-null int64 69 3SsnPorch 1460 non-null int64 70 ScreenPorch 1460 non-null int64 71 PoolArea 1460 non-null int64 72 PoolQC 7 non-null object 73 Fence 281 non-null object 74 MiscFeature 54 non-null object 75 MiscVal 1460 non-null int64 76 MoSold 1460 non-null int64 77 YrSold 1460 non-null int64 78 SaleType 1460 non-null object 79 SaleCondition 1460 non-null object 80 SalePrice 1460 non-null int64 dtypes: float64(3), int64(35), object(43) memory usage: 924.0+ KB 1test.info() &lt;class 'pandas.core.frame.DataFrame'&gt; RangeIndex: 1459 entries, 0 to 1458 Data columns (total 80 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 Id 1459 non-null int64 1 MSSubClass 1459 non-null int64 2 MSZoning 1455 non-null object 3 LotFrontage 1232 non-null float64 4 LotArea 1459 non-null int64 5 Street 1459 non-null object 6 Alley 107 non-null object 7 LotShape 1459 non-null object 8 LandContour 1459 non-null object 9 Utilities 1457 non-null object 10 LotConfig 1459 non-null object 11 LandSlope 1459 non-null object 12 Neighborhood 1459 non-null object 13 Condition1 1459 non-null object 14 Condition2 1459 non-null object 15 BldgType 1459 non-null object 16 HouseStyle 1459 non-null object 17 OverallQual 1459 non-null int64 18 OverallCond 1459 non-null int64 19 YearBuilt 1459 non-null int64 20 YearRemodAdd 1459 non-null int64 21 RoofStyle 1459 non-null object 22 RoofMatl 1459 non-null object 23 Exterior1st 1458 non-null object 24 Exterior2nd 1458 non-null object 25 MasVnrType 1443 non-null object 26 MasVnrArea 1444 non-null float64 27 ExterQual 1459 non-null object 28 ExterCond 1459 non-null object 29 Foundation 1459 non-null object 30 BsmtQual 1415 non-null object 31 BsmtCond 1414 non-null object 32 BsmtExposure 1415 non-null object 33 BsmtFinType1 1417 non-null object 34 BsmtFinSF1 1458 non-null float64 35 BsmtFinType2 1417 non-null object 36 BsmtFinSF2 1458 non-null float64 37 BsmtUnfSF 1458 non-null float64 38 TotalBsmtSF 1458 non-null float64 39 Heating 1459 non-null object 40 HeatingQC 1459 non-null object 41 CentralAir 1459 non-null object 42 Electrical 1459 non-null object 43 1stFlrSF 1459 non-null int64 44 2ndFlrSF 1459 non-null int64 45 LowQualFinSF 1459 non-null int64 46 GrLivArea 1459 non-null int64 47 BsmtFullBath 1457 non-null float64 48 BsmtHalfBath 1457 non-null float64 49 FullBath 1459 non-null int64 50 HalfBath 1459 non-null int64 51 BedroomAbvGr 1459 non-null int64 52 KitchenAbvGr 1459 non-null int64 53 KitchenQual 1458 non-null object 54 TotRmsAbvGrd 1459 non-null int64 55 Functional 1457 non-null object 56 Fireplaces 1459 non-null int64 57 FireplaceQu 729 non-null object 58 GarageType 1383 non-null object 59 GarageYrBlt 1381 non-null float64 60 GarageFinish 1381 non-null object 61 GarageCars 1458 non-null float64 62 GarageArea 1458 non-null float64 63 GarageQual 1381 non-null object 64 GarageCond 1381 non-null object 65 PavedDrive 1459 non-null object 66 WoodDeckSF 1459 non-null int64 67 OpenPorchSF 1459 non-null int64 68 EnclosedPorch 1459 non-null int64 69 3SsnPorch 1459 non-null int64 70 ScreenPorch 1459 non-null int64 71 PoolArea 1459 non-null int64 72 PoolQC 3 non-null object 73 Fence 290 non-null object 74 MiscFeature 51 non-null object 75 MiscVal 1459 non-null int64 76 MoSold 1459 non-null int64 77 YrSold 1459 non-null int64 78 SaleType 1458 non-null object 79 SaleCondition 1459 non-null object dtypes: float64(11), int64(26), object(43) memory usage: 912.0+ KB 데이터 시각화 여기에서는 생략 종속변수 분포 확인 샤피로 검정 정규분포인가요? 정규분포가 아님! –&gt; 로그변환, 박스콕스 변환 등등 정규분포로 만들어 줘야 함. 선형모델의 성능을 올리기 위해서는 123456789101112import matplotlib.pyplot as plt from scipy.stats import norm (mu, sigma) = norm.fit(train['SalePrice'])print(&quot;평균:&quot;, mu)print(&quot;표준편차:&quot;, sigma)fig, ax = plt.subplots(figsize=(10, 6))sns.histplot(train['SalePrice'])ax.set(title=&quot;SalePrice Distribution&quot;)ax.axvline(mu, color = 'r', linestyle = '--')ax.text(mu + 10000, 160, 'Mean of SalePrice', color = 'r')plt.show() 평균: 180921.19589041095 표준편차: 79415.29188606751 로그변환을 해서 정규분포로 변환해준다. 1234567891011121314# 로그변환을 함. train['SalePrice'] = np.log1p(train['SalePrice'])(mu, sigma) = norm.fit(train['SalePrice'])print(&quot;평균:&quot;, mu)print(&quot;표준편차:&quot;, sigma)fig, ax = plt.subplots(figsize=(10, 6))sns.histplot(train['SalePrice'])ax.set(title=&quot;SalePrice Distribution&quot;)ax.axvline(mu, color = 'r', linestyle = '--')ax.text(mu + 0.0001, 160, 'Mean of SalePrice', color = 'r')ax.set_ylim(0, 170)plt.show() 평균: 12.024057394918406 표준편차: 0.39931245219387496 데이터 전처리 컬럼 갯수가 많다?, 어떤 컬럼을 없앨 것인가? 머신러닝 연산 속도부터 높여야 함. 데이터 ID값 제거12345train_ID = train['Id']test_ID = test['Id']train = train.drop(['Id'], axis = 1)train.shape (1460, 80) 12test = test.drop(['Id'], axis = 1)test.shape (1459, 79) Y값 추출 train데이터에 SalePrice만 따로 저장한다. 123y = train['SalePrice']train = train.drop('SalePrice', axis = 1)train.shape (1460, 79) 1test.shape (1459, 79) 데이터 합치기 강의 목적 원칙 train, 따로 정리 test, 따로 정리 Data Leakage 오류를 범할 가능성이 높음. 12all_df = pd.concat([train, test]).reset_index(drop=True)all_df.shape (2919, 79) 결측치 확인 결측치의 비율 확인하는 사용자 정의 함수 작성 1train.info() &lt;class 'pandas.core.frame.DataFrame'&gt; RangeIndex: 1460 entries, 0 to 1459 Data columns (total 79 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 MSSubClass 1460 non-null int64 1 MSZoning 1460 non-null object 2 LotFrontage 1201 non-null float64 3 LotArea 1460 non-null int64 4 Street 1460 non-null object 5 Alley 91 non-null object 6 LotShape 1460 non-null object 7 LandContour 1460 non-null object 8 Utilities 1460 non-null object 9 LotConfig 1460 non-null object 10 LandSlope 1460 non-null object 11 Neighborhood 1460 non-null object 12 Condition1 1460 non-null object 13 Condition2 1460 non-null object 14 BldgType 1460 non-null object 15 HouseStyle 1460 non-null object 16 OverallQual 1460 non-null int64 17 OverallCond 1460 non-null int64 18 YearBuilt 1460 non-null int64 19 YearRemodAdd 1460 non-null int64 20 RoofStyle 1460 non-null object 21 RoofMatl 1460 non-null object 22 Exterior1st 1460 non-null object 23 Exterior2nd 1460 non-null object 24 MasVnrType 1452 non-null object 25 MasVnrArea 1452 non-null float64 26 ExterQual 1460 non-null object 27 ExterCond 1460 non-null object 28 Foundation 1460 non-null object 29 BsmtQual 1423 non-null object 30 BsmtCond 1423 non-null object 31 BsmtExposure 1422 non-null object 32 BsmtFinType1 1423 non-null object 33 BsmtFinSF1 1460 non-null int64 34 BsmtFinType2 1422 non-null object 35 BsmtFinSF2 1460 non-null int64 36 BsmtUnfSF 1460 non-null int64 37 TotalBsmtSF 1460 non-null int64 38 Heating 1460 non-null object 39 HeatingQC 1460 non-null object 40 CentralAir 1460 non-null object 41 Electrical 1459 non-null object 42 1stFlrSF 1460 non-null int64 43 2ndFlrSF 1460 non-null int64 44 LowQualFinSF 1460 non-null int64 45 GrLivArea 1460 non-null int64 46 BsmtFullBath 1460 non-null int64 47 BsmtHalfBath 1460 non-null int64 48 FullBath 1460 non-null int64 49 HalfBath 1460 non-null int64 50 BedroomAbvGr 1460 non-null int64 51 KitchenAbvGr 1460 non-null int64 52 KitchenQual 1460 non-null object 53 TotRmsAbvGrd 1460 non-null int64 54 Functional 1460 non-null object 55 Fireplaces 1460 non-null int64 56 FireplaceQu 770 non-null object 57 GarageType 1379 non-null object 58 GarageYrBlt 1379 non-null float64 59 GarageFinish 1379 non-null object 60 GarageCars 1460 non-null int64 61 GarageArea 1460 non-null int64 62 GarageQual 1379 non-null object 63 GarageCond 1379 non-null object 64 PavedDrive 1460 non-null object 65 WoodDeckSF 1460 non-null int64 66 OpenPorchSF 1460 non-null int64 67 EnclosedPorch 1460 non-null int64 68 3SsnPorch 1460 non-null int64 69 ScreenPorch 1460 non-null int64 70 PoolArea 1460 non-null int64 71 PoolQC 7 non-null object 72 Fence 281 non-null object 73 MiscFeature 54 non-null object 74 MiscVal 1460 non-null int64 75 MoSold 1460 non-null int64 76 YrSold 1460 non-null int64 77 SaleType 1460 non-null object 78 SaleCondition 1460 non-null object dtypes: float64(3), int64(33), object(43) memory usage: 901.2+ KB 12345678def check_na(data, head_num = 6): isnull_na = (data.isnull().sum() / len(data)) * 100 data_na = isnull_na.drop(isnull_na[isnull_na == 0].index).sort_values(ascending=False) missing_data = pd.DataFrame({'Missing Ratio' :data_na, 'Data Type': data.dtypes[data_na.index]}) print(&quot;결측치 데이터 컬럼과 건수:\\n&quot;, missing_data.head(head_num))check_na(all_df, 20) 결측치 데이터 컬럼과 건수: Missing Ratio Data Type PoolQC 99.657417 object MiscFeature 96.402878 object Alley 93.216855 object Fence 80.438506 object FireplaceQu 48.646797 object LotFrontage 16.649538 float64 GarageFinish 5.447071 object GarageQual 5.447071 object GarageCond 5.447071 object GarageYrBlt 5.447071 float64 GarageType 5.378554 object BsmtExposure 2.809181 object BsmtCond 2.809181 object BsmtQual 2.774923 object BsmtFinType2 2.740665 object BsmtFinType1 2.706406 object MasVnrType 0.822199 object MasVnrArea 0.787941 float64 MSZoning 0.137033 object BsmtFullBath 0.068517 float64 결측치 제거 결측치 비율이 높은 변수들을 모두 제거하기로 했다. 123all_df = all_df.drop(['PoolQC', 'MiscFeature', 'Alley', 'Fence', 'FireplaceQu', 'LotFrontage'], axis = 1)print(all_df.shape)check_na(all_df, 40) (2919, 73) 결측치 데이터 컬럼과 건수: Missing Ratio Data Type GarageCond 5.447071 object GarageQual 5.447071 object GarageYrBlt 5.447071 float64 GarageFinish 5.447071 object GarageType 5.378554 object BsmtCond 2.809181 object BsmtExposure 2.809181 object BsmtQual 2.774923 object BsmtFinType2 2.740665 object BsmtFinType1 2.706406 object MasVnrType 0.822199 object MasVnrArea 0.787941 float64 MSZoning 0.137033 object Functional 0.068517 object Utilities 0.068517 object BsmtFullBath 0.068517 float64 BsmtHalfBath 0.068517 float64 GarageArea 0.034258 float64 GarageCars 0.034258 float64 TotalBsmtSF 0.034258 float64 KitchenQual 0.034258 object Electrical 0.034258 object BsmtUnfSF 0.034258 float64 BsmtFinSF2 0.034258 float64 BsmtFinSF1 0.034258 float64 Exterior2nd 0.034258 object Exterior1st 0.034258 object SaleType 0.034258 object 결측치 채우기 train 데이터와 test 데이터가 섞이면 안됨. train / test 분리해서 진행해야 함. 문자데이터 : 자주 등장하는 빈도 값으로 채움 숫자데이터 : 평균이 아니라, 중간값으로 채울 예정 12# all_df['SaleType'].value_counts()all_df['SaleType'].mode()[0] 'WD' 1check_na(all_df, 40) 결측치 데이터 컬럼과 건수: Missing Ratio Data Type GarageCond 5.447071 object GarageQual 5.447071 object GarageYrBlt 5.447071 float64 GarageFinish 5.447071 object GarageType 5.378554 object BsmtCond 2.809181 object BsmtExposure 2.809181 object BsmtQual 2.774923 object BsmtFinType2 2.740665 object BsmtFinType1 2.706406 object MasVnrType 0.822199 object MasVnrArea 0.787941 float64 MSZoning 0.137033 object Functional 0.068517 object Utilities 0.068517 object BsmtFullBath 0.068517 float64 BsmtHalfBath 0.068517 float64 GarageArea 0.034258 float64 GarageCars 0.034258 float64 TotalBsmtSF 0.034258 float64 KitchenQual 0.034258 object Electrical 0.034258 object BsmtUnfSF 0.034258 float64 BsmtFinSF2 0.034258 float64 BsmtFinSF1 0.034258 float64 Exterior2nd 0.034258 object Exterior1st 0.034258 object SaleType 0.034258 object 12345678910111213141516171819202122import numpy as np# 문자열 데이터만 추출cat_all_vars = train.select_dtypes(exclude=[np.number])print(&quot;The whole number of all_vars&quot;, len(list(cat_all_vars)))# 문자열 데이터 중에서 이미 기 삭제했던 Feature들이 있었기 때문에, # 한번 더 Feature를 정리하는 코드를 작성한다. # 따라서 38개의 Feature만 추출했다. final_cat_vars = []for v in cat_all_vars: if v not in ['PoolQC', 'MiscFeature', 'Alley', 'Fence', 'FireplaceQu']: final_cat_vars.append(v)print(&quot;The whole number of final_cat_vars&quot;, len(final_cat_vars))# 이제 각 Feature 마다 빈도수가 가장 많이 나타나는 값을 추가하는 코드를 작성한다. for i in final_cat_vars: all_df[i] = all_df[i].fillna(all_df[i].mode()[0])# 이제 수치형 데이터만 남은 것을 확인한다. check_na(all_df, 20) The whole number of all_vars 43 The whole number of final_cat_vars 38 결측치 데이터 컬럼과 건수: Missing Ratio Data Type GarageYrBlt 5.447071 float64 MasVnrArea 0.787941 float64 BsmtFullBath 0.068517 float64 BsmtHalfBath 0.068517 float64 BsmtFinSF1 0.034258 float64 BsmtFinSF2 0.034258 float64 BsmtUnfSF 0.034258 float64 TotalBsmtSF 0.034258 float64 GarageCars 0.034258 float64 GarageArea 0.034258 float64 수치형 데이터의 결측치를 추가할 수 있다. 평균이 아닌 중간값으로 진행한다. 12345678910111213141516import numpy as np# 방법은 기존과 동일하다. # 이번에는 수치형 데이터만 추출한다. num_all_vars = list(train.select_dtypes(include=[np.number]))print(&quot;The whole number of all_vars&quot;, len(num_all_vars))# 수치형 데이터 중, 결측치가 많았던 `LotFrontage`만 처리한다. num_all_vars.remove('LotFrontage')print(&quot;The whole number of final_cat_vars&quot;, len(num_all_vars))# 이번에는 수치형 데이터의 평균이 아닌 중간값을 지정했다. for i in num_all_vars: all_df[i].fillna(value=all_df[i].median(), inplace=True)check_na(all_df, 20) The whole number of all_vars 36 The whole number of final_cat_vars 35 결측치 데이터 컬럼과 건수: Empty DataFrame Columns: [Missing Ratio, Data Type] Index: [] 도출 변수 새로운 도출 변수를 작성 (기존 변수 활용) 기존 변수 제거 각 층의 면적으로 모두 더해 전체 면적으로 계산한 새로운 변수를 작성한다. 123all_df['TotalSF'] = all_df['TotalBsmtSF'] + all_df['1stFlrSF'] + all_df['2ndFlrSF']all_df = all_df.drop(['TotalBsmtSF', '1stFlrSF', '2ndFlrSF'], axis=1)print(all_df.shape) (2919, 71) 1234all_df['Total_Bathrooms'] = (all_df['FullBath'] + (0.5 * all_df['HalfBath']) + all_df['BsmtFullBath'] + (0.5 * all_df['BsmtHalfBath']))all_df['Total_porch_sf'] = (all_df['OpenPorchSF'] + all_df['3SsnPorch'] + all_df['EnclosedPorch'] + all_df['ScreenPorch'])all_df = all_df.drop(['FullBath', 'HalfBath', 'BsmtFullBath', 'BsmtHalfBath', 'OpenPorchSF', '3SsnPorch', 'EnclosedPorch', 'ScreenPorch'], axis=1)print(all_df.shape) (2919, 65) 연도와 관련된 변수를 추출하는 코드 작성. 12345678910num_all_vars = list(train.select_dtypes(include=[np.number]))year_feature = []for var in num_all_vars: if 'Yr' in var: year_feature.append(var) elif 'Year' in var: year_feature.append(var) else: print(var, &quot;is not related with Year&quot;)print(year_feature) MSSubClass is not related with Year LotFrontage is not related with Year LotArea is not related with Year OverallQual is not related with Year OverallCond is not related with Year MasVnrArea is not related with Year BsmtFinSF1 is not related with Year BsmtFinSF2 is not related with Year BsmtUnfSF is not related with Year TotalBsmtSF is not related with Year 1stFlrSF is not related with Year 2ndFlrSF is not related with Year LowQualFinSF is not related with Year GrLivArea is not related with Year BsmtFullBath is not related with Year BsmtHalfBath is not related with Year FullBath is not related with Year HalfBath is not related with Year BedroomAbvGr is not related with Year KitchenAbvGr is not related with Year TotRmsAbvGrd is not related with Year Fireplaces is not related with Year GarageCars is not related with Year GarageArea is not related with Year WoodDeckSF is not related with Year OpenPorchSF is not related with Year EnclosedPorch is not related with Year 3SsnPorch is not related with Year ScreenPorch is not related with Year PoolArea is not related with Year MiscVal is not related with Year MoSold is not related with Year ['YearBuilt', 'YearRemodAdd', 'GarageYrBlt', 'YrSold'] 12345678fig, ax = plt.subplots(3, 1, figsize=(10, 6), sharex=True, sharey=True)for i, var in enumerate(year_feature): if var != 'YrSold': ax[i].scatter(train[var], y, alpha=0.3) ax[i].set_title('{}'.format(var), size=15) ax[i].set_ylabel('SalePrice', size=15, labelpad=12.5)plt.tight_layout()plt.show() 12all_df = all_df.drop(['YearBuilt', 'GarageYrBlt'], axis=1)print(all_df.shape) (2919, 63) 12345YearsSinceRemodel = train['YrSold'].astype(int) - train['YearRemodAdd'].astype(int)fig, ax = plt.subplots(figsize=(10, 6))ax.scatter(YearsSinceRemodel, y, alpha=0.3)plt.show() 123all_df['YearsSinceRemodel'] = all_df['YrSold'].astype(int) - all_df['YearRemodAdd'].astype(int)all_df = all_df.drop(['YrSold', 'YearRemodAdd'], axis=1)print(all_df.shape) (2919, 62) 더미변수 더미변수란 원 데이터 독립변수를 0과 1로 변환하는 변수를 말함. 1all_df['PoolArea'].value_counts() 0 2906 512 1 648 1 576 1 555 1 480 1 519 1 738 1 144 1 368 1 444 1 228 1 561 1 800 1 Name: PoolArea, dtype: int64 사용자 정의 함수 만들기 12345def count_dummy(x): if x &gt; 0: return 1 else: return 0 12all_df['PoolArea'] = all_df['PoolArea'].apply(count_dummy) all_df['PoolArea'].value_counts() 0 2906 1 13 Name: PoolArea, dtype: int64 12all_df['GarageArea'] = all_df['GarageArea'].apply(count_dummy)all_df['GarageArea'].value_counts() 1 2762 0 157 Name: GarageArea, dtype: int64 12all_df['Fireplaces'] = all_df['Fireplaces'].apply(count_dummy)all_df['Fireplaces'].value_counts() 1 1499 0 1420 Name: Fireplaces, dtype: int64 인코딩 문자를 숫자로 변환해주는 코드를 인코딩 변환 1print(all_df.info()) &lt;class 'pandas.core.frame.DataFrame'&gt; RangeIndex: 2919 entries, 0 to 2918 Data columns (total 62 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 MSSubClass 2919 non-null int64 1 MSZoning 2919 non-null object 2 LotArea 2919 non-null int64 3 Street 2919 non-null object 4 LotShape 2919 non-null object 5 LandContour 2919 non-null object 6 Utilities 2919 non-null object 7 LotConfig 2919 non-null object 8 LandSlope 2919 non-null object 9 Neighborhood 2919 non-null object 10 Condition1 2919 non-null object 11 Condition2 2919 non-null object 12 BldgType 2919 non-null object 13 HouseStyle 2919 non-null object 14 OverallQual 2919 non-null int64 15 OverallCond 2919 non-null int64 16 RoofStyle 2919 non-null object 17 RoofMatl 2919 non-null object 18 Exterior1st 2919 non-null object 19 Exterior2nd 2919 non-null object 20 MasVnrType 2919 non-null object 21 MasVnrArea 2919 non-null float64 22 ExterQual 2919 non-null object 23 ExterCond 2919 non-null object 24 Foundation 2919 non-null object 25 BsmtQual 2919 non-null object 26 BsmtCond 2919 non-null object 27 BsmtExposure 2919 non-null object 28 BsmtFinType1 2919 non-null object 29 BsmtFinSF1 2919 non-null float64 30 BsmtFinType2 2919 non-null object 31 BsmtFinSF2 2919 non-null float64 32 BsmtUnfSF 2919 non-null float64 33 Heating 2919 non-null object 34 HeatingQC 2919 non-null object 35 CentralAir 2919 non-null object 36 Electrical 2919 non-null object 37 LowQualFinSF 2919 non-null int64 38 GrLivArea 2919 non-null int64 39 BedroomAbvGr 2919 non-null int64 40 KitchenAbvGr 2919 non-null int64 41 KitchenQual 2919 non-null object 42 TotRmsAbvGrd 2919 non-null int64 43 Functional 2919 non-null object 44 Fireplaces 2919 non-null int64 45 GarageType 2919 non-null object 46 GarageFinish 2919 non-null object 47 GarageCars 2919 non-null float64 48 GarageArea 2919 non-null int64 49 GarageQual 2919 non-null object 50 GarageCond 2919 non-null object 51 PavedDrive 2919 non-null object 52 WoodDeckSF 2919 non-null int64 53 PoolArea 2919 non-null int64 54 MiscVal 2919 non-null int64 55 MoSold 2919 non-null int64 56 SaleType 2919 non-null object 57 SaleCondition 2919 non-null object 58 TotalSF 2919 non-null float64 59 Total_Bathrooms 2919 non-null float64 60 Total_porch_sf 2919 non-null int64 61 YearsSinceRemodel 2919 non-null int64 dtypes: float64(7), int64(17), object(38) memory usage: 1.4+ MB None Label Encoding ,Ordinal Encoding,One-Hot Encoding 인코딩은 문자 데이터를 수치로 변환하는 방법론 중의 하나이다. 123456789101112# 분류모형# 종속변수(양성,음성)from sklearn.preprocessing import LabelEncoderimport pandas as pdtemp = pd.DataFrame({'Food_Name': ['Apple', 'Chicken', 'Broccoli'], 'Calories': [95, 231, 50]})encoder = LabelEncoder()encoder.fit(temp['Food_Name'])labels = encoder.transform(temp['Food_Name'])print(list(temp['Food_Name']), &quot;==&gt;&quot;, labels) ['Apple', 'Chicken', 'Broccoli'] ==&gt; [0 2 1] 12345678910# Ordinal Encoding은 독립변수로만 사용from sklearn.preprocessing import OrdinalEncoderimport pandas as pdtemp = pd.DataFrame({'Food_Name': ['Apple', 'Chicken', 'Broccoli'], 'Calories': [95, 231, 50]})encoder = OrdinalEncoder()labels = encoder.fit_transform(temp[['Food_Name']])print(list(temp['Food_Name']), &quot;==&gt;&quot;, labels.tolist()) ['Apple', 'Chicken', 'Broccoli'] ==&gt; [[0.0], [2.0], [1.0]] 12345temp = pd.DataFrame({'Food_Name': ['Apple', 'Chicken', 'Broccoli'], 'Calories': [95, 231, 50]})temp['Food_No'] = temp.Food_Name.replace(to_replace = ['Apple', 'Chicken', 'Broccoli'], value = [1, 2, 3])print(temp[['Food_Name', 'Food_No']]) Food_Name Food_No 0 Apple 1 1 Chicken 2 2 Broccoli 3 123456789101112131415#One-Hot Encodingimport pandas as pdfrom sklearn.preprocessing import LabelBinarizertemp = pd.DataFrame({'Food_Name': ['Apple', 'Chicken', 'Broccoli'], 'Calories': [95, 231, 50]})encoder = LabelBinarizer()encoder.fit(temp['Food_Name'])transformed = encoder.transform(temp['Food_Name'])ohe_df = pd.DataFrame(transformed)temp = pd.concat([temp, ohe_df], axis=1).drop(['Food_Name'], axis=1)temp.columns = ['Calories', 'Food_Name_Apple', 'Food_Name_Broccoli', 'Food_Name_Chicken']print(temp)print(temp.shape) Calories Food_Name_Apple Food_Name_Broccoli Food_Name_Chicken 0 95 1 0 0 1 231 0 0 1 2 50 0 1 0 (3, 4) 12345678import pandas as pdtemp = pd.DataFrame({'Food_Name': ['Apple', 'Chicken', 'Broccoli'], 'Calories': [95, 231, 50]})temp = pd.get_dummies(temp)print(temp)print(temp.shape) Calories Food_Name_Apple Food_Name_Broccoli Food_Name_Chicken 0 95 1 0 0 1 231 0 0 1 2 50 0 1 0 (3, 4) 본 데이터 적용 여기서는 Ordinal Encoding적용 안함(실전에는 필요) 원 핫 인코딩 적용 12all_df = pd.get_dummies(all_df).reset_index(drop=True)all_df.shape (2919, 258) train,test 데이터 합쳐서 진행 train,test 데이터 재분리 1234X = all_df.iloc[:len(y), :]test = all_df.iloc[len(y):, :]X.shape, y.shape, test.shape ((1460, 258), (1460,), (1459, 258)) 머신러닝을 위한 데이터 전처리가 끝남 과제 남은 시간동안 교제를 모고 머신러닝 학습 및 RMSE 구하세요. 데이터셋 분리 X데이터를 X-train, X_test, y_train, y_test로 분리 1234567from sklearn.model_selection import train_test_splitX_train, X_test, y_train, y_test = train_test_split( #독립변수, 종속변수 X, y, test_size = 0.3, random_state = 0 )X_train.shape, X_test.shape, y_train.shape, y_test.shape ((1022, 258), (438, 258), (1022,), (438,)) 12345from sklearn.linear_model import LinearRegressionlr_model = LinearRegression()lr_model.fit(X_train, y_train)print(lr_model.score(X_train,y_train))print(lr_model.score(X_test,y_test)) 0.9513387660720953 -52492042.95698945 https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_val_score.html 123from sklearn.metrics import mean_squared_errordef rmse(y_true, y_pred): return np.sqrt(mean_squared_error(y_true, y_pred)) 머신러닝 모형 정의, 검증평가 교차검증함수 만들기 1234567#다양한 모형from sklearn.metrics import mean_squared_errorfrom sklearn.model_selection import KFold, cross_val_scorefrom sklearn.linear_model import LinearRegressionfrom sklearn.ensemble import RandomForestRegressor from lightgbm import LGBMRegressorfrom xgboost import XGBRegressor 123456789101112# 교차 검증from sklearn.model_selection import KFold, cross_val_score # 모형 정의from sklearn.linear_model import LinearRegression def cv_rmse(model, n_folds=5): cv = KFold(n_splits = n_folds, random_state=42, shuffle=True) rmse_list = np.sqrt(-cross_val_score(model, X, y, scoring='neg_mean_squared_error', cv=cv)) print('CV RMSE Value List:', np.round(rmse_list, 4)) print('CV RMSE mean List:', np.round(np.mean(rmse_list), 4)) return rmse_list 1234567rmse_scores = {} lr_model = LinearRegression()score = cv_rmse(lr_model, n_folds=5)print('linear regression - mean : {:.4f} (std: {:.4f})'.format(score.mean(), score.std()))rmse_scores['Linear Regression'] = (score.mean(), score.std()) CV RMSE Value List: [1.2790000e-01 1.2580000e-01 1.1136811e+03 1.5580000e-01 9.2491152e+03] CV RMSE mean List: 2072.6412 linear regression - mean : 2072.6412 (std: 3614.0617) 제출방법 123456789from sklearn.model_selection import cross_val_predict# X = all_df.iloc[:len(y), :]# X_test = all_df.iloc[len(y):, :]# X.shape, y.shape, X_test.shapelr_model_fit = lr_model.fit(X_train, y_train)final_preds = np.floor(np.expm1(lr_model_fit.predict(test)))print(final_preds) [109089. 160608. 181348. ... 167207. 109945. 204485.] /usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:8: RuntimeWarning: overflow encountered in expm1 1234submission = pd.read_csv(DATA_PATH + &quot;sample_submission.csv&quot;)submission.iloc[:,1] = final_predsprint(submission.head())submission.to_csv(&quot;submission.csv&quot;, index=False) Id SalePrice 0 1461 109089.0 1 1462 160608.0 2 1463 181348.0 3 1464 195114.0 4 1465 189188.0 모형 만들기123456from sklearn.linear_model import LinearRegressionlr_model = LinearRegression()lr_model.fit(X_train, y_train) print(lr_model.score(X_train, y_train))print(lr_model.score(X_test, y_test)) 0.9513387660720953 -52492042.95698945 번외)평가지표-MAE, MSE, RMSE MAE 실제값과 예측값의 차이, 오차와 오차들의 절댓값 평균을 말함. 1234567891011121314151617181920212223242526272829303132333435363738394041import numpy as np def mean_absolute_error(y_true, y_pred): error = 0 for yt, yp in zip(y_true, y_pred): # yt : 실젯값 # yp : 예측값 error = error + np.abs(yt - yp) # 절댓값 오차의 평균 mae = error / len(y_true) return maedef mean_squared_error(y_true, y_pred): error = 0 for yt, yp in zip(y_true, y_pred): # yt : 실젯값 # yp : 예측값 error = error + (yt - yp) ** 2 # 제곱값 오차의 평균 mse = error / len(y_true) return msedef root_mean_squared_error(y_true, y_pred): error = 0 for yt, yp in zip(y_true, y_pred): # yt : 실젯값 # yp : 예측값 error = error + (yt - yp) ** 2 # 제곱값 오차의 평균 mse = error / len(y_true) # 제곱근 추가 rmse = np.round(np.sqrt(mse), 3) return rmsey_true = [400, 300, 800]y_pred = [380, 320, 777]print(&quot;MAE:&quot;, mean_absolute_error(y_true, y_pred))print(&quot;MSE:&quot;, mean_squared_error(y_true, y_pred))print(&quot;RMSE:&quot;, root_mean_squared_error(y_true, y_pred))","link":"/2022/07/07/lecture_in_humanedu/"},{"title":"","text":"title: ‘자전거’date: ‘2022-07-08 09:00’ 12from google.colab import drive drive.mount(&quot;/content/drive&quot;) Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(&quot;/content/drive&quot;, force_remount=True). step 01. 필수 라이브러리 불러오기 주석 12345678910111213import pandas as pd # 데이터 가공import numpy as np # 수치 연산import matplotlib as mpl # 시각화 import matplotlib.pyplot as plt import seaborn as sns # 시각화 import sklearn # 머신러닝# 버전 확인print(&quot;pandas version :&quot;, pd.__version__)print(&quot;numpy version :&quot;, np.__version__)# matplotlib# sklearn# seaborn pandas version : 1.3.5 numpy version : 1.21.6 step 02. 데이터 불러오기 데이터를 불러온다. 1234567DATA_PATH = '/content/drive/MyDrive/Colab Notebooks/Human_ai/lecture/bike/'train = pd.read_csv(DATA_PATH + 'train.csv') # 훈련 데이터test = pd.read_csv(DATA_PATH + 'test.csv') # 테스트 데이터submission = pd.read_csv(DATA_PATH + 'sampleSubmission.csv') # 제출 샘플 데이터train.shape, test.shape, submission.shape ((10886, 12), (6493, 9), (6493, 2)) step 03. 데이터 확인하기1train.info() &lt;class 'pandas.core.frame.DataFrame'&gt; RangeIndex: 10886 entries, 0 to 10885 Data columns (total 12 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 datetime 10886 non-null object 1 season 10886 non-null int64 2 holiday 10886 non-null int64 3 workingday 10886 non-null int64 4 weather 10886 non-null int64 5 temp 10886 non-null float64 6 atemp 10886 non-null float64 7 humidity 10886 non-null int64 8 windspeed 10886 non-null float64 9 casual 10886 non-null int64 10 registered 10886 non-null int64 11 count 10886 non-null int64 dtypes: float64(3), int64(8), object(1) memory usage: 1020.7+ KB 1test.info() &lt;class 'pandas.core.frame.DataFrame'&gt; RangeIndex: 6493 entries, 0 to 6492 Data columns (total 9 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 datetime 6493 non-null object 1 season 6493 non-null int64 2 holiday 6493 non-null int64 3 workingday 6493 non-null int64 4 weather 6493 non-null int64 5 temp 6493 non-null float64 6 atemp 6493 non-null float64 7 humidity 6493 non-null int64 8 windspeed 6493 non-null float64 dtypes: float64(3), int64(5), object(1) memory usage: 456.7+ KB 현재 판단으로는 결측치가 없어 보임. step 03. 탐색적 자료 분석 시각화 날짜 기반 train 다이렉트로 변화를 주면 전처리 시 헷갈림 복제본 뜬다. (탐색적 자료 분석) 데이터 셋 매우 작음 전체를 다 써도 큰 상관 없음 전체 데이터에서 일부 샘플링 함 12temp_df = train.copy()temp_df.info() &lt;class 'pandas.core.frame.DataFrame'&gt; RangeIndex: 10886 entries, 0 to 10885 Data columns (total 12 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 datetime 10886 non-null object 1 season 10886 non-null int64 2 holiday 10886 non-null int64 3 workingday 10886 non-null int64 4 weather 10886 non-null int64 5 temp 10886 non-null float64 6 atemp 10886 non-null float64 7 humidity 10886 non-null int64 8 windspeed 10886 non-null float64 9 casual 10886 non-null int64 10 registered 10886 non-null int64 11 count 10886 non-null int64 dtypes: float64(3), int64(8), object(1) memory usage: 1020.7+ KB 시각화를 위한 날짜 데이터 처리 연도, 월, 일자, 시간, 분, 초 1temp_df['datetime'][:10] 0 2011-01-01 00:00:00 1 2011-01-01 01:00:00 2 2011-01-01 02:00:00 3 2011-01-01 03:00:00 4 2011-01-01 04:00:00 5 2011-01-01 05:00:00 6 2011-01-01 06:00:00 7 2011-01-01 07:00:00 8 2011-01-01 08:00:00 9 2011-01-01 09:00:00 Name: datetime, dtype: object 12print(temp_df['datetime'][0].split()[1])print(temp_df['datetime'][100].split()[0].split('-')[2]) 00:00:00 05 12345year = temp_df['datetime'][100].split()[0].split('-')[0]month = temp_df['datetime'][100].split()[0].split('-')[1]day = temp_df['datetime'][100].split()[0].split('-')[2]print(year, month, day) 2011 01 05 12345hour = temp_df['datetime'][2].split()[1].split(':')[0]minutes = temp_df['datetime'][2].split()[1].split(':')[1]seconds = temp_df['datetime'][2].split()[1].split(':')[2]hour, minutes, seconds ('02', '00', '00') 1temp_df['datetime'][0].split()[0] '2011-01-01' 시간 데이터 전처리 방법론 1. 1234567891011121314151617import time import datetime # 시간 테스트 start_time = time.time()temp_df['date'] = temp_df['datetime'].apply(lambda x : x.split()[0])temp_df['year'] = temp_df['datetime'].apply(lambda x : x.split()[0].split('-')[0])temp_df['month'] = temp_df['datetime'].apply(lambda x : x.split()[0].split('-')[1])temp_df['day'] = temp_df['datetime'].apply(lambda x : x.split()[0].split('-')[2])temp_df['hour'] = temp_df['datetime'].apply(lambda x : x.split()[1].split(':')[0])end_time = time.time() lambda_ctime = end_time - start_timeprint(&quot;실행시간 (second) -&gt; &quot;, np.round(lambda_ctime, 3))temp_df[['datetime', 'year', 'month', 'day', 'hour']] 실행시간 (second) -&gt; 0.043 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } datetime year month day hour 0 2011-01-01 00:00:00 2011 01 01 00 1 2011-01-01 01:00:00 2011 01 01 01 2 2011-01-01 02:00:00 2011 01 01 02 3 2011-01-01 03:00:00 2011 01 01 03 4 2011-01-01 04:00:00 2011 01 01 04 ... ... ... ... ... ... 10881 2012-12-19 19:00:00 2012 12 19 19 10882 2012-12-19 20:00:00 2012 12 19 20 10883 2012-12-19 21:00:00 2012 12 19 21 10884 2012-12-19 22:00:00 2012 12 19 22 10885 2012-12-19 23:00:00 2012 12 19 23 10886 rows × 5 columns &lt;svg xmlns=”http://www.w3.org/2000/svg&quot; height=”24px”viewBox=”0 0 24 24” width=”24px”&gt; .colab-df-container { display:flex; flex-wrap:wrap; gap: 12px; } .colab-df-convert { background-color: #E8F0FE; border: none; border-radius: 50%; cursor: pointer; display: none; fill: #1967D2; height: 32px; padding: 0 0 0 0; width: 32px; } .colab-df-convert:hover { background-color: #E2EBFA; box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15); fill: #174EA6; } [theme=dark] .colab-df-convert { background-color: #3B4455; fill: #D2E3FC; } [theme=dark] .colab-df-convert:hover { background-color: #434B5C; box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15); filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3)); fill: #FFFFFF; } &lt;script&gt; const buttonEl = document.querySelector('#df-4e800a1d-593a-452f-8f87-30e7464779ba button.colab-df-convert'); buttonEl.style.display = google.colab.kernel.accessAllowed ? 'block' : 'none'; async function convertToInteractive(key) { const element = document.querySelector('#df-4e800a1d-593a-452f-8f87-30e7464779ba'); const dataTable = await google.colab.kernel.invokeFunction('convertToInteractive', [key], {}); if (!dataTable) return; const docLinkHtml = 'Like what you see? Visit the ' + '&lt;a target=&quot;_blank&quot; href=https://colab.research.google.com/notebooks/data_table.ipynb&gt;data table notebook&lt;/a&gt;' + ' to learn more about interactive tables.'; element.innerHTML = ''; dataTable['output_type'] = 'display_data'; await google.colab.output.renderOutput(dataTable, element); const docLink = document.createElement('div'); docLink.innerHTML = docLinkHtml; element.appendChild(docLink); } &lt;/script&gt; &lt;/div&gt; 시간 데이터 방법론 전처리 2 12temp_df['date'] = pd.to_datetime(temp_df['datetime'])temp_df['year'] = temp_df['date'].dt.year 123456789101112131415161718import time import datetime # 시간 테스트 start_time = time.time()temp_df['date'] = pd.to_datetime(temp_df['datetime'])temp_df['year'] = temp_df['date'].dt.yeartemp_df['month'] = temp_df['date'].dt.monthtemp_df['day'] = temp_df['date'].dt.daytemp_df['hour'] = temp_df['date'].dt.hourend_time = time.time() dt_ctime = end_time - start_timeprint(&quot;실행시간 (second) -&gt; &quot;, np.round(dt_ctime, 3))temp_df[['datetime', 'year', 'month', 'day', 'hour']] 실행시간 (second) -&gt; 0.013 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } datetime year month day hour 0 2011-01-01 00:00:00 2011 1 1 0 1 2011-01-01 01:00:00 2011 1 1 1 2 2011-01-01 02:00:00 2011 1 1 2 3 2011-01-01 03:00:00 2011 1 1 3 4 2011-01-01 04:00:00 2011 1 1 4 ... ... ... ... ... ... 10881 2012-12-19 19:00:00 2012 12 19 19 10882 2012-12-19 20:00:00 2012 12 19 20 10883 2012-12-19 21:00:00 2012 12 19 21 10884 2012-12-19 22:00:00 2012 12 19 22 10885 2012-12-19 23:00:00 2012 12 19 23 10886 rows × 5 columns &lt;svg xmlns=”http://www.w3.org/2000/svg&quot; height=”24px”viewBox=”0 0 24 24” width=”24px”&gt; .colab-df-container { display:flex; flex-wrap:wrap; gap: 12px; } .colab-df-convert { background-color: #E8F0FE; border: none; border-radius: 50%; cursor: pointer; display: none; fill: #1967D2; height: 32px; padding: 0 0 0 0; width: 32px; } .colab-df-convert:hover { background-color: #E2EBFA; box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15); fill: #174EA6; } [theme=dark] .colab-df-convert { background-color: #3B4455; fill: #D2E3FC; } [theme=dark] .colab-df-convert:hover { background-color: #434B5C; box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15); filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3)); fill: #FFFFFF; } &lt;script&gt; const buttonEl = document.querySelector('#df-60e2ba36-d212-44ef-bb9b-c70818ecf681 button.colab-df-convert'); buttonEl.style.display = google.colab.kernel.accessAllowed ? 'block' : 'none'; async function convertToInteractive(key) { const element = document.querySelector('#df-60e2ba36-d212-44ef-bb9b-c70818ecf681'); const dataTable = await google.colab.kernel.invokeFunction('convertToInteractive', [key], {}); if (!dataTable) return; const docLinkHtml = 'Like what you see? Visit the ' + '&lt;a target=&quot;_blank&quot; href=https://colab.research.google.com/notebooks/data_table.ipynb&gt;data table notebook&lt;/a&gt;' + ' to learn more about interactive tables.'; element.innerHTML = ''; dataTable['output_type'] = 'display_data'; await google.colab.output.renderOutput(dataTable, element); const docLink = document.createElement('div'); docLink.innerHTML = docLinkHtml; element.appendChild(docLink); } &lt;/script&gt; &lt;/div&gt; 1temp_df['date'] 0 2011-01-01 00:00:00 1 2011-01-01 01:00:00 2 2011-01-01 02:00:00 3 2011-01-01 03:00:00 4 2011-01-01 04:00:00 ... 10881 2012-12-19 19:00:00 10882 2012-12-19 20:00:00 10883 2012-12-19 21:00:00 10884 2012-12-19 22:00:00 10885 2012-12-19 23:00:00 Name: date, Length: 10886, dtype: datetime64[ns] 요일 추출하기 1temp_df['date'] 0 2011-01-01 00:00:00 1 2011-01-01 01:00:00 2 2011-01-01 02:00:00 3 2011-01-01 03:00:00 4 2011-01-01 04:00:00 ... 10881 2012-12-19 19:00:00 10882 2012-12-19 20:00:00 10883 2012-12-19 21:00:00 10884 2012-12-19 22:00:00 10885 2012-12-19 23:00:00 Name: date, Length: 10886, dtype: datetime64[ns] 12temp_df['weekday'] = temp_df['date'].dt.day_name()temp_df['weekday'] 0 Saturday 1 Saturday 2 Saturday 3 Saturday 4 Saturday ... 10881 Wednesday 10882 Wednesday 10883 Wednesday 10884 Wednesday 10885 Wednesday Name: weekday, Length: 10886, dtype: object 12345678temp_df['season'] = temp_df['season'].map({ 1: 'Spring', 2: 'Summer', 3: 'Fall', 4: 'Winter'})temp_df['season'] 0 Spring 1 Spring 2 Spring 3 Spring 4 Spring ... 10881 Winter 10882 Winter 10883 Winter 10884 Winter 10885 Winter Name: season, Length: 10886, dtype: object 12345678temp_df['weather'] = temp_df['weather'].map({ 1 : 'Clear', 2 : 'Few clouds', 3 : 'Light Snow, Rain', 4 : 'Heavy Snow, Rain'})temp_df['weather'] 0 Clear 1 Clear 2 Clear 3 Clear 4 Clear ... 10881 Clear 10882 Clear 10883 Clear 10884 Clear 10885 Clear Name: weather, Length: 10886, dtype: object 1temp_df.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } datetime season holiday workingday weather temp atemp humidity windspeed casual registered count date year month day hour weekday 0 2011-01-01 00:00:00 Spring 0 0 Clear 9.84 14.395 81 0.0 3 13 16 2011-01-01 00:00:00 2011 1 1 0 Saturday 1 2011-01-01 01:00:00 Spring 0 0 Clear 9.02 13.635 80 0.0 8 32 40 2011-01-01 01:00:00 2011 1 1 1 Saturday 2 2011-01-01 02:00:00 Spring 0 0 Clear 9.02 13.635 80 0.0 5 27 32 2011-01-01 02:00:00 2011 1 1 2 Saturday 3 2011-01-01 03:00:00 Spring 0 0 Clear 9.84 14.395 75 0.0 3 10 13 2011-01-01 03:00:00 2011 1 1 3 Saturday 4 2011-01-01 04:00:00 Spring 0 0 Clear 9.84 14.395 75 0.0 0 1 1 2011-01-01 04:00:00 2011 1 1 4 Saturday &lt;svg xmlns=”http://www.w3.org/2000/svg&quot; height=”24px”viewBox=”0 0 24 24” width=”24px”&gt; .colab-df-container { display:flex; flex-wrap:wrap; gap: 12px; } .colab-df-convert { background-color: #E8F0FE; border: none; border-radius: 50%; cursor: pointer; display: none; fill: #1967D2; height: 32px; padding: 0 0 0 0; width: 32px; } .colab-df-convert:hover { background-color: #E2EBFA; box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15); fill: #174EA6; } [theme=dark] .colab-df-convert { background-color: #3B4455; fill: #D2E3FC; } [theme=dark] .colab-df-convert:hover { background-color: #434B5C; box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15); filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3)); fill: #FFFFFF; } &lt;script&gt; const buttonEl = document.querySelector('#df-97103711-b9e3-40fa-a0bb-5e1055da6831 button.colab-df-convert'); buttonEl.style.display = google.colab.kernel.accessAllowed ? 'block' : 'none'; async function convertToInteractive(key) { const element = document.querySelector('#df-97103711-b9e3-40fa-a0bb-5e1055da6831'); const dataTable = await google.colab.kernel.invokeFunction('convertToInteractive', [key], {}); if (!dataTable) return; const docLinkHtml = 'Like what you see? Visit the ' + '&lt;a target=&quot;_blank&quot; href=https://colab.research.google.com/notebooks/data_table.ipynb&gt;data table notebook&lt;/a&gt;' + ' to learn more about interactive tables.'; element.innerHTML = ''; dataTable['output_type'] = 'display_data'; await google.colab.output.renderOutput(dataTable, element); const docLink = document.createElement('div'); docLink.innerHTML = docLinkHtml; element.appendChild(docLink); } &lt;/script&gt; &lt;/div&gt; step 04. 데이터 시각화 수치를 예측하는 대회 종속 변수를 시각화 해야 함. 분포 확인 후, 로그변환을 줄지 안줄지 결정 해야 함. 12345678910fig, ax = plt.subplots(nrows = 1, ncols = 2)sns.histplot(train['count'], ax = ax[0]) sns.histplot(np.log(train['count']), ax = ax[1])# 옵션 제목ax[0].set_title('Normal Graph')ax[1].set_title(&quot;Log Transformed Graph&quot;)plt.show() 막대 그래프 year, count month, count day, count hour, count 각 그래프의 타이틀 추가 123456789101112131415161718192021222324fig, ax = plt.subplots(nrows = 2, ncols = 2)## 1단계 : 전체 그래프 기본 설정# 그래프 사이 간격fig.tight_layout()# 전체 그래프 사이즈 관리fig.set_size_inches(10, 9)## 2단계 : 각 개별 그래프 입력sns.barplot(x = 'year', y = 'count', data = temp_df, ax=ax[0,0])sns.barplot(x = 'month',y = 'count', data = temp_df, ax=ax[0,1])sns.barplot(x = 'day', y = 'count', data = temp_df, ax=ax[1,0])sns.barplot(x = 'hour', y = 'count', data = temp_df, ax=ax[1,1])## 3단계 : 디테일 옵션ax[0, 0].set_title(&quot;Rental Amounts by Year&quot;)ax[0, 1].set_title(&quot;Rental Amounts by month&quot;)ax[1, 0].set_title(&quot;Rental Amounts by day&quot;)ax[1, 1].set_title(&quot;Rental Amounts by hour&quot;)ax[0, 0].tick_params(axis = 'x', labelrotation=90)plt.show() boxplot season, count weather, count holiday, count workingday, count 123456789101112131415161718192021222324fig, ax = plt.subplots(nrows = 2, ncols = 2)## 1단계 : 전체 그래프 기본 설정# 그래프 사이 간격fig.tight_layout()# 전체 그래프 사이즈 관리fig.set_size_inches(10, 9)## 2단계 : 각 개별 그래프 입력sns.boxplot(x = 'season', y = 'count', data = temp_df, ax=ax[0,0])sns.boxplot(x = 'weather',y = 'count', data = temp_df, ax=ax[0,1])sns.boxplot(x = 'holiday', y = 'count', data = temp_df, ax=ax[1,0])sns.boxplot(x = 'workingday', y = 'count', data = temp_df, ax=ax[1,1])## 3단계 : 디테일 옵션ax[0, 0].set_title(&quot;Box Plot On Count Across Season&quot;)ax[0, 1].set_title(&quot;Rental Amounts by month&quot;)ax[1, 0].set_title(&quot;Rental Amounts by day&quot;)ax[1, 1].set_title(&quot;Rental Amounts by hour&quot;)ax[0, 1].tick_params(axis = 'x', labelrotation=20)plt.show() 포인트 플롯 5개의 행 그래프 작성 workingday, holiday, weekday, season, weather 5개의 그래프를 한 이미지로 그리세요. 12345678910fig, ax = plt.subplots(nrows = 5)fig.set_size_inches(12, 18)sns.pointplot(x = 'hour', y = 'count', hue = 'workingday', data = temp_df, ax = ax[0])sns.pointplot(x = 'hour', y = 'count', hue = 'holiday', data = temp_df, ax = ax[1])sns.pointplot(x = 'hour', y = 'count', hue = 'weekday', data = temp_df, ax = ax[2])sns.pointplot(x = 'hour', y = 'count', hue = 'season', data = temp_df, ax = ax[3])sns.pointplot(x = 'hour', y = 'count', hue = 'weather', data = temp_df, ax = ax[4])plt.show() 회귀선을 포함한 산점도 그래프 x, y 모두 수치형 그래프여야 함. 연속형 수치형 데이터야 함. 총 4개의 그래프가 나와야 함. 1temp_df.info() &lt;class 'pandas.core.frame.DataFrame'&gt; RangeIndex: 10886 entries, 0 to 10885 Data columns (total 18 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 datetime 10886 non-null object 1 season 10886 non-null object 2 holiday 10886 non-null int64 3 workingday 10886 non-null int64 4 weather 10886 non-null object 5 temp 10886 non-null float64 6 atemp 10886 non-null float64 7 humidity 10886 non-null int64 8 windspeed 10886 non-null float64 9 casual 10886 non-null int64 10 registered 10886 non-null int64 11 count 10886 non-null int64 12 date 10886 non-null datetime64[ns] 13 year 10886 non-null int64 14 month 10886 non-null int64 15 day 10886 non-null int64 16 hour 10886 non-null int64 17 weekday 10886 non-null object dtypes: datetime64[ns](1), float64(3), int64(10), object(4) memory usage: 1.5+ MB 1temp_df['windspeed'].value_counts() 0.0000 1313 8.9981 1120 11.0014 1057 12.9980 1042 7.0015 1034 15.0013 961 6.0032 872 16.9979 824 19.0012 676 19.9995 492 22.0028 372 23.9994 274 26.0027 235 27.9993 187 30.0026 111 31.0009 89 32.9975 80 35.0008 58 39.0007 27 36.9974 22 43.0006 12 40.9973 11 43.9989 8 46.0022 3 56.9969 2 47.9988 2 51.9987 1 50.0021 1 Name: windspeed, dtype: int64 123456789fig, ax = plt.subplots(nrows=2, ncols =2)fig.tight_layout()sns.regplot(x ='temp', y ='count', data = temp_df, scatter_kws = {'alpha' : 0.2}, line_kws = {'color' : 'blue'}, ax = ax[0, 0])sns.regplot(x ='atemp', y ='count', data = temp_df, scatter_kws = {'alpha' : 0.2}, line_kws = {'color' : 'blue'}, ax = ax[0, 1])sns.regplot(x ='humidity', y ='count', data = temp_df, scatter_kws = {'alpha' : 0.2}, line_kws = {'color' : 'blue'}, ax = ax[1, 0])sns.regplot(x ='windspeed', y ='count', data = temp_df, scatter_kws = {'alpha' : 0.2}, line_kws = {'color' : 'blue'}, ax = ax[1, 1])plt.show() 히트맵 그래프 그리기 상관계수 해석 수치가 양수다 (양의 관계) 수치가 음수다 (음의 관계) 0~0.2 : 두 변수 사이의 상관관계는 없다. 0.2 ~ 1: 값이 커지면 커질수록 두 변수 간 상관관계는 크다. 12corrMat = temp_df[['temp', 'atemp', 'humidity', 'windspeed', 'count']].corr()corrMat .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } temp atemp humidity windspeed count temp 1.000000 0.984948 -0.064949 -0.017852 0.394454 atemp 0.984948 1.000000 -0.043536 -0.057473 0.389784 humidity -0.064949 -0.043536 1.000000 -0.318607 -0.317371 windspeed -0.017852 -0.057473 -0.318607 1.000000 0.101369 count 0.394454 0.389784 -0.317371 0.101369 1.000000 &lt;svg xmlns=”http://www.w3.org/2000/svg&quot; height=”24px”viewBox=”0 0 24 24” width=”24px”&gt; .colab-df-container { display:flex; flex-wrap:wrap; gap: 12px; } .colab-df-convert { background-color: #E8F0FE; border: none; border-radius: 50%; cursor: pointer; display: none; fill: #1967D2; height: 32px; padding: 0 0 0 0; width: 32px; } .colab-df-convert:hover { background-color: #E2EBFA; box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15); fill: #174EA6; } [theme=dark] .colab-df-convert { background-color: #3B4455; fill: #D2E3FC; } [theme=dark] .colab-df-convert:hover { background-color: #434B5C; box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15); filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3)); fill: #FFFFFF; } &lt;script&gt; const buttonEl = document.querySelector('#df-85088303-7931-4007-8f20-6de2c39d6b03 button.colab-df-convert'); buttonEl.style.display = google.colab.kernel.accessAllowed ? 'block' : 'none'; async function convertToInteractive(key) { const element = document.querySelector('#df-85088303-7931-4007-8f20-6de2c39d6b03'); const dataTable = await google.colab.kernel.invokeFunction('convertToInteractive', [key], {}); if (!dataTable) return; const docLinkHtml = 'Like what you see? Visit the ' + '&lt;a target=&quot;_blank&quot; href=https://colab.research.google.com/notebooks/data_table.ipynb&gt;data table notebook&lt;/a&gt;' + ' to learn more about interactive tables.'; element.innerHTML = ''; dataTable['output_type'] = 'display_data'; await google.colab.output.renderOutput(dataTable, element); const docLink = document.createElement('div'); docLink.innerHTML = docLinkHtml; element.appendChild(docLink); } &lt;/script&gt; &lt;/div&gt; 1sns.heatmap(corrMat, annot=True) &lt;matplotlib.axes._subplots.AxesSubplot at 0x7f96ce4d4fd0&gt; 데이터 전처리 첫번째 : train 데이터의 causal, registered 컬럼 제거할 것임 두번째 : 날짜 데이터 처리, dt.month, 이렇게 할 것 세번째 : season 컬럼 처리 필요 (숫자를 문자로 바꾸자) 인코딩 변환 (라벨인코딩, 원핫 인코딩) 네번째 : weather 컬럼 처리 필요 (숫자를 문자로 바꾸자) 인코딩 변환 (라벨인코딩, 원핫 인코딩) 다섯번째 : month, day 컬럼 삭제 예정 여섯번째 : weather 4인 데이터는 삭제 (이상치) 일곱번째 : windspeed 컬럼 삭제 여덟번째 : temp, atemp중 하나 삭제 (옵션) 마지막 : 모든 문자를 숫자로 인코딩 (원-핫 인코딩) 첫번째 train 데이터의 causal, registered 컬럼 제거 1train.info() &lt;class 'pandas.core.frame.DataFrame'&gt; RangeIndex: 10886 entries, 0 to 10885 Data columns (total 12 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 datetime 10886 non-null object 1 season 10886 non-null int64 2 holiday 10886 non-null int64 3 workingday 10886 non-null int64 4 weather 10886 non-null int64 5 temp 10886 non-null float64 6 atemp 10886 non-null float64 7 humidity 10886 non-null int64 8 windspeed 10886 non-null float64 9 casual 10886 non-null int64 10 registered 10886 non-null int64 11 count 10886 non-null int64 dtypes: float64(3), int64(8), object(1) memory usage: 1020.7+ KB 12train = train.drop(['casual', 'registered'], axis = 1)train.shape, test.shape ((10886, 10), (6493, 9)) weather 컬럼 지우기 4인 데이터는 삭제 12train = train[train['weather'] != 4].reset_index(drop=True)train.shape (10885, 10) 데이터 합치기 train, test 데이터 합치기 12all_data = pd.concat([train, test], ignore_index=True)all_data.info() &lt;class 'pandas.core.frame.DataFrame'&gt; RangeIndex: 17378 entries, 0 to 17377 Data columns (total 22 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 holiday 17378 non-null int64 1 workingday 17378 non-null int64 2 temp 17378 non-null float64 3 atemp 17378 non-null float64 4 humidity 17378 non-null int64 5 year 17378 non-null int64 6 hour 17378 non-null int64 7 season_Fall 17378 non-null uint8 8 season_Spring 17378 non-null uint8 9 season_Summer 17378 non-null uint8 10 season_Winter 17378 non-null uint8 11 weather_Clear 17378 non-null uint8 12 weather_Few clouds 17378 non-null uint8 13 weather_Heavy Snow, Rain 17378 non-null uint8 14 weather_Light Snow, Rain 17378 non-null uint8 15 weekday_Friday 17378 non-null uint8 16 weekday_Monday 17378 non-null uint8 17 weekday_Saturday 17378 non-null uint8 18 weekday_Sunday 17378 non-null uint8 19 weekday_Thursday 17378 non-null uint8 20 weekday_Tuesday 17378 non-null uint8 21 weekday_Wednesday 17378 non-null uint8 dtypes: float64(2), int64(5), uint8(15) memory usage: 1.2 MB 두번째 &amp; 다섯번째 날짜 데이터 처리, dt.month, 이렇게 할 것 month, day 컬럼 삭제 예정 123456all_data['date'] = pd.to_datetime(all_data['datetime'])all_data['year'] = all_data['date'].dt.yearall_data['hour'] = all_data['date'].dt.hourall_data['weekday'] = all_data['date'].dt.day_name()all_data.shape (17378, 14) 1all_data.info() &lt;class 'pandas.core.frame.DataFrame'&gt; RangeIndex: 17378 entries, 0 to 17377 Data columns (total 14 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 datetime 17378 non-null object 1 season 17378 non-null int64 2 holiday 17378 non-null int64 3 workingday 17378 non-null int64 4 weather 17378 non-null int64 5 temp 17378 non-null float64 6 atemp 17378 non-null float64 7 humidity 17378 non-null int64 8 windspeed 17378 non-null float64 9 count 10885 non-null float64 10 date 17378 non-null datetime64[ns] 11 year 17378 non-null int64 12 hour 17378 non-null int64 13 weekday 17378 non-null object dtypes: datetime64[ns](1), float64(4), int64(7), object(2) memory usage: 1.9+ MB 세번째 season 컬럼 처리 필요 (숫자를 문자로 바꾸자) 인코딩 변환 (라벨인코딩, 원핫 인코딩) 12345678all_data['season'] = all_data['season'].map({ 1: 'Spring', 2: 'Summer', 3: 'Fall', 4: 'Winter'})all_data.shape (17378, 14) 1all_data['season'] 0 Spring 1 Spring 2 Spring 3 Spring 4 Spring ... 17373 Spring 17374 Spring 17375 Spring 17376 Spring 17377 Spring Name: season, Length: 17378, dtype: object 네번째 weather 컬럼 처리 필요 (숫자를 문자로 바꾸자) 인코딩 변환 (라벨인코딩, 원핫 인코딩) 12345678all_data['weather'] = all_data['weather'].map({ 1 : 'Clear', 2 : 'Few clouds', 3 : 'Light Snow, Rain', 4 : 'Heavy Snow, Rain'})all_data.shape (17378, 14) 1all_data['weather'] 0 Clear 1 Clear 2 Clear 3 Clear 4 Clear ... 17373 Few clouds 17374 Few clouds 17375 Clear 17376 Clear 17377 Clear Name: weather, Length: 17378, dtype: object 다섯번째 weather 4인 데이터는 삭제 (이상치) 4 대신, ‘Heavy Snow, Rain’ 1all_data.shape (17378, 14) 1# all_data['weather'].value_counts() all_data에서 지우는 것이 아니라, train에서만 지워야 함. 12# all_data = all_data[all_data['weather'] != 'Heavy Snow, Rain']# all_data.shape 일곱번째 windspeed 컬럼 삭제 train만 제거 12all_data = all_data.drop('windspeed', axis = 1)all_data.shape (17378, 13) 1all_data.info() &lt;class 'pandas.core.frame.DataFrame'&gt; RangeIndex: 17378 entries, 0 to 17377 Data columns (total 13 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 datetime 17378 non-null object 1 season 17378 non-null object 2 holiday 17378 non-null int64 3 workingday 17378 non-null int64 4 weather 17378 non-null object 5 temp 17378 non-null float64 6 atemp 17378 non-null float64 7 humidity 17378 non-null int64 8 count 10885 non-null float64 9 date 17378 non-null datetime64[ns] 10 year 17378 non-null int64 11 hour 17378 non-null int64 12 weekday 17378 non-null object dtypes: datetime64[ns](1), float64(3), int64(5), object(4) memory usage: 1.7+ MB date 컬럼 제거1all_data.info() &lt;class 'pandas.core.frame.DataFrame'&gt; RangeIndex: 17378 entries, 0 to 17377 Data columns (total 13 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 datetime 17378 non-null object 1 season 17378 non-null object 2 holiday 17378 non-null int64 3 workingday 17378 non-null int64 4 weather 17378 non-null object 5 temp 17378 non-null float64 6 atemp 17378 non-null float64 7 humidity 17378 non-null int64 8 count 10885 non-null float64 9 date 17378 non-null datetime64[ns] 10 year 17378 non-null int64 11 hour 17378 non-null int64 12 weekday 17378 non-null object dtypes: datetime64[ns](1), float64(3), int64(5), object(4) memory usage: 1.7+ MB 12all_data = all_data.drop(['datetime', 'date'], axis = 1)all_data.shape (17378, 11) 원-핫 인코딩- 12all_data = pd.get_dummies(all_data).reset_index(drop=True)all_data.shape (17378, 23) 1all_data.info() &lt;class 'pandas.core.frame.DataFrame'&gt; RangeIndex: 17378 entries, 0 to 17377 Data columns (total 23 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 holiday 17378 non-null int64 1 workingday 17378 non-null int64 2 temp 17378 non-null float64 3 atemp 17378 non-null float64 4 humidity 17378 non-null int64 5 count 10885 non-null float64 6 year 17378 non-null int64 7 hour 17378 non-null int64 8 season_Fall 17378 non-null uint8 9 season_Spring 17378 non-null uint8 10 season_Summer 17378 non-null uint8 11 season_Winter 17378 non-null uint8 12 weather_Clear 17378 non-null uint8 13 weather_Few clouds 17378 non-null uint8 14 weather_Heavy Snow, Rain 17378 non-null uint8 15 weather_Light Snow, Rain 17378 non-null uint8 16 weekday_Friday 17378 non-null uint8 17 weekday_Monday 17378 non-null uint8 18 weekday_Saturday 17378 non-null uint8 19 weekday_Sunday 17378 non-null uint8 20 weekday_Thursday 17378 non-null uint8 21 weekday_Tuesday 17378 non-null uint8 22 weekday_Wednesday 17378 non-null uint8 dtypes: float64(3), int64(5), uint8(15) memory usage: 1.3 MB 데이터셋 나누기 훈련데이터와 테스트 데이터로 재 나누기 count 타깃 데이터(=종속 데이터) 타깃 데이터가 있으면 훈련 데이터 타깃 데이터가 없으면 테스트 데이터 1234train = all_data[~pd.isnull(all_data['count'])]test = all_data[pd.isnull(all_data['count'])]train.shape, test.shape ((10885, 23), (6493, 23)) count 컬럼을 제거해야 함 타깃 데이터만 y로 추출 train, test count 컬럼 제거 123456y = train['count'] # 타깃값 train = train.drop(['count'], axis = 1)test = test.drop(['count'], axis = 1)train.shape, test.shape ((10885, 22), (6493, 22)) 모델 훈련 LinearRegression 모형만 학습 12345678910111213from sklearn.linear_model import LinearRegression# train_test_splitlr_model = LinearRegression()# 모형 학습 전# 로그변환을 해준다. log_y = np.log(y)lr_model.fit(train, log_y)# 모형 예측lr_preds = lr_model.predict(test)lr_preds[:10] array([2.37067025, 2.51692861, 2.61965969, 2.70791364, 2.81064472, 2.79551553, 2.85117928, 3.02430512, 3.12703621, 3.47246024]) 모형 예측123# 지수변환final_preds = np.exp(lr_preds)final_preds array([ 10.70456458, 12.39048215, 13.73105001, ..., 133.25111439, 158.50077047, 154.745211 ]) 제출12submission['count'] = final_preds submission.to_csv('submission.csv', index=False) 1submission.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } datetime count 0 2011-01-20 00:00:00 10.704565 1 2011-01-20 01:00:00 12.390482 2 2011-01-20 02:00:00 13.731050 3 2011-01-20 03:00:00 14.997952 4 2011-01-20 04:00:00 16.620630 &lt;svg xmlns=”http://www.w3.org/2000/svg&quot; height=”24px”viewBox=”0 0 24 24” width=”24px”&gt; .colab-df-container { display:flex; flex-wrap:wrap; gap: 12px; } .colab-df-convert { background-color: #E8F0FE; border: none; border-radius: 50%; cursor: pointer; display: none; fill: #1967D2; height: 32px; padding: 0 0 0 0; width: 32px; } .colab-df-convert:hover { background-color: #E2EBFA; box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15); fill: #174EA6; } [theme=dark] .colab-df-convert { background-color: #3B4455; fill: #D2E3FC; } [theme=dark] .colab-df-convert:hover { background-color: #434B5C; box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15); filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3)); fill: #FFFFFF; } &lt;script&gt; const buttonEl = document.querySelector('#df-4e6175ca-0693-4468-b21f-0f9394ab9a74 button.colab-df-convert'); buttonEl.style.display = google.colab.kernel.accessAllowed ? 'block' : 'none'; async function convertToInteractive(key) { const element = document.querySelector('#df-4e6175ca-0693-4468-b21f-0f9394ab9a74'); const dataTable = await google.colab.kernel.invokeFunction('convertToInteractive', [key], {}); if (!dataTable) return; const docLinkHtml = 'Like what you see? Visit the ' + '&lt;a target=&quot;_blank&quot; href=https://colab.research.google.com/notebooks/data_table.ipynb&gt;data table notebook&lt;/a&gt;' + ' to learn more about interactive tables.'; element.innerHTML = ''; dataTable['output_type'] = 'display_data'; await google.colab.output.renderOutput(dataTable, element); const docLink = document.createElement('div'); docLink.innerHTML = docLinkHtml; element.appendChild(docLink); } &lt;/script&gt; &lt;/div&gt;","link":"/2022/07/08/day0708/"},{"title":"","text":"프로젝트 개요 강의명 : 2022년 K-디지털 직업훈련(Training) 사업 - AI데이터플랫폼을 활용한 빅데이터 분석전문가 과정 교과목명 : 빅데이터 분석 및 시각화, AI개발 기초, 인공지능 프로그래밍 프로젝트 주제 : 캐글 대회 Bike Sharing Demand 데이터를 활용한 수요 예측 대회 프로젝트 마감일 : 2022년 7월 19일 화요일 수강생명 : 채제형 12from google.colab import drive drive.mount(&quot;/content/drive&quot;) Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(&quot;/content/drive&quot;, force_remount=True). 1234567891011121314151617# This Python 3 environment comes with many helpful analytics libraries installed# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python# For example, here's several helpful packages to loadimport numpy as np # linear algebraimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)# Input data files are available in the read-only &quot;../input/&quot; directory# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directoryimport osfor dirname, _, filenames in os.walk('/kaggle/input'): for filename in filenames: print(os.path.join(dirname, filename))# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using &quot;Save &amp; Run All&quot; # You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session STEP 1. 필수 라이브러리 불러오기1234567891011import pandas as pd # 데이터 가공import numpy as np # 수치 연산import matplotlib as mpl # 시각화 import matplotlib.pyplot as plt %matplotlib inlineimport seaborn as sns # 시각화 import sklearn # 머신러닝# 버전 확인print(&quot;pandas version :&quot;, pd.__version__)print(&quot;numpy version :&quot;, np.__version__) pandas version : 1.3.5 numpy version : 1.21.6 STEP 2. 데이터 불러오기123456DATA_PATH = '/content/drive/MyDrive/Colab Notebooks/Human_ai/lecture/bike/'train = pd.read_csv(DATA_PATH +'train.csv')test = pd.read_csv(DATA_PATH + 'test.csv')submission = pd.read_csv(DATA_PATH + 'sampleSubmission.csv')print(&quot;데이터 불러오기 완료!~&quot;) 데이터 불러오기 완료!~ STEP 3.데이터 둘러보기1train.shape, test.shape ((10886, 12), (6493, 9)) 1train.info() # 12개의 변수 &lt;class 'pandas.core.frame.DataFrame'&gt; RangeIndex: 10886 entries, 0 to 10885 Data columns (total 12 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 datetime 10886 non-null object 1 season 10886 non-null int64 2 holiday 10886 non-null int64 3 workingday 10886 non-null int64 4 weather 10886 non-null int64 5 temp 10886 non-null float64 6 atemp 10886 non-null float64 7 humidity 10886 non-null int64 8 windspeed 10886 non-null float64 9 casual 10886 non-null int64 10 registered 10886 non-null int64 11 count 10886 non-null int64 dtypes: float64(3), int64(8), object(1) memory usage: 1020.7+ KB 1test.info() &lt;class 'pandas.core.frame.DataFrame'&gt; RangeIndex: 6493 entries, 0 to 6492 Data columns (total 9 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 datetime 6493 non-null object 1 season 6493 non-null int64 2 holiday 6493 non-null int64 3 workingday 6493 non-null int64 4 weather 6493 non-null int64 5 temp 6493 non-null float64 6 atemp 6493 non-null float64 7 humidity 6493 non-null int64 8 windspeed 6493 non-null float64 dtypes: float64(3), int64(5), object(1) memory usage: 456.7+ KB 1target = train['count'] # 타겟변수 확인 1sum(train.duplicated())# 데이터 중복은 없어보인다. 0 1train.isnull().sum()# 결측치는 없어 보인다. datetime 0 season 0 holiday 0 workingday 0 weather 0 temp 0 atemp 0 humidity 0 windspeed 0 casual 0 registered 0 count 0 dtype: int64 변수의 내용을 파악해 보면 datetime 시간별 날자(-&gt;수치화 시킬 필요가 있다) season 계절 :봄(1)여름(2)가을(3)겨울(4) holiday 휴일 workingday 평일 -&gt; 평일도 휴일도 아닌 날이 존재하게 된다.이를 주말로 정의하자. weather 날씨 : 맑음(1),흐림(2),작은비(3),폭우(4) temp (섭씨)온도 atemp 체감온도 humidity (상대)습도 windspeed 풍속 casual 미등록 사용자 대여수 registered 등록된 사용자 대여수 count 총대여수(미등록자+등록자) 123# 비회원(casual)과 회원(registered)의 대여량의 비율을 확인해본다.print('미등록자수의 비율:',round((train['casual'].sum()/train['count'].sum())*100,2))print('등록자수의 비율:',round((train['registered'].sum()/train['count'].sum())*100,2)) 미등록자수의 비율: 18.8 등록자수의 비율: 81.2 STEP 4. 탐색적 자료 분석 복제본 사용 시간변수를 수치형 변수로 변환 시각화 타겟 변수의 상태 확인 수치형 변수와 대여량 간의 관계(온도,체감온도,습도,풍속) 범주형 변수와 대여량 간의 관계(시간,날씨,계절,일하는날,휴일) 1234temp_train = train.copy()temp_test = test.copy()temp_target = target.copy()# 복제본 완료temp_train.shape, temp_test.shape, temp_target.shape ((10886, 12), (6493, 9), (10886,)) 12345678# 시간관련 변수를 수치형 변수로 바꿔준다.temp_train['date'] = pd.to_datetime(temp_train['datetime'])temp_train['year'] = temp_train['date'].dt.yeartemp_train['month'] = temp_train['date'].dt.monthtemp_train['day'] = temp_train['date'].dt.daytemp_train['hour'] = temp_train['date'].dt.hour#분과 초는 0값이기에 필요가 없어서 생략함temp_train.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } datetime season holiday workingday weather temp atemp humidity windspeed casual registered count date year month day hour 0 2011-01-01 00:00:00 1 0 0 1 9.84 14.395 81 0.0 3 13 16 2011-01-01 00:00:00 2011 1 1 0 1 2011-01-01 01:00:00 1 0 0 1 9.02 13.635 80 0.0 8 32 40 2011-01-01 01:00:00 2011 1 1 1 2 2011-01-01 02:00:00 1 0 0 1 9.02 13.635 80 0.0 5 27 32 2011-01-01 02:00:00 2011 1 1 2 3 2011-01-01 03:00:00 1 0 0 1 9.84 14.395 75 0.0 3 10 13 2011-01-01 03:00:00 2011 1 1 3 4 2011-01-01 04:00:00 1 0 0 1 9.84 14.395 75 0.0 0 1 1 2011-01-01 04:00:00 2011 1 1 4 &lt;svg xmlns=”http://www.w3.org/2000/svg&quot; height=”24px”viewBox=”0 0 24 24” width=”24px”&gt; .colab-df-container { display:flex; flex-wrap:wrap; gap: 12px; } .colab-df-convert { background-color: #E8F0FE; border: none; border-radius: 50%; cursor: pointer; display: none; fill: #1967D2; height: 32px; padding: 0 0 0 0; width: 32px; } .colab-df-convert:hover { background-color: #E2EBFA; box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15); fill: #174EA6; } [theme=dark] .colab-df-convert { background-color: #3B4455; fill: #D2E3FC; } [theme=dark] .colab-df-convert:hover { background-color: #434B5C; box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15); filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3)); fill: #FFFFFF; } &lt;script&gt; const buttonEl = document.querySelector('#df-5f3c05a7-4906-4dd1-8371-266fd67341a3 button.colab-df-convert'); buttonEl.style.display = google.colab.kernel.accessAllowed ? 'block' : 'none'; async function convertToInteractive(key) { const element = document.querySelector('#df-5f3c05a7-4906-4dd1-8371-266fd67341a3'); const dataTable = await google.colab.kernel.invokeFunction('convertToInteractive', [key], {}); if (!dataTable) return; const docLinkHtml = 'Like what you see? Visit the ' + '&lt;a target=&quot;_blank&quot; href=https://colab.research.google.com/notebooks/data_table.ipynb&gt;data table notebook&lt;/a&gt;' + ' to learn more about interactive tables.'; element.innerHTML = ''; dataTable['output_type'] = 'display_data'; await google.colab.output.renderOutput(dataTable, element); const docLink = document.createElement('div'); docLink.innerHTML = docLinkHtml; element.appendChild(docLink); } &lt;/script&gt; &lt;/div&gt; 123456789# 타겟변수(총대여수)를 확인하여 시각화를 해봤다fig, ax = plt.subplots(nrows = 1, ncols =2, figsize = (10,6))sns.histplot(temp_train['count'],ax = ax[0])sns.histplot(np.log(temp_train['count']),ax = ax[1])ax[0].set_title('Normal Graph')ax[1].set_title('Log Graph')plt.show() -&gt; 시각화 결과 왼쪽으로 치우쳐졌기에 로그함수를 이용 정규분포와 비슷하게 만들었다. 1.수치형 변수와 대여량 간의 관계 온도,체감온도,습도,풍속은 연속형 변수로 총대여수와의 관계를 살펴보기 위해 상관계수 히트맵 그래프를 그려본다. 12345plt.figure(figsize=(12,10))plt.title(&quot;Weather Correlation of Features&quot;,y= 1, size = 15)coorMat = temp_train[['temp','atemp','humidity','windspeed','casual','registered','count']].corr()sns.heatmap(coorMat, annot = True)plt.show() 총대여수(count)와 상관성을 갖는 값을 나열해보면 온도 및 체감온도(0.39)&gt;습도(-0.32)&gt;풍속(0.1)이였다. 이는 온도 및 체감온도는 총대여수와 정비례하고 습도는 반비례 풍속은 미미한 정비례 관계이다. 온도와 체감온도는 중복이 된다고 할 수 있다.둘 중 하나는 생략하자. casual과 registered가 온도,습도에서 차이가 나는 것은 아마도 casual(비회원)이 날씨에 더 민감함을 알 수 있다. 온도, 체감온도, 습도, 풍속의 산점도와 회귀선을 그려 더 구체화 시켜보자. 123456789101112131415161718# 산점도 플렛plt. rc('font',size =8)fig, ax = plt.subplots(nrows = 2, ncols = 2, figsize= (10,9))# 2단계 : 각 개별 그래프 입력sns.regplot(x = 'temp', y = 'count', data = temp_train, scatter_kws = {'alpha':0.2}, line_kws = {'color':'red'},ax=ax[0,0])sns.regplot(x = 'atemp',y = 'count', data = temp_train,scatter_kws = {'alpha':0.2}, line_kws = {'color':'red'}, ax=ax[0,1])sns.regplot(x = 'humidity', y = 'count', data = temp_train,scatter_kws = {'alpha':0.2}, line_kws = {'color':'red'}, ax=ax[1,0])sns.regplot(x = 'windspeed', y = 'count', data = temp_train, scatter_kws = {'alpha':0.2}, line_kws = {'color':'red'},ax=ax[1,1])# 3단계 : 디테일 옵션ax[0, 0].set_title(&quot;Temp&quot;)ax[0, 1].set_title(&quot;Atemp&quot;)ax[1, 0].set_title(&quot;Humidity&quot;)ax[1, 1].set_title(&quot;Windspeed&quot;)plt.show() windspeed에서 풍속이 0인 수치가 많고 약 6까지 수치가 없는 것으로 봐서 6이하를 0으로 표시한 것 같다. 풍속이 0인 수치를 평균값으로 대체하도록 하자. 12plt.figure(figsize=(14,3))sns.countplot(data = temp_train, x = &quot;windspeed&quot;) # 윈드스피드의 수치 확인 &lt;matplotlib.axes._subplots.AxesSubplot at 0x7f5ba4f7a850&gt; 2.범주형 변수와 총대여수와의 관계 시간(year,month,day,hour) 날씨(weather) 계절(season) 평일(workingday), 휴일(holiday) 를 시각화하자(그래프로 그려보자). 123456789101112131415# 시간과 관련 그래프를 그려보자.fig, ax = plt.subplots(nrows = 2, ncols = 2)fig.tight_layout()fig.set_size_inches(10, 9)sns.barplot(x = 'year', y = 'count', data = temp_train, ax=ax[0,0])sns.barplot(x = 'month',y = 'count', data = temp_train, ax=ax[0,1])sns.barplot(x = 'day', y = 'count', data = temp_train, ax=ax[1,0])sns.barplot(x = 'hour', y = 'count', data = temp_train, ax=ax[1,1])ax[0, 0].set_title(&quot;Rental Amounts by Year&quot;)ax[0, 1].set_title(&quot;Rental Amounts by month&quot;)ax[1, 0].set_title(&quot;Rental Amounts by day&quot;)ax[1, 1].set_title(&quot;Rental Amounts by hour&quot;)plt.show() 그래프를 해석해보면 2012년도가 전년에 비해 대여량이 늘었다 겨울보다는 여름에 더 대여량이 높다. 날짜별로는 특별한 차이가 없기에 데이터로서 가치가 없다(제거) 출퇴근 시간으로 보이는 오전 8시와 오후 5-6시에 대여량이 높다. 123456789101112# 날씨 관련 그래프를 그려보자.fig, ax = plt.subplots(nrows = 1, ncols = 3,figsize=(10,3))fig.tight_layout()sns.barplot(x = 'weather', y = 'casual', data = temp_train, ax=ax[0])sns.barplot(x = 'weather',y = 'registered', data = temp_train, ax=ax[1])sns.barplot(x = 'weather', y = 'count', data = temp_train, ax=ax[2])ax[0].set_title(&quot;Causal&quot;)ax[1].set_title(&quot;Registered&quot;)ax[2].set_title(&quot;Count=C+R&quot;)plt.show() 그래프가 이상하다. 안좋은 날씨 데이터(4)는 적은데 높게 나와있다. 평균치로 나타냈기 때문일 것이다.이를 해결하기 위해 y축을 합계로 계산해보자. 1(temp_train.groupby('weather')[['datetime']].count()/len(temp_train))*100 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } datetime weather 1 66.066507 2 26.033437 3 7.890869 4 0.009186 &lt;svg xmlns=”http://www.w3.org/2000/svg&quot; height=”24px”viewBox=”0 0 24 24” width=”24px”&gt; .colab-df-container { display:flex; flex-wrap:wrap; gap: 12px; } .colab-df-convert { background-color: #E8F0FE; border: none; border-radius: 50%; cursor: pointer; display: none; fill: #1967D2; height: 32px; padding: 0 0 0 0; width: 32px; } .colab-df-convert:hover { background-color: #E2EBFA; box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15); fill: #174EA6; } [theme=dark] .colab-df-convert { background-color: #3B4455; fill: #D2E3FC; } [theme=dark] .colab-df-convert:hover { background-color: #434B5C; box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15); filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3)); fill: #FFFFFF; } &lt;script&gt; const buttonEl = document.querySelector('#df-343e42f8-4a2f-4ca7-b2b0-ab2b95d24e1f button.colab-df-convert'); buttonEl.style.display = google.colab.kernel.accessAllowed ? 'block' : 'none'; async function convertToInteractive(key) { const element = document.querySelector('#df-343e42f8-4a2f-4ca7-b2b0-ab2b95d24e1f'); const dataTable = await google.colab.kernel.invokeFunction('convertToInteractive', [key], {}); if (!dataTable) return; const docLinkHtml = 'Like what you see? Visit the ' + '&lt;a target=&quot;_blank&quot; href=https://colab.research.google.com/notebooks/data_table.ipynb&gt;data table notebook&lt;/a&gt;' + ' to learn more about interactive tables.'; element.innerHTML = ''; dataTable['output_type'] = 'display_data'; await google.colab.output.renderOutput(dataTable, element); const docLink = document.createElement('div'); docLink.innerHTML = docLinkHtml; element.appendChild(docLink); } &lt;/script&gt; &lt;/div&gt; 123W_train = temp_train.groupby('weather')[['count']].sum().reset_index()Wc_train = temp_train.groupby('weather')[['casual']].sum().reset_index()Wr_train = temp_train.groupby('weather')[['registered']].sum().reset_index() 12345678910fig, ax = plt.subplots(nrows = 1, ncols = 3,figsize=(12,3))# fig.tight_layout()sns.barplot(x = 'weather', y = 'casual', data = Wc_train, ax=ax[0])sns.barplot(x = 'weather',y = 'registered', data = Wr_train, ax=ax[1])sns.barplot(x = 'weather', y = 'count', data = W_train, ax=ax[2])ax[0].set_title(&quot;Causal&quot;)ax[1].set_title(&quot;Registered&quot;)ax[2].set_title(&quot;Count=C+R&quot;) Text(0.5, 1.0, 'Count=C+R') 1234567891011# 계절 관련 그래프를 그려보자fig, ax = plt.subplots(nrows = 1, ncols = 3,figsize=(12,3))sns.barplot(x = 'season', y = 'casual', data = temp_train, ax=ax[0])sns.barplot(x = 'season',y = 'registered', data = temp_train, ax=ax[1])sns.barplot(x = 'season', y = 'count', data = temp_train, ax=ax[2])ax[0].set_title(&quot;Causal&quot;)ax[1].set_title(&quot;Registered&quot;)ax[2].set_title(&quot;Count=C+R&quot;) Text(0.5, 1.0, 'Count=C+R') season과 month는 비슷한 양상의 그래프이다. 실제로 month를 3개월 단위로 자른 것이기에 비슷한 변수가 머신러닝 모델에 중복해서 들어가면 예측 성는이 안 좋아지므로 변수 season은 생략하는 것이 좋겠다. 1234567891011# workingday와 holyday를 그래프로 그려보자# 여기서 workingday는 평일 holyday는 공휴일로 봤을 때 둘다 아닌 날은 주말(weekend)로 정의한다.temp_train['day_type'] = 'weekend'is_holi = (train['holiday'] == 1)is_week = (train['workingday'] == 1)temp_train.loc[(is_holi),'day_type'] = 'holiday'temp_train.loc[(is_week),'day_type'] = 'workingday'work_holi = (temp_train.groupby('day_type')[['casual','registered','count']].sum()/ temp_train.groupby('day_type')[['casual','registered','count']].count())temp_train.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } datetime season holiday workingday weather temp atemp humidity windspeed casual registered count date year month day hour day_type 0 2011-01-01 00:00:00 1 0 0 1 9.84 14.395 81 0.0 3 13 16 2011-01-01 00:00:00 2011 1 1 0 weekend 1 2011-01-01 01:00:00 1 0 0 1 9.02 13.635 80 0.0 8 32 40 2011-01-01 01:00:00 2011 1 1 1 weekend 2 2011-01-01 02:00:00 1 0 0 1 9.02 13.635 80 0.0 5 27 32 2011-01-01 02:00:00 2011 1 1 2 weekend 3 2011-01-01 03:00:00 1 0 0 1 9.84 14.395 75 0.0 3 10 13 2011-01-01 03:00:00 2011 1 1 3 weekend 4 2011-01-01 04:00:00 1 0 0 1 9.84 14.395 75 0.0 0 1 1 2011-01-01 04:00:00 2011 1 1 4 weekend &lt;svg xmlns=”http://www.w3.org/2000/svg&quot; height=”24px”viewBox=”0 0 24 24” width=”24px”&gt; .colab-df-container { display:flex; flex-wrap:wrap; gap: 12px; } .colab-df-convert { background-color: #E8F0FE; border: none; border-radius: 50%; cursor: pointer; display: none; fill: #1967D2; height: 32px; padding: 0 0 0 0; width: 32px; } .colab-df-convert:hover { background-color: #E2EBFA; box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15); fill: #174EA6; } [theme=dark] .colab-df-convert { background-color: #3B4455; fill: #D2E3FC; } [theme=dark] .colab-df-convert:hover { background-color: #434B5C; box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15); filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3)); fill: #FFFFFF; } &lt;script&gt; const buttonEl = document.querySelector('#df-c3150710-c4d4-4b72-8754-d26e6f327bc3 button.colab-df-convert'); buttonEl.style.display = google.colab.kernel.accessAllowed ? 'block' : 'none'; async function convertToInteractive(key) { const element = document.querySelector('#df-c3150710-c4d4-4b72-8754-d26e6f327bc3'); const dataTable = await google.colab.kernel.invokeFunction('convertToInteractive', [key], {}); if (!dataTable) return; const docLinkHtml = 'Like what you see? Visit the ' + '&lt;a target=&quot;_blank&quot; href=https://colab.research.google.com/notebooks/data_table.ipynb&gt;data table notebook&lt;/a&gt;' + ' to learn more about interactive tables.'; element.innerHTML = ''; dataTable['output_type'] = 'display_data'; await google.colab.output.renderOutput(dataTable, element); const docLink = document.createElement('div'); docLink.innerHTML = docLinkHtml; element.appendChild(docLink); } &lt;/script&gt; &lt;/div&gt; 1234#시각화plt.plot(work_holi)plt.legend(work_holi.columns)plt.show() 사용량은 등록자 대여수(registered)가 비등록자 대여수(casual)보다 많다. 평일(workingday)에 등록자 대여수가 가장 높고 비등록자 대여수가 적은 것을 볼 수 있다. 1234# 데이터 나누기h1 = temp_train.loc[temp_train['day_type'] == 'workingday']h2 = temp_train.loc[temp_train['day_type'] == 'weekend']h3 = temp_train.loc[temp_train['day_type'] == 'holiday'] 123456789# 평일에 대해 -시간대별 평균대여량h1_df = h1.groupby('hour')[['casual','registered','count']].mean()plt.figure(figsize=(10,4))plt.plot(h1_df)plt.xticks(np.arange(0,25))plt.legend(h1_df.columns)plt.title('workingday')plt.show() 123456789# 주말에 대해 -시간대별 평균대여량h2_df = h2.groupby('hour')[['casual','registered','count']].mean()plt.figure(figsize=(10,4))plt.plot(h2_df)plt.xticks(np.arange(0,25))plt.legend(h2_df.columns)plt.title('Weekend')plt.show() 123456789# 공휴일에 대해 -시간대별 평균대여량h3_df = h3.groupby('hour')[['casual','registered','count']].mean()plt.figure(figsize=(10,4))plt.plot(h3_df)plt.xticks(np.arange(0,25))plt.legend(h3_df.columns)plt.title('Holiday')plt.show() 12345678# 모든 그래프를 한번에 보면h_all = temp_train.pivot_table(index ='hour',columns = 'day_type', values = 'count', aggfunc='mean')plt.figure(figsize=(10,4))plt.plot(h_all)plt.xticks(np.arange(0,25))plt.legend(h_all.columns)plt.title('All')plt.show() 평일에는 출퇴근 시간인 오전 8시와 오후 5-6시에 많이 이용되고 주말과 공휴일에는 낮시간에 대다수 이용된다. 공휴일에는 놀러가는 사람이 많은지 출퇴근 시간만큼은 아니지만 많이 이용되는 시간대가 있다. 1234567891011121314151617fig, ax = plt.subplots(nrows = 2, ncols = 2)fig.tight_layout()fig.set_size_inches(10, 9)sns.boxplot(x = 'weather', y = 'count', data = temp_train, ax=ax[0,0])sns.boxplot(x = 'season',y = 'count', data = temp_train, ax=ax[0,1])sns.boxplot(x = 'holiday', y = 'count', data = temp_train, ax=ax[1,0])sns.boxplot(x = 'workingday', y = 'count', data = temp_train, ax=ax[1,1])ax[0, 0].set_title(&quot;Box Plot On Count weather&quot;)ax[0, 1].set_title(&quot;Box Plot On Count Across Season&quot;)ax[1, 0].set_title(&quot;Rental Amounts by Holiday&quot;)ax[1, 1].set_title(&quot;Rental Amounts by Workingday&quot;)ax[0, 1].tick_params(axis = 'x', labelrotation=20)plt.show() weather 데이터에서 4번은 반드시 처리해야 한다. 1temp_train['weekday'] = temp_train['date'].dt.day_name() # 요일 추출하기 12345678910fig, ax = plt.subplots(nrows = 5)fig.set_size_inches(12, 18)sns.pointplot(x = 'hour', y = 'count', hue = 'workingday', data = temp_train, ax = ax[0])sns.pointplot(x = 'hour', y = 'count', hue = 'holiday', data = temp_train, ax = ax[1])sns.pointplot(x = 'hour', y = 'count', hue = 'weekday', data = temp_train, ax = ax[2])sns.pointplot(x = 'hour', y = 'count', hue = 'season', data = temp_train, ax = ax[3])sns.pointplot(x = 'hour', y = 'count', hue = 'weather', data = temp_train, ax = ax[4])plt.show() 3.시각화의 결론 causal과 registered의 합계가 target값(count)이므로 비례관계가 성립하기에 제거한다. 수치형 변수 온도(temp,atemp)는 정비례 관계가 습도(humidity)는 반비례 관계가 나타나며 풍향(windspeed)는 정비례관계가 너무 작기에 제거(또는 0의 값 보완)한다. 범주형 변수 시간 변수는 다른 변수들과의 대여량 간 관계를 설명해주는 강력한 변수이다 날씨(weather)는 값이 4인 것은 이상치 이므로 제거한다. 계절(season)은 달(month)와 같은 변수에서 나왔기에 단순한 계절(season)만 남긴다. 타깃(target) count STEP5. 데이터 전처리 datetime으로부터 필요한 열 생성 : hour, dayofweek(month나 day는 의미없다) causal과 registered 컬럼 제거 weather의 4인 값 제거 windspeed의 변수 보완 count의 아웃라이어 제거(3시그마를 초과하는 값을 이상치로 간주하고 제거) atemp제거(temp와 별차이가 없다) target인 count의 열 로그변환 12345678# datetime으로부터 필요한 열 생성 : hour, dayofweek(month나 day는 의미없다)train['date'] = pd.to_datetime(train['datetime'])train['year'] = train['date'].dt.yeartrain['hour'] = train['date'].dt.hourtrain['dayofweek'] = train['date'].dt.dayofweek# 불필요한 열 삭제del train ['datetime'] 12del train ['date']display(train.head()) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } season holiday workingday weather temp atemp humidity windspeed casual registered count year hour dayofweek 0 1 0 0 1 9.84 14.395 81 0.0 3 13 16 2011 0 5 1 1 0 0 1 9.02 13.635 80 0.0 8 32 40 2011 1 5 2 1 0 0 1 9.02 13.635 80 0.0 5 27 32 2011 2 5 3 1 0 0 1 9.84 14.395 75 0.0 3 10 13 2011 3 5 4 1 0 0 1 9.84 14.395 75 0.0 0 1 1 2011 4 5 &lt;svg xmlns=”http://www.w3.org/2000/svg&quot; height=”24px”viewBox=”0 0 24 24” width=”24px”&gt; .colab-df-container { display:flex; flex-wrap:wrap; gap: 12px; } .colab-df-convert { background-color: #E8F0FE; border: none; border-radius: 50%; cursor: pointer; display: none; fill: #1967D2; height: 32px; padding: 0 0 0 0; width: 32px; } .colab-df-convert:hover { background-color: #E2EBFA; box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15); fill: #174EA6; } [theme=dark] .colab-df-convert { background-color: #3B4455; fill: #D2E3FC; } [theme=dark] .colab-df-convert:hover { background-color: #434B5C; box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15); filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3)); fill: #FFFFFF; } &lt;script&gt; const buttonEl = document.querySelector('#df-1bbf8d85-9f45-45d1-9c2c-f20d16cf8b0a button.colab-df-convert'); buttonEl.style.display = google.colab.kernel.accessAllowed ? 'block' : 'none'; async function convertToInteractive(key) { const element = document.querySelector('#df-1bbf8d85-9f45-45d1-9c2c-f20d16cf8b0a'); const dataTable = await google.colab.kernel.invokeFunction('convertToInteractive', [key], {}); if (!dataTable) return; const docLinkHtml = 'Like what you see? Visit the ' + '&lt;a target=&quot;_blank&quot; href=https://colab.research.google.com/notebooks/data_table.ipynb&gt;data table notebook&lt;/a&gt;' + ' to learn more about interactive tables.'; element.innerHTML = ''; dataTable['output_type'] = 'display_data'; await google.colab.output.renderOutput(dataTable, element); const docLink = document.createElement('div'); docLink.innerHTML = docLinkHtml; element.appendChild(docLink); } &lt;/script&gt; &lt;/div&gt; 123# causal과 registered 컬럼 제거train = train.drop(['casual','registered'],axis = 1)train.shape, test.shape ((10886, 12), (6493, 9)) 123#1 weather 4값 제거train = train[train['weather']!=4].reset_index(drop = True)train.shape, test.shape# test값을 건딜면 제출이 안되니 그냥 보존 ((10885, 12), (6493, 9)) 12345# count의 아웃라이어 제거(3시그마를 초과하는 값을 이상치로 간주하고 제거)train = train[train['count'] - train['count'].mean()&lt; 3*train['count'].std()]train.reset_index(inplace = True, drop = True)print(train.shape) (10738, 12) 123# target인 count열의 로그변환train['count'] = np.log1p(train['count'])display(train.head()) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } season holiday workingday weather temp atemp humidity windspeed count year hour dayofweek 0 1 0 0 1 9.84 14.395 81 0.0 2.833213 2011 0 5 1 1 0 0 1 9.02 13.635 80 0.0 3.713572 2011 1 5 2 1 0 0 1 9.02 13.635 80 0.0 3.496508 2011 2 5 3 1 0 0 1 9.84 14.395 75 0.0 2.639057 2011 3 5 4 1 0 0 1 9.84 14.395 75 0.0 0.693147 2011 4 5 &lt;svg xmlns=”http://www.w3.org/2000/svg&quot; height=”24px”viewBox=”0 0 24 24” width=”24px”&gt; .colab-df-container { display:flex; flex-wrap:wrap; gap: 12px; } .colab-df-convert { background-color: #E8F0FE; border: none; border-radius: 50%; cursor: pointer; display: none; fill: #1967D2; height: 32px; padding: 0 0 0 0; width: 32px; } .colab-df-convert:hover { background-color: #E2EBFA; box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15); fill: #174EA6; } [theme=dark] .colab-df-convert { background-color: #3B4455; fill: #D2E3FC; } [theme=dark] .colab-df-convert:hover { background-color: #434B5C; box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15); filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3)); fill: #FFFFFF; } &lt;script&gt; const buttonEl = document.querySelector('#df-444f55fc-3233-4b24-a1ec-30d643fd547c button.colab-df-convert'); buttonEl.style.display = google.colab.kernel.accessAllowed ? 'block' : 'none'; async function convertToInteractive(key) { const element = document.querySelector('#df-444f55fc-3233-4b24-a1ec-30d643fd547c'); const dataTable = await google.colab.kernel.invokeFunction('convertToInteractive', [key], {}); if (!dataTable) return; const docLinkHtml = 'Like what you see? Visit the ' + '&lt;a target=&quot;_blank&quot; href=https://colab.research.google.com/notebooks/data_table.ipynb&gt;data table notebook&lt;/a&gt;' + ' to learn more about interactive tables.'; element.innerHTML = ''; dataTable['output_type'] = 'display_data'; await google.colab.output.renderOutput(dataTable, element); const docLink = document.createElement('div'); docLink.innerHTML = docLinkHtml; element.appendChild(docLink); } &lt;/script&gt; &lt;/div&gt; 12# 왜도확인 -&gt; +-2범위에 들어가므로 치우침이 없다.print(train['count'].skew()) -0.8704448527723871 123456# 범주형 변수의 타입변환cate_name = ['weather','season','year','hour','dayofweek']for c in cate_name: train[c] = train[c].astype('category')train.dtypes season category holiday int64 workingday int64 weather category temp float64 atemp float64 humidity int64 windspeed float64 count float64 year category hour category dayofweek category dtype: object 1print(train.info()) &lt;class 'pandas.core.frame.DataFrame'&gt; RangeIndex: 10738 entries, 0 to 10737 Data columns (total 12 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 season 10738 non-null category 1 holiday 10738 non-null int64 2 workingday 10738 non-null int64 3 weather 10738 non-null category 4 temp 10738 non-null float64 5 atemp 10738 non-null float64 6 humidity 10738 non-null int64 7 windspeed 10738 non-null float64 8 count 10738 non-null float64 9 year 10738 non-null category 10 hour 10738 non-null category 11 dayofweek 10738 non-null category dtypes: category(5), float64(4), int64(3) memory usage: 641.3 KB None test 데이터도 train 데이터와 동일한 작업을 해준다 1234567891011# datetime으로부터 필요한 열 생성 : hour, dayofweek(month나 day는 의미없다)test['date'] = pd.to_datetime(test['datetime'])test['year'] = test['date'].dt.yeartest['hour'] = test['date'].dt.hourtest['dayofweek'] = test['date'].dt.dayofweek#불필요한 열 삭제del test ['datetime']del test ['date']display(test.head()) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } season holiday workingday weather temp atemp humidity windspeed year hour dayofweek 0 1 0 1 1 10.66 11.365 56 26.0027 2011 0 3 1 1 0 1 1 10.66 13.635 56 0.0000 2011 1 3 2 1 0 1 1 10.66 13.635 56 0.0000 2011 2 3 3 1 0 1 1 10.66 12.880 56 11.0014 2011 3 3 4 1 0 1 1 10.66 12.880 56 11.0014 2011 4 3 &lt;svg xmlns=”http://www.w3.org/2000/svg&quot; height=”24px”viewBox=”0 0 24 24” width=”24px”&gt; .colab-df-container { display:flex; flex-wrap:wrap; gap: 12px; } .colab-df-convert { background-color: #E8F0FE; border: none; border-radius: 50%; cursor: pointer; display: none; fill: #1967D2; height: 32px; padding: 0 0 0 0; width: 32px; } .colab-df-convert:hover { background-color: #E2EBFA; box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15); fill: #174EA6; } [theme=dark] .colab-df-convert { background-color: #3B4455; fill: #D2E3FC; } [theme=dark] .colab-df-convert:hover { background-color: #434B5C; box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15); filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3)); fill: #FFFFFF; } &lt;script&gt; const buttonEl = document.querySelector('#df-7f495a76-ee75-4609-b1cb-6f33686b3e83 button.colab-df-convert'); buttonEl.style.display = google.colab.kernel.accessAllowed ? 'block' : 'none'; async function convertToInteractive(key) { const element = document.querySelector('#df-7f495a76-ee75-4609-b1cb-6f33686b3e83'); const dataTable = await google.colab.kernel.invokeFunction('convertToInteractive', [key], {}); if (!dataTable) return; const docLinkHtml = 'Like what you see? Visit the ' + '&lt;a target=&quot;_blank&quot; href=https://colab.research.google.com/notebooks/data_table.ipynb&gt;data table notebook&lt;/a&gt;' + ' to learn more about interactive tables.'; element.innerHTML = ''; dataTable['output_type'] = 'display_data'; await google.colab.output.renderOutput(dataTable, element); const docLink = document.createElement('div'); docLink.innerHTML = docLinkHtml; element.appendChild(docLink); } &lt;/script&gt; &lt;/div&gt; 1test.info() &lt;class 'pandas.core.frame.DataFrame'&gt; RangeIndex: 6493 entries, 0 to 6492 Data columns (total 11 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 season 6493 non-null int64 1 holiday 6493 non-null int64 2 workingday 6493 non-null int64 3 weather 6493 non-null int64 4 temp 6493 non-null float64 5 atemp 6493 non-null float64 6 humidity 6493 non-null int64 7 windspeed 6493 non-null float64 8 year 6493 non-null int64 9 hour 6493 non-null int64 10 dayofweek 6493 non-null int64 dtypes: float64(3), int64(8) memory usage: 558.1 KB 123#1 weather 4값 제거#test = test[test['weather']!=4].reset_index(drop = True)#test.shape# test값을 건딜면 제출이 안되니 그냥 보존 123456# 범주형 변수의 타입변환cate_name1 = ['weather','season','year','hour','dayofweek']for c in cate_name1: test[c] = test[c].astype('category')test.dtypes season category holiday int64 workingday int64 weather category temp float64 atemp float64 humidity int64 windspeed float64 year category hour category dayofweek category dtype: object 1print(test.info()) &lt;class 'pandas.core.frame.DataFrame'&gt; RangeIndex: 6493 entries, 0 to 6492 Data columns (total 11 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 season 6493 non-null category 1 holiday 6493 non-null int64 2 workingday 6493 non-null int64 3 weather 6493 non-null category 4 temp 6493 non-null float64 5 atemp 6493 non-null float64 6 humidity 6493 non-null int64 7 windspeed 6493 non-null float64 8 year 6493 non-null category 9 hour 6493 non-null category 10 dayofweek 6493 non-null category dtypes: category(5), float64(3), int64(3) memory usage: 337.8 KB None STEP 6.모델링12345678910from sklearn.model_selection import train_test_split, GridSearchCVfrom sklearn.linear_model import LinearRegression, Ridgefrom xgboost import XGBRegressorfrom sklearn.ensemble import RandomForestRegressorfrom lightgbm import LGBMRegressorfrom sklearn.metrics import mean_squared_errorfrom sklearn.model_selection import cross_validatefrom sklearn.model_selection import GridSearchCVfrom sklearn.pipeline import Pipeline, make_pipeline 예측모델 변수와 타겟값을 나눠 X,y 데이터를 만든다. 12X = train.drop(['count'],axis = 1)y = train['count'] 수치형 변수인 temp,atemp,humidity,windspeed에 스케일링을 한다. 123from sklearn.preprocessing import StandardScalerss=StandardScaler()from sklearn.preprocessing import MinMaxScaler 1234567# 선형회귀모델lr_reg = LinearRegression()# 피처에 대한 표준화 진행과 K-fold를 함께 진행pipe = make_pipeline(MinMaxScaler(),lr_reg)scores = cross_validate(pipe,X,y,cv=5, scoring='neg_mean_squared_error',return_train_score =True)print(&quot;MSLE: {0:.3f}&quot;.format(np.mean(-scores['test_score']))) MSLE: 1.056 12345678#rfnp.random.seed(0)rf = RandomForestRegressor(n_estimators = 300)# 피처에 대한 표준화 진행과 K-fold를 함께 진행pipe = make_pipeline(MinMaxScaler(),rf)scores = cross_validate(pipe,X,y,cv=5, scoring='neg_mean_squared_error',return_train_score =True)print(&quot;MSLE: {0:.3f}&quot;.format(np.mean(-scores['test_score']))) MSLE: 0.225 1234567#LGBMlgbm = LGBMRegressor(n_estimators = 500, objective = 'regression')# 피처에 대한 표준화 진행과 K-fold를 함께 진행pipe = make_pipeline(MinMaxScaler(),lgbm)scores = cross_validate(pipe,X,y,cv=5, scoring='neg_mean_squared_error',return_train_score =True)print(&quot;MSLE: {0:.3f}&quot;.format(np.mean(-scores['test_score']))) MSLE: 0.183 12345678910111213# 피쳐 표준화minmax = MinMaxScaler()minmax.fit(X) # 훈련셋 모수 분포 저장X_scaled = minmax.transform(X)X_test_scaled = minmax.transform(test)# 최종파라미터 튜닝된 모델로 학습lgbm = LGBMRegressor(n_estimators = 300, objective = 'regression', learning_rate = 0.1, max_depth = 3, reg_lambda =1 ,subsample = 0.5, random_state = 99)# 학습lgbm.fit(X_scaled, y) LGBMRegressor(max_depth=3, n_estimators=300, objective='regression', random_state=99, reg_lambda=1, subsample=0.5) 123456789101112# test에 대한 예측pred = lgbm.predict(X_test_scaled)fpred = np.expm1(pred)# 로그변환을 풀어줌#lgbm 모델의 feature importanceimp = pd.DataFrame({'feature': test.columns, 'coefficient':lgbm.feature_importances_})imp = imp.sort_values(by = 'coefficient', ascending = False)plt.barh(imp['feature'],imp['coefficient'])plt.show() 피처의 중요도를 뽑아봤을때 역시 hour가 가장 높은 기여도로 나왔다. STEP7. 제출1234del submission['count']submission['count'] =fpred#submission.to_csv('submission.csv',index=False) 1submission.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } datetime count 0 2011-01-20 00:00:00 8.674224 1 2011-01-20 01:00:00 4.650374 2 2011-01-20 02:00:00 2.842267 3 2011-01-20 03:00:00 1.616020 4 2011-01-20 04:00:00 1.506303 &lt;svg xmlns=”http://www.w3.org/2000/svg&quot; height=”24px”viewBox=”0 0 24 24” width=”24px”&gt; .colab-df-container { display:flex; flex-wrap:wrap; gap: 12px; } .colab-df-convert { background-color: #E8F0FE; border: none; border-radius: 50%; cursor: pointer; display: none; fill: #1967D2; height: 32px; padding: 0 0 0 0; width: 32px; } .colab-df-convert:hover { background-color: #E2EBFA; box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15); fill: #174EA6; } [theme=dark] .colab-df-convert { background-color: #3B4455; fill: #D2E3FC; } [theme=dark] .colab-df-convert:hover { background-color: #434B5C; box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15); filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3)); fill: #FFFFFF; } &lt;script&gt; const buttonEl = document.querySelector('#df-7fa8931c-8dc7-4b8d-aea5-9bf16a3575e2 button.colab-df-convert'); buttonEl.style.display = google.colab.kernel.accessAllowed ? 'block' : 'none'; async function convertToInteractive(key) { const element = document.querySelector('#df-7fa8931c-8dc7-4b8d-aea5-9bf16a3575e2'); const dataTable = await google.colab.kernel.invokeFunction('convertToInteractive', [key], {}); if (!dataTable) return; const docLinkHtml = 'Like what you see? Visit the ' + '&lt;a target=&quot;_blank&quot; href=https://colab.research.google.com/notebooks/data_table.ipynb&gt;data table notebook&lt;/a&gt;' + ' to learn more about interactive tables.'; element.innerHTML = ''; dataTable['output_type'] = 'display_data'; await google.colab.output.renderOutput(dataTable, element); const docLink = document.createElement('div'); docLink.innerHTML = docLinkHtml; element.appendChild(docLink); } &lt;/script&gt; &lt;/div&gt;","link":"/2022/07/12/bikesd_220711/"}],"tags":[],"categories":[]}